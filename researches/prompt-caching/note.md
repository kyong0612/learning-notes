# プロンプトキャッシュとは何か

プロンプトキャッシュ（Prompt Cache）は、大規模言語モデル（LLM）のAPIリクエストを最適化するための技術で、プロンプトの一部を保存して再利用することにより、処理速度を向上させてコストを削減する仕組みです。

## 基本的な仕組み

1. **キャッシュの仕組み**：
   - LLMにプロンプトを送る際、プロンプトの前半部分（プレフィックス）がキャッシュされます
   - 次回同じプレフィックスを使用する場合、その部分の再計算が不要になります
   - これにより、処理時間（レイテンシー）とコストが大幅に削減されます

2. **具体的なメリット**：
   - レイテンシー（応答時間）：最大80-85%削減
   - コスト：最大50-90%削減（プロバイダーによって異なる）
   - サーバー負荷の軽減：同じリソースでより多くのリクエストを処理可能
   - 環境への配慮：計算リソースの節約によるエネルギー効率の向上

## 主要プロバイダーの実装比較

### OpenAI

- **設定方法**：自動的に有効化（コード変更不要）
- **対象**：1,024トークン以上のプロンプト
- **効果**：レイテンシー最大80%削減、コスト50%削減
- **キャッシュ寿命**：5-10分（オフピーク時は最大1時間）
- **対応モデル**：GPT-4o、GPT-4o-mini、o1-previewなど
- **モニタリング**：`response.usage.prompt_tokens_details.cached_tokens`で確認可能

### Anthropic（Claude）

- **設定方法**：手動設定（`cache_control`パラメータを使用）
- **対象**：
  - Claude 3.5 SonnetとClaude Opus：1,024トークン以上
  - Claude 3.5 HaikuとClaude 3 Haiku：2,048トークン以上
- **効果**：キャッシュ読み取りで90%のコスト削減
- **料金**：キャッシュ書き込みは通常料金の25%増し
- **キャッシュ寿命**：5分（使用するたびに更新）
- **特徴**：最大4つのキャッシュブレークポイントを設定可能
- **モニタリング**：`cache_creation_input_tokens`と`cache_read_input_tokens`で確認可能

### Google（Gemini）

- **設定方法**：手動設定（「context caching」と呼ばれる）
- **対象**：32,768トークン以上のプロンプト
- **キャッシュ寿命**：デフォルトは1時間（カスタマイズ可能）
- **対応モデル**：Gemini 1.5の安定版

## 主なユースケース

1. **チャットボットとカスタマーサポート**：
   - 類似の問い合わせに迅速に応答
   - 長い会話の文脈を効率的に維持
   - ユーザー体験の向上と運用コストの削減

2. **コーディングアシスタント**：
   - コードベースの大部分をプロンプトに含めても処理が迅速
   - 共通するコードパターンや修正を効率的に提供
   - 開発者の生産性向上をサポート

3. **大規模ドキュメント処理**：
   - 法的契約書、マニュアル、報告書などの長文を効率的に処理
   - 同じドキュメントに対する異なる質問を低コストで処理
   - 一貫した分析結果を提供

4. **ツールを使うAIエージェント**：
   - 多数のツール定義やスキーマをキャッシュして効率化
   - 複数のツール呼び出しを含む複雑な操作を高速化
   - エージェントの応答性と経済性を向上

5. **RAGアプリケーション**：
   - 検索で取得した長いコンテキストをキャッシュ
   - 同じ情報源に対する複数のクエリを最適化
   - より大きなコンテキストウィンドウを経済的に活用

## 実装方法

### OpenAIでの実装（自動）

```python
from openai import OpenAI

client = OpenAI()

# 静的なコンテンツをプロンプトの先頭に配置
system_prompt = """You are an expert assistant that provides detailed explanations on various topics.
Please ensure all responses are accurate and comprehensive.
"""

# 可変コンテンツを末尾に配置
user_prompt = "Can you explain the theory of relativity?"

# プロンプトを結合
messages = [
    {"role": "system", "content": system_prompt},  # 静的部分（キャッシュ対象）
    {"role": "user", "content": user_prompt}       # 動的部分
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages
)

# キャッシュされたトークン数を確認
print(response.usage.prompt_tokens_details.cached_tokens)
```

### Anthropicでの実装（手動）

```python
import anthropic

client = anthropic.Anthropic()

# 大きなコンテキスト（例：法的文書など）
large_context = """[ここに長い文書のテキストが入る]"""

# キャッシュを有効化した最初のリクエスト
response = client.messages.create(
    model="claude-3-5-sonnet-20240620",
    system=[
        {
            "type": "text",
            "text": large_context,
            "cache_control": {"type": "ephemeral"}  # キャッシュを有効化
        }
    ],
    messages=[
        {"role": "user", "content": "この文書の要点を教えてください"}
    ]
)

# キャッシュのパフォーマンスを確認
print(response.usage)
# 出力例: {"cache_creation_input_tokens":188086, "cache_read_input_tokens":0, "input_tokens":21, "output_tokens":393}

# 2回目のリクエスト（キャッシュを利用）
response2 = client.messages.create(
    model="claude-3-5-sonnet-20240620",
    system=[
        {
            "type": "text",
            "text": large_context,
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[
        {"role": "user", "content": "この文書の法的影響について説明してください"}
    ]
)

# 2回目のキャッシュ利用状況
print(response2.usage)
# 出力例: {"cache_creation_input_tokens":0, "cache_read_input_tokens":188086, "input_tokens":21, "output_tokens":393}
```

## 効果的な使用のためのベストプラクティス

1. **プロンプト構造の最適化**：
   - 静的・再利用可能な内容（指示、コンテキスト、例など）をプロンプト先頭に配置
   - 動的な内容（ユーザー入力など）を末尾に配置
   - 一貫したフォーマットを維持（空白、改行、記号なども同一に）

2. **キャッシュのモニタリング**：
   - キャッシュヒット率を定期的に確認
   - コスト削減効果を測定
   - 必要に応じてプロンプト構造を調整

3. **キャッシュの効果を最大化**：
   - より長いプロンプトを使用（最小トークン数を超える）
   - プロンプトの静的部分を頻繁に変更しない
   - ピーク時間外にリクエストを行う（特にOpenAIの場合）

## 課題と制限

1. **キャッシュの管理**：
   - 古いデータが残ると不正確な応答の原因になる可能性
   - キャッシュの更新タイミングを適切に管理する必要がある

2. **リソース消費**：
   - キャッシュはサーバー側でメモリとストレージを消費
   - 大規模な展開では追加リソースが必要になる場合も

3. **実装の複雑さ**：
   - 特にAnthropicやGoogleでは手動設定が必要
   - キャッシュブレークポイントの設計には経験が必要

4. **セキュリティとプライバシー**：
   - 機密データのキャッシングにはリスクが伴う
   - 適切なセキュリティ対策が必要

5. **最小トークン要件**：
   - 短いプロンプトではキャッシュの恩恵を受けられない
   - プロバイダーによって最小要件が異なる（1,024〜32,768トークン）

## まとめ

プロンプトキャッシュは、LLMアプリケーションのパフォーマンスとコスト効率を大幅に向上させる強力な最適化技術です。特に長いプロンプトや反復的なコンテンツを扱うアプリケーションにおいて、応答時間の短縮とコスト削減の両方を実現できます。

プロバイダーによって実装方法は異なりますが、基本的な原理は共通しています：プロンプトの静的な部分をキャッシュして再利用することで、処理の効率化を図るものです。適切に構造化されたプロンプトと定期的なモニタリングにより、この技術の恩恵を最大限に活用することができます。

今後のAIアプリケーション開発において、プロンプトキャッシュは標準的な最適化テクニックとして、より一般的に採用されていくでしょう。
