# CS 194/294-280 (Advanced LLM Agents) - Lecture 1, Xinyun Chen

ref: <https://www.youtube.com/watch?v=g0Dwtf3BH-0&list=PLS01nW3RtgorL3AW8REU9nGkzhvtn6Egn&index=5>

## 1. 講義の概要とイントロダクション

- **講義タイトル・担当者**
  - **講義名**：「Advanced Large Language Model Agents（先進的大規模言語モデルエージェント）」
  - **講師・共催**：
    - 講師：Xinyun Chen（Google DeepMindのリサーチサイエンティスト）  
      → 最新の LLM 技術を活用したエージェントの開発背景や実用例について解説
    - 共催：UC Berkeley の Dawn Song 教授、Meta の Kaiyu Yang  
      → 複数の先端研究機関・企業の連携により、実世界での応用可能性や実証実験が進められている
  - **コース運営**：TA チーム（Alex、Tara、Ashwin、Jason）  
    → 大規模なオンライン講義として、学内外から多数の参加者が集まる中での運営体制についても言及

- **前学期の実績**
  - UC Berkeley や MOOC 上での初版公開
  - ゲストスピーカーによる幅広いトピックの講演実施  
    → 業界最前線の事例や研究成果（例：LLM を活用した自動プログラミング、対話システム等）を共有
  - MOOC登録者：約15,000名、ハッカソン参加者：約3,000名  
    → オープンな学習コミュニティや国際的な参加者による活発な議論の場となっている
  - 産業パートナーから約20万ドル相当の賞品・リソース提供  
    → 産学連携の一例として、実世界での応用に向けた資金・技術支援の重要性を示す

---

## 2. 大規模言語モデル（LLM）エージェントの基本概念

- **エージェントの仕組み**
  - **基本原理**：  
    - エージェントは、LLM を「脳」として利用し、各タイムステップで「推論」および「計画」を行いながら、環境との相互作用を通じて行動を決定する。
    - この仕組みにより、単一の入力から最終的な解答や行動を生成する従来のシステムとは異なり、動的に状況に応じた意思決定が可能となる。
  - **内部メモリとフィードバックの活用**：  
    - 行動後に得られる環境からのフィードバックを基に、内部状態（メモリ）を更新することで、次のステップでより良い判断を下す仕組みが構築される。

- **エージェントフレームワークの特徴**
  - **外部ツールの活用**：  
    - エージェントは、必要に応じて外部ツールや知識ベースを利用し、単一のモデル内に収まりきらない情報を補完できる（例：計算ツール、データベースアクセス）。
  - **試行錯誤のプロセス**：  
    - 実世界のタスクは未知の要素が多いため、環境と対話することで成功例や失敗例を学習し、より効果的な行動パターンを獲得する。

- **背景と応用目的**
  - **実世界タスクの複雑性**：  
    - タスクは多くの場合、分解やサブタスクの割り当て、協調など複数のステップに分かれる。エージェントフレームワークはこうした複雑性を管理するための手法として有効。
  - **応用分野の拡大**：  
    - 既存の応用例（コード生成、パーソナルアシスタント、ロボティクス）に加え、教育、金融など新たな分野への展開が期待される。  
      → 近年の LLM の性能向上（例：GPT-4 や PaLM 2 など）により、より複雑なタスクへの応用が現実味を帯びている。

---

## 3. コース運営とプロジェクト・ラボの概要

- **実習課題（ラボ）の運用変更**
  - **変更点**：  
    - 昨年に比べ、ラボの提出期限が後ろ倒しとなり、より充実した内容となる。  
      → 参加者は深い理解と実践を通じた学習が可能となる。
  - **参加者の学習機会の拡大**：  
    - 時間的余裕を持って実験・実装が行えるため、実際の開発現場に近い環境での学習体験が提供される。

- **プロジェクトのトラック設定**
  - **アプリケーショントラックとリサーチトラックの2本柱**
    - アプリケーショントラック：実践的な応用例に焦点を当て、実装やデモの作成に取り組む
    - リサーチトラック：学会発表やワークショップ投稿を目指す参加者向け。  
      → 例として、実際に大規模言語モデルの新たな手法を論文としてまとめる経験が積める
  - **選考プロセス**：  
    - 専用の Google フォームによる応募の後、ポスドクや大学院生の指導のもとで進められる。

- **最新情報の確認**
  - **ウェブサイトと Edstem**：  
    - 最新の講義資料、スライド、変更情報、ゲストスピーカーの日程が随時更新される。  
      → 受講者は常に最新の情報にアクセス可能で、動的な学習環境が整えられている。

---

## 4. 推論時技術の概要とその背景

- **推論時技術の目的**
  - **可変計算**：  
    - 問題の難易度に応じて、推論に使用するトークン数（計算量）を柔軟に調整する必要がある。  
      → 「思考過程」を生成することで、単純な出力だけでなく、その過程自体が検証可能な情報となる。
  - **内部プロセスの透明化**：  
    - 生成過程（チェーン・オブ・ソート）が明示されると、どの段階で誤りが生じたか、またはどのステップが有効であったかを分析できる。

- **モデルの進化と性能向上**
  - **モデル間の性能差**：  
    - OpenAI の o1～o3、Gemini 2.0 など、最新のモデルでは推論時の計算資源の増加により性能が大幅に向上している。  
      → 特に数学やプログラミング問題において、従来のモデルでは不可能だった複雑な推論が可能に。
  - **具体的事例**：  
    - Alpha Proof や Alpha Geometry システムは、数学競技（IMO など）で高い成績を収めており、最新の o3 モデルは Codeforces の競技プログラミングでも上位にランクインしている。  
      → これらは、LLM が実世界の複雑なタスクに対応可能であることを示す好例となっている。

- **デモンストレーションの事例**
  - **内部思考の可視化**：  
    - o1 モデルでは、従来の単一出力生成から、途中の「思考過程」を内部で生成し、最終解答までの流れを柔軟に調整する仕組みが導入されている。
  - **画像入力の例**：  
    - Gemini 2.0 Thinking モデルは、画像内の数字を認識し、例えば「9」を「6」に変換するなど、複雑な視覚情報の処理と推論が統合されている。  
      → このようなマルチモーダル対応は、今後の応用領域を大きく拡げる可能性がある。

---

## 5. 基本的なプロンプト技法と Chain-of-Thought（CoT）

- **標準プロンプト手法の限界**
  - **従来手法**：  
    - 質問と最終解答のみを提示するため、モデルは解答の「根拠」や「計算過程」を内部で隠蔽し、出力に反映しない。  
      → その結果、推論の過程が不透明となり、エラーの原因分析が困難になる。

- **Chain-of-Thought プロンプティングの導入**
  - **基本概念**：  
    - 問題解決の際に、計算手順や中間的な推論ステップを例示として与えることで、モデルが段階的に解答を導出する。
  - **補足説明**：  
    - この手法は、Wei et al.（2022）の「Chain-of-Thought Prompting Elicits Reasoning in Large Language Models」で初めて体系的に示され、モデルサイズが大きくなるほど効果が顕著になると報告されている。  
      → []

- **Zero-shot CoT の提案**
  - **特徴**：  
    - 単一の指示文（例："Let's think step by step"）のみで、モデルが自然に思考過程を生成する手法。  
      → 例示を用意する手間が省け、実践的な利用が促進されるが、Few-shot 方式に比べると性能面で劣る場合もある。
  - **背景**：  
    - 最近の研究では、Zero-shot CoT が特に難解な数学問題やシンボリック推論タスクで効果的であることが示されている。

---

## 6. アナロジカル・プロンプティングとプロンプト最適化

- **アナロジカル・プロンプティングの概要**
  - **基本アイディア**：  
    - テスト問題に対して、あらかじめ用意された例示ではなく、モデル自身がその問題に関連する Chain-of-Thought を自己生成し、解答を導出する。  
      → これにより、各問題ごとに最適な例示が動的に生成され、汎用性と個別最適化が可能になる。
  - **補足説明**：  
    - この手法は、人間が新しい問題に直面した際に、過去の類似経験を思い出して解法を構築するプロセスと類似している。  
      → Polya の『How to Solve It』のアプローチとも比較される。

- **プロンプト自動生成・最適化の手法**
  - **自動生成**：  
    - 「Large Language Models are Human Level Prompt Engineers」（ICLR 2023 など）の研究により、大規模モデルを用いてタスク記述から自動的に複数の初期プロンプトを生成し、少数の検証問題で評価する方法が提案されている。  
      → 手動のプロンプト設計にかかる労力を大幅に削減できる。
  - **反復最適化**：  
    - 生成されたプロンプトとその解答精度の履歴をもとに、モデル自身がプロンプトを反復的に改善する仕組みが導入され、Few-shot CoT に匹敵する、あるいはそれ以上の性能が実現される可能性がある。

---

## 7. 問題解決における思考過程の戦略

- **チェーン・オブ・ソート（Chain-of-Thought）の意義**
  - **可変計算**：  
    - 問題の複雑性に応じて必要な推論ステップを動的に増減させることで、単一の一定計算量ではなく、より柔軟な対応が可能となる。
  - **人間の思考プロセスとの類似性**：  
    - 分解、計画、自己反省など、人間が難問に取り組む際のプロセスをモデルに取り入れることで、より自然で解釈可能な推論が実現する。

- **Least-to-Most プロンプティング**
  - **基本概念**：  
    - 複雑な問題を、より簡単なサブ問題に分解し、段階的に解決する手法。  
      → 各サブタスクの解答を組み合わせることで、全体の解答を導出する。
  - **補足説明**：  
    - この手法は、問題の構造化に焦点を当て、全体の解法を体系的に導くため、特に数学やプログラム生成などのタスクで有効とされる。

- **動的な例示選択と自己発見**
  - **アプローチ**：  
    - 各サブタスクに対して、その問題に最も関連性の高い例示や高レベルの知識を動的に生成・選択する仕組みを導入。  
      → これにより、固定の例示では捉えきれない多様な問題状況に対応可能になる。

---

## 8. 複数候補生成と自己整合性（Self-Consistency）

- **基本概念**
  - **候補生成**：  
    - 同一問題に対して、複数の解答候補を並列に生成し、それぞれの最終解答の頻度や整合性を評価する。  
      → 例えば、数学問題で複数解答のうち多数派を正答とする。
  - **補足説明**：  
    - Wang et al.（2022）などの研究で、Self-Consistency による多数決方式が、単一解答生成よりも安定した正答率向上に寄与することが示されている。  
      → []

- **候補生成手法と多様性の確保**
  - **サンプリング技術**：  
    - 温度パラメータや nucleus sampling などの技法を用いて、生成される候補に十分な多様性を持たせる。
  - **評価手法**：  
    - 対数確率によるサンプル＆ランク方式と比較して、候補数を増やすことで性能が向上する傾向がある。

- **コード生成への応用**
  - **AlphaCode の事例**：  
    - Google DeepMind の AlphaCode では、生成されたコード候補を実際に実行し、出力結果の一致によって候補をクラスタリングする手法が用いられている。  
      → これにより、コードの正確性や実行可能性が検証される。

- **Universal Self-Consistency**
  - **自由形式タスクへの拡張**：  
    - 要約や質問応答など、明確な「最終解答」の抽出が難しいタスクに対しても、候補全体の整合性を評価する手法が提案されている。

- **検証モデルの利用**
  - **ランク付けの補助**：  
    - アウトカム・サーヴァイズド報酬モデルやプロセス・サーヴァイズド報酬モデルを用いて、各解答やその推論過程の正当性をより精密に評価する試みが進められている。

---

## 9. ツリー探索（Tree-of-Thought）による探索拡張

- **基本概念**
  - **探索手法の導入**：  
    - 解答生成の各ステップで複数の候補（枝）を生成し、部分解答の評価に基づいて有望な枝を選択することで、全体として最適な解答を探索する手法。
  - **補足説明**：  
    - 従来の Chain-of-Thought が1本の解答生成に依存するのに対し、Tree-of-Thought は並列的な枝展開により、探索空間を広げ、局所解に陥らずに全体最適解を目指す。  
      → 最近の研究では、基本的な幅優先探索（BFS）や深さ優先探索（DFS）のほか、モンテカルロ木探索（MCTS）などの手法が検討されている。

- **具体例**
  - **24ゲームの例**：  
    - 各ステップで可能な数字の組み合わせ（思考生成）を提示し、最終目標（数値「24」への到達）の可能性を評価（思考評価）する。  
      → この例は、探索と評価のサイクルが如何に有効に働くかを示す代表的なケースである。

---

## 10. 反復的自己改善（自己修正）の手法

- **背景と必要性**
  - **初回生成の限界**：  
    - 先進モデルであっても、最初の解答生成に誤りが含まれる可能性があるため、単に並列生成するだけではエラー修正が十分でない。
  - **人間の自己修正との類似性**：  
    - 人間が問題解決の過程で内省し、エラーを見直すように、モデルにも同様のフィードバックループ（自己反省・自己修正）を組み込む必要がある。

- **自己反省・自己修正のプロセス**
  - **手法の概要**：  
    - 初回解答後、モデルがその解答の問題点や改善策を自己生成し、外部からの評価信号（例：環境の正誤判定、ユニットテスト結果）と合わせて、解答を再生成する。
  - **補足説明**：  
    - 「Reflection」や「Self-Refine」といった手法は、実際に HotPotQA やコード生成タスクで効果が示されているが、外部オラクルが存在しない実環境では自己評価の精度に課題が残る。  
      → ICLR で発表された「Large Language Models Cannot Self-Correct Reasoning Yet」などで否定的な結果も報告されている。

- **コード生成への応用**
  - **自己デバッグプロセス**：  
    - コード生成時には、生成したコードに対して簡潔な正誤情報、ユニットテスト結果、行ごとの説明、実行トレースのシミュレーションなど、複数のフィードバック形式を適用し、逐次的に修正を行う。
  - **実践例**：  
    - IDE でのデバッグ作業に近いプロセスをモデルに適用することで、実際のプログラムの実行可能性や正確性が向上する。

- **推論時予算の最適化**
  - **並列生成と逐次生成のバランス**：  
    - 問題の難易度やモデル特性に応じ、複数候補の並列生成と、自己修正を重ねる逐次生成の最適なバランスを見極める必要がある。
  - **FLOPs とモデルサイズの考慮**：  
    - 同じ計算資源内で、軽量モデルで多数の候補を生成する戦略や、高性能モデルで精度向上を狙う戦略など、タスクに応じた選択が求められる。

---

## 11. 総括と今後の展望

- **本講義の主要内容**
  - **推論時トークン予算拡大の手法**：  
    - 基本的なプロンプト技法、複数候補生成と自己整合性、反復的自己改善という 3 つの側面を通して、LLM の推論性能を向上させる方法を解説。
  - **各手法の統合の重要性**：  
    - 使用する LLM の能力やタスクの特性に応じ、最適な手法を組み合わせることが、実世界での応用において極めて重要である。

- **今後の課題と方向性**
  - **計算資源のスケールと最適化**：  
    - Richard Sutton の「The Bitter Lesson」に示されるように、計算資源の増加に伴って性能がスケールする手法の追求が重要。  
      → すなわち、モデルに継続的な自己改善や自己発見の仕組みを組み込むアプローチが求められる。
  - **自己修正ループの精度向上**：  
    - 外部オラクルが存在しない状況下で、モデルが正確に自己評価・自己修正を行えるような新たなフィードバック手法の開発が今後の大きな課題となる。

- **結論**
  - **統合的アプローチの必要性**：  
    - LLM の推論性能向上には、単に最終解答を生成するだけでなく、思考過程の明示、複数候補の探索、そして自己改善ループの統合が不可欠である。
  - **今後の展望**：  
    - 各種手法の最適な組み合わせと推論予算のバランス調整により、より効率的かつ柔軟な AI エージェントの開発が進展することが期待される。

---

## 補足参考文献

- Wei, J., et al. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." (2022). [論文では、CoT プロンプティングの効果やそのスケーリング特性について詳述されている]
- Wang, A., et al. "Self-Consistency Improves Chain-of-Thought Reasoning in Language Models." (2022). [自己整合性の手法とその効果について解説]
- その他、ICLR 2023 の「Large Language Models are Human Level Prompt Engineers」など、最新の研究成果も参照することで、上記手法の背景や実装例を理解できる。
