# CS 194/294-280 (Advanced LLM Agents) - Lecture 2, Jason Weston

ref: <https://www.youtube.com/watch?v=_MNlLhU33H0&list=PLS01nW3RtgorL3AW8REU9nGkzhvtn6Egn&index=5>

## 自己改善型言語モデルの研究：歴史、手法、評価、および今後の展望

本稿は、近年急速に発展している自己改善型言語モデルに関する講義内容を、詳細な背景情報、補足説明、関連研究の動向および参考文献（概説）を交えながら、論理的な章立てと段落構成、箇条書きを用いて整理・再構成したものである。以下では、初期の言語モデリングの歴史から、システム１とシステム２の概念、自己評価および自己改善を実現するための最新手法、さらにその評価と今後の研究課題について詳細に解説する。

---

### 1. はじめに

現代のAI研究は、芸術の創作、コードの生成、形式的な文書の作成、ブレインストーミングなど、日常生活の多岐にわたるタスクに活用されており、その進化は極めて急速である。講義では、以下の点が強調された：

- **急速な技術進歩**  
  - 大規模言語モデル（LLM）の性能向上が実時間に近い形で実現されている。
  - 最新の改善例として、推論能力や数学的問題解決におけるベンチマークの向上が報告されている。

- **自己改善の概念**  
  - 従来の固定データセットに基づく教師あり学習から、強化学習や自己報酬機構を用いた自己改善型手法へと進化している。
  - モデル自らが生成したタスクや応答に対して、内部で評価・報酬を与え、次の学習サイクルに反映させる仕組みが注目されている。

---

### 2. 言語モデリングの歴史的背景と進化

言語モデリングの基礎概念は、1950年代のクロード・シャノンによる情報理論に端を発し、その後の研究で次のような発展を遂げた。

#### 2.1 初期の理論とニューラルネットワークの登場

- **基本概念の確立**  
  - 言語の次の単語やトークンの確率分布を予測することにより、言語データの構造を捉える手法が確立された。
  
- **2003年の初期研究**  
  - Ben Duchamp や Vincent（おそらく初期のニューラルネットワークを用いた言語モデリング手法）の研究では、単語埋め込み（word embeddings）を初層に、非線形活性化（tanh）やソフトマックスを組み合わせたモデルが提案された。
  - この時点で、モデルの容量や計算効率を向上させるための多くの課題が提示され、今後の発展への布石となった。

#### 2.2 2000年代中盤のアプローチと転換

- **サポートベクターマシン（SVM）の活用**  
  - 当時は、SVMやカーネル法が主流であり、部分的なシステム（品詞タグ付け、固有表現抽出など）に対して、カスケード型の手法が採用されていた。
  - Ronald Collobert との共同研究では、統一的なニューラルネットワークによるエンドツーエンドの学習が提案され、後の大規模言語モデルへの道を切り拓いた。

- **初期のマスクド言語モデルと埋め込み手法**  
  - Wikipediaのような大規模コーパスを用いたマスクド言語モデルの先駆けとなる研究も行われ、単語の類似性や文脈情報の抽出が示された。

#### 2.3 トランスフォーマーの登場と大規模モデルへのシフト

- **2017年：トランスフォーマーの提案**  
  - 多頭注意機構（multi-head attention）や自己注意機構（self-attention）、正規化技術の導入により、従来のアーキテクチャを大幅に改善。
  
- **2018年：BERTなどのマスクド言語モデル**  
  - トランスフォーマーアーキテクチャを用いたBERTの登場により、事前学習とファインチューニングの組み合わせが広く普及。

- **スケーリング仮説の提唱**  
  - 「大きなデータセット＋大きなモデル＝成功」という考え方が浸透し、GPTシリーズなど大規模モデルの開発へと繋がった。

---

### 3. システム１とシステム２の推論メカニズム

人間の思考プロセスをモデル化する際、以下の2つのシステムが提唱される：

#### 3.1 システム１：自動的・反応的な処理

- **概要**  
  - 人間における直感的かつ迅速な判断に類似し、ニューラルネットワーク内部の隠れ状態の連続的な計算過程に対応する。
  
- **言語モデルにおける特徴**  
  - 固定の計算量で次の単語を予測する。
  - 学習過程で生じる不要な相関関係や幻覚（hallucination）、シコピー（sycophancy）、ジャイルブレイク（jailbreaking）の問題が指摘される。

#### 3.2 システム２：意識的かつ反省的な思考（Chain-of-Thought）

- **概要**  
  - 問題解決や計画、検証など、より複雑な推論プロセスに対応する。
  - モデルが最終出力前に内部の「思考過程」を生成することで、段階的な推論を実現する。

- **応用例と利点**  
  - 数学問題や論理的推論において、従来のシステム１だけの出力に比べ大幅な性能向上が報告されている。
  - 検証や計画、自己修正のプロセスを通じ、事実誤認の低減や出力の整合性向上が期待される。

---

### 4. 自己改善型言語モデルの学習手法

自己改善型の言語モデルは、モデル自身が生成したデータや評価を用いて、反復的なトレーニングループにより性能を向上させる。ここでは主な手法について整理する。

#### 4.1 自己報酬機構（Self-Rewarding Language Models）

- **基本概念**  
  - モデルが自身の出力に対して報酬を割り当て、その評価に基づいてパラメータを更新する。
  - 従来の人間によるフィードバック（RLHF：Reinforcement Learning from Human Feedback）をモデル自身の評価に置き換える試み。

- **トレーニングパイプラインの概要**  
  1. **タスク生成**  
     - シードとなる人間作成の指示を用い、自己指導（Self-Instruct）で新たなタスクを生成。
  2. **応答生成**  
     - 各タスクに対して複数の候補応答を生成。
  3. **評価と報酬の付与**  
     - 「LLM-as-a-Judge」プロンプトを用いて、各候補に対するスコア（例：0～5点）を算出し、最良・最悪の応答を選定。
  4. **最適化**  
     - Direct Preference Optimization（DPO）などの手法で、良い応答の確率を高め、悪い応答の確率を低減させる。

- **実験結果と評価**  
  - AlpacaEval 2.0 や MT-Bench といったベンチマークにおいて、反復サイクルごとに性能が向上し、最終的には人間の評価やGPT-4の評価とも一致する結果が得られている。

#### 4.2 Chain-of-Thought（CoT）による推論の強化

- **Few-shot と Zero-shot アプローチ**  
  - 数学問題や知識問題に対し、具体例（Few-shot）または「step-by-step」などのトリガー（Zero-shot）を用いて、段階的な推論過程を生成する手法。

- **検証プロセスの導入**  
  - 初期のドラフト応答に対して、さらに詳細な検証質問を生成させることで、誤った出力の修正を図る「Chain of Verification」手法が提案されている。

#### 4.3 思考プロセスを用いた評価能力の向上

- **LLM-as-a-Judge とその拡張**  
  - モデル自身が応答の評価を行う「自己評価」機構を導入し、さらにその評価プロセスをメタレベルで検証する「Meta-Rewarding Language Models」が提案されている。
  
- **評価のための計画立案と実行**  
  - 「Thinking LLM-as-a-Judge」と呼ばれる手法では、評価の前に評価計画を立案し、その計画に従って評価を実行することで、評価精度を向上させる工夫がなされている。
  
- **具体的な実験フロー**  
  1. 指示と候補応答を入力として与える。
  2. 内部で評価基準（例：関連性、明瞭性、正確性、独創性）に基づくチェーン・オブ・ソート（思考過程）を生成。
  3. 複数の評価結果を統合し、最終的な評価を確定する。
  4. メタジャッジを用いて、複数の評価結果間の一貫性や精度を比較・向上させる。

---

### 5. 反復的な最適化と今後の研究課題

#### 5.1 反復的自己改善ループ

- **トレーニングサイクル**  
  - 初期のシードデータを基に、自己生成タスク・応答・評価のループを複数回実施することで、モデルの指示従属性と推論能力が向上する。
  - 各反復で、評価基準（win rate やペアワイズ精度など）に基づいた改善が観察される。

- **実績**  
  - 例えば、LLAMA-2-70Bを基盤とした実験では、反復サイクルごとに勝率が10%から20%に向上し、最終的にはGPT-4に匹敵する性能が得られた。

#### 5.2 多様なタスクへの適用と拡張

- **数学問題以外への展開**  
  - 詩の作成、文章生成、知識抽出など、数値的検証が困難なタスクに対しても、チェーン・オブ・ソートの手法を応用する試みが進んでいる。
  
- **連続空間での推論表現（COCONUT）**  
  - チェーン・オブ・ソートの生成結果をテキストではなく連続的なベクトル表現に置き換えることで、より柔軟な推論を実現する可能性が検討されている。
  - これにより、従来の自然言語による解釈性と、ニューラルネットワーク内部の連続表現の両立が期待される。

#### 5.3 今後の展望と課題

- **自己評価と自己認識の深化**  
  - 自己評価機能のさらなる向上は、モデルが自らの知識の限界を認識し、適切なフィードバックを生成するために重要である。
  - Ilya Sutskever など著名な研究者は、エージェントとしての自律性、合成データの利用、推論時計算の最適化、そして「自己認識」につながる概念を提唱している。

- **システム１とシステム２の融合**  
  - 現行のモデルでは、システム１（高速な直感的推論）とシステム２（反省的な思考）が同一のネットワーク内で動作しているが、これらを明確に分離または統合する新たなアーキテクチャの研究が期待される。

- **安全性と解釈性の確保**  
  - 推論過程が明示的に出力されることで、安全性（誤情報の排除やシコピー現象の抑制）と解釈性の向上が期待される一方、内部表現の連続性と人間の解釈可能性とのバランスをとる必要がある。

---

### 6. まとめ

本稿で示したように、自己改善型言語モデルは以下の点で大きな可能性を秘めている：

- **急速な進歩と実用化**  
  - 日常の多岐にわたるタスクにおける実用性が既に示され、今後もさらに改善が進む見込みである。

- **自己評価と自己改善のメカニズム**  
  - モデル自身が評価（LLM-as-a-Judge、Meta-Judge）を行い、自己報酬によって性能を向上させるサイクルは、従来の人間フィードバックに依存しない新たな学習パラダイムを構築する。

- **今後の研究課題**  
  - システム１とシステム２の融合、連続表現を用いた推論、そして安全性・解釈性の向上など、多くの未解決問題が存在する。
  - これらの課題に対する解決策は、将来的な「超人型AI」の実現に向けた重要な一歩となるだろう。

本稿は、過去から現在に至る言語モデリングの進化と、最新の自己改善手法の全体像を示すとともに、今後の展望についても概観するものである。各セクションで述べた内容は、個別の研究論文や実験結果に基づくものであり、今後の研究進展とともに更なる発展が期待される。

---

### 参考文献（概説）

- **初期の言語モデルとニューラルネットワーク**  
  - Claude Shannon, "A Mathematical Theory of Communication"（1948年）  
  - Bengio et al., "A Neural Probabilistic Language Model"（2003年）

- **統一アーキテクチャとエンドツーエンド学習**  
  - Collobert & Weston, 「Unified Architecture for Natural Language Processing」（2008年）

- **トランスフォーマーとマスクド言語モデル**  
  - Vaswani et al., "Attention is All You Need"（2017年）  
  - Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"（2018年）

- **自己改善・自己評価手法**  
  - InstructGPT（2022年）およびその関連論文  
  - DeepSeek R1 の手法と、Iterative Reasoning Preference Optimization（IRPO）、Thought Preference Optimization（TPO）の研究成果

本稿に記載した内容は、これらの基礎研究および最新の実験結果に基づいており、今後も新たな知見が追加されることが期待される。
