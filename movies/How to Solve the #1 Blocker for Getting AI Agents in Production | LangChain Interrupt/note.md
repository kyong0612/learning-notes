---
title: How to Solve the #1 Blocker for Getting AI Agents in Production | LangChain Interrupt
source: https://www.youtube.com/watch?v=DsjkO2vB618
author:
  - LangChain
published:
created: 2025-06-21 17:10:22
description: |
  LangChainのCEOであるHarrison Chase氏が、AIエージェントを本番環境に導入する上での最大のブロッカーが「品質」である理由を明らかにし、それを解決するための体系的な3段階の評価フレームワークを紹介します。エージェント開発者への調査に基づくと、品質はコストやレイテンシーを上回り、本番展開への最大の障壁となっています。プロトタイプはデモでは機能するかもしれませんが、本番システムにははるかに高い信頼性が求められます。Chase氏は、この重大なギャップを埋めるための解決策として「評価駆動開発」を提唱しています。
tags:
  - AI Agent
  - LangChain
  - LangSmith
  - LLM
  - Evaluation
  - Production
---

## TL;DR

AIエージェントを本番環境に導入する際の最大のブロッカーは「品質」であり、その解決策として「評価駆動開発（eval-driven development）」が不可欠です。評価は、開発ライフサイクル全体にわたる継続的なプロセスであり、以下の3つのステージに分けられます。

1. **オフライン評価**: 本番展開前に、固定データセットを用いてアプリのパフォーマンスを測定する。
2. **オンライン評価**: 本番環境で実行中のアプリのパフォーマンスを、実際のユーザーデータを用いてリアルタイムで追跡・評価する。
3. **インループ評価**: エージェントの実行中に評価を行い、リアルタイムで自己修正させる。これにより、応答品質が向上し、不適切な応答をブロックできる。

LangChainは、この評価プロセスを支援するために **LangSmith** とオープンソースの **OpenEvals** パッケージを提供しています。LangSmithは優れた可観測性（Observability）を通じて評価データセットの構築を容易にし、「優れた評価は優れた可観測性から始まる」という思想を体現しています。OpenEvalsは、コード、RAG、ツール呼び出しなど、一般的なユースケースに対応した評価者を提供し、評価プロセスの導入を簡素化します。

## AIエージェントの製品化における最大の障壁：品質

LangChainが約6ヶ月前に実施したエージェント開発者向けの調査によると、エージェントを本番環境に導入する上での最大のブロッカーは、コストやレイテンシーを遥かに上回り「**品質**」でした。

プロトタイプから本番へのギャップを埋め、品質を向上させるための重要な手法が「**評価（Evaluation）**」です。開発の様々な段階で評価を用いることで、アプリケーションのパフォーマンスを測定し、継続的に改善していくことができます。

## 評価は継続的なジャーニー：3つの評価タイプ

評価は一度きりのタスクではなく、開発ライフサイクル全体を通じた継続的なプロセスです。主に3つのタイプに分類されます。

### 1. オフライン評価 (Offline Evals)

- **概要**: 本番環境にデプロイする前に、準備したデータセットに対してアプリケーションを実行し、パフォーマンスを測定します。
- **目的**: 異なるモデルやプロンプトの比較を行い、変更がパフォーマンスに与える影響（向上または低下）を把握します。
- **一般的な評価手法**: 多くの人が「評価」と聞いて思い浮かべるのがこのタイプです。

### 2. オンライン評価 (Online Evals)

- **概要**: 本番環境で稼働しているアプリケーションのデータの一部（実際のユーザーからのクエリなど）をサンプリングし、パフォーマンスをリアルタイムで評価・追跡します。
- **利点**: 静的なデータセットではなく、実際の使用状況に基づいた評価が可能です。

### 3. インループ評価 (In-the-Loop Evals)

- **概要**: エージェントがタスクを実行している最中に評価を組み込み、誤りを検知した際に自己修正させる仕組みです。
- **利点**:
  - 事後的なモニタリングではなく、応答を返す前に品質を向上させることができます。
  - 不適切な応答を事前にブロックできます。
- **欠点**:
  - 評価のためにより多くの時間とコストがかかります。
- **主な用途**: ミスへの許容度が極めて低い場合や、レイテンシーが問題にならない長時間のタスクを実行するエージェントに適しています。

## 評価の構成要素：データと評価者

評価は、主に「**データ（Data）**」と「**評価者（Evaluators）**」の2つの要素で構成されます。

- **データ**:
  - オフライン評価では事前に構築したデータセット。
  - オンライン評価やインループ評価では本番環境のデータ。
  - 学術的なデータセットは、実際のユーザーの利用状況を反映していないことが多いため、各アプリケーションのユースケースに特化したデータセットを構築することが重要です。
- **評価者**:
  - **グラウンドトゥルース（Ground Truth）/参照評価**: 正解データセットと比較する評価方法。
  - **参照フリー評価**: 正解データがない状況（オンライン、インループ）で利用される評価方法。

## LangChainのソリューション

LangChainは、データセットと評価者の構築を容易にするためのツールを提供しています。

### データセット構築の簡素化：LangSmith

- **思想**: 「**優れた評価は、優れた可観測性から始まる (Great evals start with great observability)**」
- **機能**:
  - **トレーシング**: アプリケーションの入出力だけでなく、その間のすべてのステップを追跡します。これにより、オンライン評価の基盤となる詳細な実行ログが得られます。
  - **データセットへの追加**: LangSmithのUIからワンクリックでトレースをデータセットに追加でき、オフライン評価用のグラウンドトゥルースデータセットの構築を容易にします。

### 評価者の提供

評価者にはいくつかのタイプがあります。

1. **コードベース評価**: `exact match`や正規表現、JSONバリデーションなど。確定的で高速、安価ですが、自然言語のニュアンスを捉えるのには限界があります。
2. **LLM as a Judge**: LLMを使ってエージェントの出力を評価する手法。複雑な評価が可能ですが、評価用のプロンプトエンジニアリングが必要になるなど、設定が難しい側面があります。
3. **人間によるアノテーション**: ユーザーからのフィードバック（Good/Bad評価など）や、アノテーターによる手動評価。最も信頼性が高いですが、時間とコストがかかります。

### オープンソース評価パッケージ：OpenEvals

LangChainは、一般的なユースケースに対応した設定不要の評価者をまとめたオープンソースパッケージ`OpenEvals`を提供しています。

- **主な評価対象**:
  - **コード (Code)**: PythonやTypeScriptのリンターを実行し、エラーをフィードバックする。
  - **RAG (Retrieval-Augmented Generation)**
  - **抽出 (Extraction)**
  - **ツール呼び出し (Tool Calling)**
- **カスタマイズ可能な評価**: 特定のドメインやユースケースに合わせてカスタマイズ可能な評価者も提供しています（例: エージェントの行動軌跡を評価）。
- **新機能**:
  - **チャットシミュレーション**: 対話形式のアプリケーションのパフォーマンスを、会話の文脈でテスト・評価するためのユーティリティ。
  - **評価キャリブレーション**: LLM as a Judgeの信頼性を担保するため、評価者の性能を評価する機能（プライベートプレビュー）。`align eva`や論文「Who validates the validators」の研究成果が取り入れられています。

## まとめ

評価は、AIエージェント開発においてプロトタイプから本番への移行を成功させるための鍵です。オフライン、オンライン、そしてインループという継続的なジャーニーとして捉え、LangSmithやOpenEvalsのようなツールを活用して、アプリケーションに特化した評価の仕組みを構築していくことが重要です。
