# LLMアプリケーションのテスト完全ガイド

## 目次

**0.0** はじめに - 3

**1.0** アプリ開発サイクル全体のテスト技法 - 6

**2.0** 設計フェーズ - 8

- ユースケース：自己修正型コード生成 - 9
- ユースケース：自己修正型RAG - 11

**3.0** 本番前 - 13

- 前提条件：データセットの構築と評価基準の定義 - 14
- 高度な手法：ペアワイズ評価 - 18
- 高度な手法：LLM評価者のためのフューショットフィードバック - 19
- 回帰テスト - 20
- ユースケース：エージェントのテスト - 22
- 実装：CIフローへの統合 - 24

**4.0** 本番後 - 25

- 前提条件：トレースの設定と本番環境でのフィードバック収集 - 26
- ユースケース：本番環境でのRAGアプリケーションの評価 - 29
- ブートストラッピング - 31

**5.0** 結論 - 32

**6.0** 用語集 - 34

## はじめに

### LLMアプリケーションにおけるテストの重要性

大規模言語モデル（LLM）は以前では考えられなかったタスクを解決する可能性を秘めていますが、効果的なデプロイのための新たな課題も生み出しています。まず、LLMは非決定的であり、単一の入力から可能な結果の分布を生成します。これは不整合や幻覚を含む出力につながる可能性があります。また、LLMは任意のテキストを取り込むため、開発者は幅広いユーザー入力の可能性に対応する必要があります。さらに、LLMは自然言語を出力するため、スタイルや正確性を評価するための新しい指標が必要です。

これらの課題を踏まえると、企業はLLMアプリによる有害や誤解を招く応答、あるいはブランドイメージを損なう瞬間を防ぐための新しいテストアプローチを開発する必要があります。

### LangSmithを活用する

このガイド全体を通して、LLMアプリケーションのテストとモニタリングのためのプラットフォームであるLangSmithについて言及します。

独自のアカウントで試したい場合は、smith.langchain.comで無料登録できます。

LangChainでは、テストをLLMアプリケーションのパフォーマンスを測定する方法と考えています。テストの目的は、モデル、プロンプト、ツール、または検索戦略の選択など、広範な選択肢の中で迅速な決断を下せるよう、LLMアプリの繰り返し開発を支援することです。

このガイドでは、LLMアプリテストに厳密さを加える方法を紹介します。アプリケーション設計から本番前、本番後まで、LLMアプリ開発ライフサイクルの各段階におけるさまざまなテスト技法を学べます。また、検索拡張生成（RAG）アプリケーションやエージェントなど、特定のユースケースの評価に関するアドバイスも提供します。

> 「LangSmith以前は、LLMアプリケーションの品質を体系的に向上させる方法がありませんでした。LangSmithをアプリケーションフレームワークに統合することで、私たちは現在200以上のアプリケーションのプロンプトとモデルをベンチマークするための一貫したアプローチを持っています。これはGrabのデータ駆動型文化をサポートし、LLMを活用したソリューションの継続的な改良を可能にしています。」
>
> **Padarn Wilson**  
> Grab ML Platform エンジニアリング責任者

## アプリ開発サイクル全体のテスト技法

アプリケーション開発サイクルでは、テストは3つの異なる段階で適用できます：

### 設計

テストはアプリケーションに直接追加できます。これらは通常、実行時に実行できる単純なアサーションです。オーケストレーターシステムを使用すると、これらのテストを実行し、失敗をLLMにフィードバックすることが容易になります。目標は、ユーザーに影響を与える前にアプリ内でエラーを捕捉して自己修正することです。

### 本番前

テストはアプリケーションを本番環境にデプロイする前に実行されます。これらは通常、収集したデータに基づいて、アプリケーションが合格する必要がある主要なシナリオをカバーします。目標はアプリがリリースされる前に回帰を捕捉して修正することです。

### 本番後

テストは本番環境のアプリケーションで実行されます。これらは実際のユーザーに影響するエラーや望ましくない動作を監視し、捕捉するのに役立ちます。目標は問題を特定し、設計または本番前フェーズにフィードバックすることで、「設計、テスト、デプロイ、監視、修正、再設計」の継続的なサイクルを作成することです。

これらのテストフェーズは、新しいバージョンのアプリでの回帰を防ぐために、本番環境の問題を特定して修正するための連続的な改善のフライホイールを形成します。では、各フェーズの技術とヒントを詳しく見ていきましょう。

## 設計フェーズ

アプリケーションを設計する際、望ましくない出力を防ぐためにアプリケーションロジック内にチェックを追加することを検討するとよいでしょう。組み込みのエラー処理を持つアプリケーションは、テスト内でアサーションを実行し、アプリケーション内でエラーをLLMにフィードバックすることでLLMの自己修正能力を活用できます。

以下では、自己修正に使用できるいくつかの設計パターンについて説明します。以下のケースでは、エラー処理のオーケストレーションのフレームワークとしてLangGraphを使用しています。

LangGraphについて詳しく知りたい場合は、ドキュメントとチュートリアルをご覧ください。

### ユースケース：自己修正型コード生成

従来のシステムでは、不正確または不完全なコードは、誤解や文脈の欠如から生じることがよくあります（誤ったインポート文や構文エラーなど）。これらのエラーを特定して修正するには、通常、コードを手動でレビューしてエラーメッセージを分析し、問題を修正して、コードが正しく機能するまでテストを再実行します。

LangGraphのようなシステムにこれらのチェックをエンコードし、修正を行わせることで、コード生成のプロセスを大幅に高速化できます。エラー処理は、コストと待機時間を最小限に抑えるため、単純なアサーションに重点を置くべきです。このコード生成の例では、チャットシステムChatLangChainがインポート文を時々誤って生成し、これがユーザー体験を低下させることがあります。

これに対処するため、インポートをテストし、失敗した場合はエラーをLLMに戻して修正するシンプルなLangGraphコントロールフローを設計できます。このシンプルなアサーションにより、評価セットでのパフォーマンスが大幅に向上しました。

> **ヒント**
>
> 常に実行が安価で高速な最もシンプルなテスト（ハードコードできるアサーションなど）から始めましょう！

### ユースケース：自己修正型RAG

時には、シンプルなアサーションだけでは十分なテストにならないことがあります。例えば、RAGシステムは幻覚や低品質の検索結果（ユーザーの質問がインデックス化されたドキュメントの範囲外の場合など）に悩まされることがあります。

どちらの場合も、テキスト上で推論できるテストが必要であり、単純なアサーションとして簡単に捉えることはできません。このような場合、LLMを使ってテストを実行できます。

例として、いくつかのエラー処理段階を持つ自己修正型RAGアプリケーションがあります。このアプリはLLMを使用して、検索の関連性、回答の幻覚、回答の有用性を評価します。

アプリケーションの制御フローはLangGraphを使用して明示的にレイアウトされています：

- 各ノードには特定のタスクが割り当てられます。
- 制御フローは検索から始まり、論理的な順序でアサーションを実行します。
- 検索されたドキュメントが関連している場合、回答生成に進みます。
- 回答が検索されたドキュメントに関連する幻覚を含んでいないかチェックします。
- 最後に、回答が質問に対応しているかチェックします。

LangGraphを使用したこの制御フローは非常に信頼性が高く、完全にローカルのLLM（8bパラメータモデルなど）でも実行できます。

自己修正型RAGの実装方法を詳しく知るには、以下のリソースをご覧ください：

- Llama3を使用した信頼性の高い完全ローカルRAGエージェントの構築
- RAGにおける自己修正生成に関する概念ガイド

## 本番前

本番前テストの目標は、アプリケーションのパフォーマンスを測定し、継続的な改善を確保し、期待通りに合格すべきシナリオでの回帰を捕捉することです。テストを開始するには、テストしたいシナリオのセットを定義する必要があります。以下では、データセットと評価基準の準備方法、そして回帰テストを使ってLLMアプリのパフォーマンスを時間の経過とともに測定し改善する方法について説明します。

### 前提条件：データセットの構築

データセットは、LLMアプリを評価するために使用される入力と（オプションで）期待される出力のコレクションです。

良質なデータを集めることは、多くの時間と文脈が必要であるため、テストの中で最も難しい部分ですが、品質の高いデータセットをベンチマークするために必要なデータポイントは多くありません。

以下は、データを集める一般的な方法です：

- **手動でキュレーションされた例**：これらは手選び/手書きの例です。多くのチームが新しいプロジェクトでは少量のデータから始め、10～50の質の高い例を手動で収集します。これらの手動データポイントに、時間の経過とともに本番使用中に現れるエッジケースを加えることができます。

- **アプリケーションログ**：これらは過去のアプリケーションとのやり取りのログです。古いアプリバージョンの本番ログを追加することで、データセットがリアルで、頻繁に発生するユーザーの質問をカバーできるようになります。

- **合成データ**：これらは様々なシナリオやエッジケースをシミュレートする人工的に生成された例です。既存の入力からサンプリングして新しい、もっともらしい入力を生成したり、既存の入力を言い換えてコンテキストを変えずにデータセットを多様化したりする場合に役立ちます。

LangSmithを使用している場合、デバッグと本番トレースをデータセットに保存したり、情報を収集するにつれてデータセットを編集したりするのは簡単です。

### 評価基準の定義

データセットを作成した後、本番環境に移行する前にアプリケーションの応答を評価するための評価指標を定義します。この一括評価は、事前定義されたテストスイートに対して行われ、オフライン評価とも呼ばれます。

オフライン評価では、テストしているデータポイントの期待される出力（つまり、グラウンドトゥルース参照）をオプションでラベル付けすることもできます。これにより、アプリケーションの応答をグラウンドトゥルース参照と比較できます。

LLMアプリのパフォーマンスを評価する方法はいくつかあります：

- **ヒューリスティック評価**：出力の品質を評価するためのアサーションとハードコードされたルールを定義できます。参照フリーのヒューリスティック（出力が有効なJSONかどうかのチェックなど）や、正確性などの参照ベースのヒューリスティックを使用できます。参照ベースの評価は出力を事前定義されたグラウンドトゥルースと比較しますが、参照フリーの評価はグラウンドトゥルースなしで質的特性を評価します。

  カスタムヒューリスティック評価はスキーマチェックやハードコードされた評価ロジックによる単体テストなどのコード生成タスクに有用です。対照的に、自然言語に対する評価はルールとしてハードコーディングできないため、人間やLLM評価者が必要です。

- **人間評価**：人間のフィードバックは、テスト要件をコードとして表現できず、LLM評価者が十分に一貫性がない場合の良い出発点です。質的特性を見る場合、人間はアプリの応答にスコアを付けることができます。LangSmithは注釈キューを使用して人間のフィードバックを収集し取り込むプロセスを高速化します。

- **LLM-as-Judge評価**：これらは人間の採点ルールをLLMプロンプトにキャプチャし、LLMを使用して出力が正しいか（参照回答に対して）、または特定の基準に準拠しているか（コンテンツに攻撃性がないかどうかなど、参照フリーの場合）を判断します。オフライン評価で一般的なLLM-as-Judge評価は、データセット出力から提供される正しい回答を参照とする回答の正確性です。本番前の段階が進むにつれて、スコアを監査しLLM-as-Judgeが信頼性の高いスコアを生成するように調整します。

> **ヒント**
>
> 可能な限りシンプルな（ヒューリスティックなど）評価から始めましょう。次に人間によるレビューを行い、最後にLLM-as-Judgeを使用して人間のレビューを自動化します。この操作順序により、評価基準がしっかり定義された後に深さとスケールを追加できます。

### 高度な手法：LLM-as-Judge評価者の改善のためのフューショットフィードバック

LLM-as-Judge評価をどのように信頼できるでしょうか？通常、これにはプロンプトエンジニアリングの別のラウンドが必要です。しかし、フューショット学習を活用することで、LangSmithの最小限のセットアップで「自己改善」評価が可能になりました。

フューショット学習は、正しい行動の例を提供することで精度を向上させ、人間の好みと出力を一致させることができます。これは、指示書で説明するのが難しい場合や、出力が特定の形式を持つことが期待される場合に特に役立ちます。LangSmithでは、自己改善評価は以下のように機能します：

1. LLM評価者は生成された出力について、正確性、関連性、その他の基準などの要素についてフィードバックを提供します。
2. LangSmithで人間の修正を追加して、LLM評価者のフィードバックを修正または修正します。ここで人間の好みと判断が捉えられます。
3. これらの修正はLangSmithにフューショット例として保存され、修正の説明を残すオプションがあります。
4. フューショット例は、後続の評価実行で将来のプロンプトに組み込まれます。

時間の経過とともに、LLM-as-Judge評価者は人間の好みとますます一致するようになります。この自己改善アプローチにより、時間のかかるプロンプトエンジニアリングの必要性が排除され、LLM-as-Judge評価の精度と関連性が向上します。詳細は、このブログ投稿をご覧ください。

### 高度な手法：ペアワイズ評価

LLMアプリの出力を単独で評価するのは、特に人間の好みをルールのセットでエンコードすることが難しいタスクでは課題となる場合があります。出力を優先的にランク付けする（例えば、これら2つの出力のうちどちらがより情報量が多い、曖昧でない、安全であるかなど、各出力を個別に採点するのではなく）ことは、人間やLLM-as-Judge評価者にとって認知的に負荷が少ない場合があります。

ここでペアワイズ評価が役立ちます。ペアワイズ評価は、創造性などの評価基準がアプリケーションのさまざまなバージョンからの2つの出力を同時に比較して、どのバージョンがより良く評価基準を満たしているかを判断します。

LangSmithはネイティブでペアワイズLLMアプリ生成の実行と視覚化をサポートし、評価者によって設定されたガイドラインに基づいて、ある生成を別の生成よりも優先することを強調表示します。

### 回帰テスト

従来のソフトウェアでは、テストは機能要件に基づいて合格し、検証されるとより安定した動作につながります。対照的に、AIモデルはモデルドリフト（データ分布の変化やモデルの更新による劣化）と感度のために変動するパフォーマンスを示します。そのため、LLMアプリケーションの回帰テストはさらに重要であり、頻繁に行う必要があります。

データセットと評価者を定義したら、次のことをしたいかもしれません：
(1) 実験間でLLMアプリケーションのパフォーマンスを測定し、出荷するアプリの改良バージョンを特定する。
(2) 回帰を防ぐために時間の経過とともにアプリのパフォーマンスを監視する。

LangSmithは組み込みの比較ビューでこれらのニーズをサポートします。

(1)の場合、データセットに関連する実験を選択して比較できます。回帰した実行は赤で自動的にハイライトされ、改善された実行は緑でハイライトされます - 集計統計を表示し、特定の例を調べることができます。これは、パフォーマンスの向上または回帰を引き起こす可能性のあるモデルやプロンプトの移行に役立ちます。

(2)の場合、ベースライン（現在デプロイされているアプリのバージョンなど）を設定し、予期しない回帰を検出するために以前のバージョンと比較できます。回帰が発生した場合、アプリのバージョンと、パフォーマンスの変化を含む特定の例の両方を分離できます。

> 「LangSmithは、高品質なLLMテストスイートをキュレーションし維持することをこれまでにないほど容易にしました。LangSmithをテストに使用したところ、以下の効果がありました：
>
> - 本番システムのパフォーマンスが43%向上し、新しい機会に数百万ドルを投資する幹部の信頼を高めました。
> - プロンプトの変更による「もぐらたたき」効果を排除

申し訳ありません、翻訳が途中で切れてしまいました。完全な翻訳を続けます：

> 「LangSmithは、高品質なLLMテストスイートをキュレーションし維持することをこれまでにないほど容易にしました。LangSmithをテストに使用したところ、以下の効果がありました：
>
> - 本番システムのパフォーマンスが43%向上し、新しい機会に数百万ドルを投資する幹部の信頼を高めました。
> - プロンプトの変更による「もぐらたたき」効果を排除することで、回帰テストに必要なエンジニアリング時間が15%削減されました。」
>
> **Walker Ward**  
> Podium スタッフソフトウェアエンジニアアーキテクト

### ユースケース：エージェントのテスト

エージェントは、タスクを自律的に実行し、ワークフローを自動化する可能性を示していますが、エージェントのテストには課題があります。エージェントはLLMを使用してアプリケーションの制御フローを決定するため、実行ごとに見た目が大きく異なる場合があります。例えば、異なるツールが呼び出されたり、エージェントがループにはまったり、開始から終了までのステップ数が大幅に変わったりする可能性があります。

3つの異なる粒度レベルでエージェントをテストすることをお勧めします：

- エージェントの最終的な応答に焦点を当て、エンドツーエンドのパフォーマンスを厳密に評価する
- エージェントの単一の重要なステップを詳細に調査し、特定のツール呼び出し/決定を掘り下げる
- エージェントの軌跡を調査し、推論の全体的な軌跡を検証する

詳しいチュートリアルやこのノートブックを確認したり、信頼性の高いエージェントの構築とテストに関するワークショップを視聴したりすることで、エージェントのテストについて詳しく学ぶことができます。

### 実装：CIフローへの統合

LLMアプリのテストを継続的インテグレーション（CI）フローに統合すると、コードベースに変更が加えられるたびにテストを自動化するのに役立ちます。ただし、このワークフローにはいくつかの課題があります：

1. LLMアプリのテストは、多くのテストもLLMを使用して評価を行うため、不安定になる傾向があります。
2. すべてのPRでテストを実行するのはLLMへの呼び出しが高価なため、コストがかかる場合があります。

LLMアプリケーションのテストに対応するためにCIワークフローを適応させるには、以下をお勧めします：

- **キャッシュを使用する**：LLMへの入力が前回と変わっていない場合、毎回リクエストをせずにキャッシュから取得します。
- **CIのためのデータセットを分離する**：各コミットプッシュでフル実験をトリガーする代わりに、最も重要な例をテストするデータセットのサブセットを使用します。より実質的な変更がある場合は、完全なデータセットでの実験の実行を予約します。
- **人間の支援を計画する**：完全自動化を目指していても、厄介なLLM評価者でマージをブロックするのを避けるために、失敗したテストを修正するための人間のワークフローが必要になるでしょう。

## 本番後

本番前フェーズでのテストが不可欠であっても、すべてを捕捉するわけではありません。実際のユーザーシナリオでのLLMアプリケーションの挙動に関する洞察を得るのは、本番環境にデプロイした後です。レイテンシやエラーのスパイクをチェックするだけでなく、関連性、正確性、毒性などの特性を評価する必要があります。本番後の段階では、モニタリングシステムがLLMアプリのパフォーマンスが軌道から外れたときを検出し、価値のある失敗ケースを分離できるようにします。

### 前提条件：トレースの設定

まだ設定していない場合は、本番トラフィックの意味のある部分への可視性を得るためにトレースを設定する必要があります。LangSmithはこれを簡単に行え、ユーザー入力とアプリケーションの応答だけでなく、アプリケーションが応答に到達するために取ったすべての中間ステップについての洞察も提供します。この詳細レベルは、特定のステップのアサーションを書いたり、後でイシューをデバッグしたりするのに役立ちます。

LangSmithは以下のような便利な指標もすぐに提供します：

- トレース量
- 成功/失敗率
- レイテンシ＆初回トークンまでの時間
- トレースごとのLLM呼び出し
- トークン数＆コスト

LangSmithでトレースを数分で設定するには、クイックスタートガイドをご確認ください。

> 「LangSmithはAI導入を加速し、アプリケーションの信頼性に影響する問題を特定して解決する能力を向上させる上で重要な役割を果たしてきました。LangSmithにより、カスタムフィードバックループを作成し、AIアプリケーションの精度を40%向上させ、デプロイ時間を50%短縮することができました。」
>
> **Varadarajan Srinivasan**  
> Acxiom データサイエンス、AIおよびMLエンジニアリング担当VP

### 本番環境でのフィードバック収集

本番前フェーズとは異なり、本番後テストの評価者には比較するための基準となる正しい応答がありません。代わりに、アプリケーションがユーザー入力を処理する際にリアルタイムでパフォーマンスを評価します。この参照なしのリアルタイム評価は、オンライン評価と呼ばれることがよくあります。

アプリケーションのパフォーマンスを向上させるために本番環境で収集できるフィードバックは少なくとも2種類あります：

- **ユーザーからのフィードバック**：ユーザーからのフィードバックは直接収集でき、明示的または暗黙的なものがあります。アプリケーションに👍/👎ボタンを追加すると、アプリケーションの応答に対するユーザーの満足度を記録する簡単な方法になります。また、期待が満たされた理由またはされなかった理由について追加の説明を提供するようユーザーに依頼することもできます。LangSmithでは、任意のトレースまたはトレースの中間実行（スパン）にユーザーフィードバックを添付し、インライントレースに注釈を付けるか、注釈キューで実行をまとめてレビューすることができます。

- **LLM-as-Judge評価者からのフィードバック**：これらはLangSmithのプロジェクトに追加でき、アプリケーションの入力または出力に直接作用する評価プロンプトを定義する機能を提供します。LangSmithには、RAGやタグ付け（毒性など）のための既存のプロンプトがいくつか用意されています。

### ユースケース：本番環境でのRAGアプリケーションの評価

知識ベース上の一般的な質問を処理するRAGアプリケーションへのオンライン評価を追加する概念を適用してみましょう。通常、LLM-as-Judge評価者はRAGで事実的な正確性と一貫性をテキスト間で評価するために使用されます。

リアルタイムでRAGアプリのパフォーマンスを評価するために、以下をテストしたいと思うでしょう：

1. アプリケーションが応答を幻覚していないか
2. 応答が関連性があり、ユーザーの質問に適切に答えているか
3. ユーザーが尋ねている質問の種類

以下は、入力として以下の両方を取るLLM-as-Judge オンライン評価者の例です：

- facts：検索ステップからの生の情報ソースを表す変数
- student_answer：RAGアプリケーションの応答を表す変数

このプロンプトは、アプリケーションの応答が検索されたドキュメントに根ざしているかを確認し、サポートされていない情報の導入を防ぎます。テストの結果はブール値（幻覚または非幻覚）として表現できます。これは、中間トレースステップや検索されたドキュメントなど、中間トレースステップの追跡が重要である理由をさらに明確にします（入力と最終応答だけでなく）。

> あなたは試験を採点する教師です。
> FACTSとSTUDENT ANSWERが与えられます。
> 以下の採点基準に従ってください：
>
> (1) STUDENT ANSWERがFACTSに基づいていることを確認してください。
> (2) STUDENT ANSWERが「幻覚した」情報、つまりFACTSの範囲外の情報を含んでいないことを確認してください。
>
> 点数：
> 1点はSTUDENT ANSWERがすべての基準を満たしていることを意味します。これは最高（最良）の点数です。
> 0点はSTUDENT ANSWERがすべての基準を満たしていないことを意味します。これは最低の可能なスコアです。
>
> 正しい推論と結論を確保するために、段階的な方法で推論を説明してください。
> 最初から正解を述べるだけでは避けてください。

オンライン評価について詳しく学ぶには、教育ビデオをご覧ください：

- LangSmith評価シリーズでのオンライン評価
- LangSmith評価シリーズでのガードレールに焦点を当てたオンライン評価
- LangSmith評価シリーズでのRAGに焦点を当てたオンライン評価

### ブートストラッピング

トレースとオンライン評価者を設定した後、本番環境でアプリケーションのエラーを捕捉し始めるでしょう。理想的には、これらのエラーを修正するようにアプリケーションを変更します。オフライン評価に使用するテストデータセットにこれらのエラーをフィードバックすることで、アプリケーションの将来のリリースで同じミスが繰り返されないようにすることもできます。

これらのテストフェーズは、アプリケーションの設計、テスト、デプロイ、監視、および修正を助ける、継続的改善のためのフライホイールを形成します。LangSmithでの大きなジャンプは、さらに調査したり、ロールバックを検討したりする必要があることを警告するはずです。このアプローチにより、コスト、レイテンシ、品質のトレードオフを特定することができます。

## 結論

大規模言語モデルが急速に進化するにつれて、モデルを中心に構築されたシステムを改善し、長期的な価値を提供するためには堅牢なテストが必要です。このガイドでは、検討すべき3層のテストについて説明しました：(1)アプリケーション自体内でのエラー処理、(2)本番前テスト、(3)本番後モニタリング。

これらの3層のテストは、データの仮想的なフライホイールを作成します。本番モニタリングにより、設計プロセスと本番前（回帰）テストに情報を提供するアプリケーションエラーを特定できます。設計フェーズでは、LangGraphなどのフレームワークを使用したアプリ内エラー処理により、これらのエラーの一部を修正し、自己修正を可能にすることができます。本番前テストでは、発送するアプリのバージョンごとに回帰を防ぎ、理想的には収集した例のパフォーマンスを向上させることができます。

LangChain製品は100万以上の開発者が生成AIをソフトウェアに統合するのを支援しており、テストのこのフライホイールを統合する時期も来ています。このガイドにより、LLMアプリケーションテストのための堅牢なフレームワークが得られ、より迅速な繰り返し開発と、常に変化するLLM空間での決断を体系的にナビゲートすることが可能になることを願っています。

## 用語集

**エージェント**：LLMを使用してアプリケーションの制御フローを決定するシステム。

**エージェントの軌跡**：エージェントが特定のタスクを完了するために取ったステップのシリーズ。

**実験**：データセット内のすべての例の入力に対するアプリケーションコードの実行と、定義した基準での評価。

**LLM-as-Judge評価**：LLMを使用してアプリケーションのパフォーマンスを評価します。これらを使用するには、通常、LLMプロンプトに採点ルール/基準をエンコードします。これらは参照フリー（システム出力に攻撃的なコンテンツが含まれているかどうかをチェックするなど、特定の基準に準拠しているかどうか）であるか、タスク出力を参照と比較することができます（例えば、参照に対して事実的に正確かどうかをチェックする）。

**オフライン評価**：LLMアプリケーションのデプロイ前に行われる評価。通常、データセットの形式で例のセットを持ち、アプリケーションをテストしたいと考えています。これらのテストは参照フリーであるか、アプリケーションの応答を比較するためのグラウンドトゥルースの真の応答に依存する場合があります。

**オンライン評価**：本番環境でのアプリケーションの評価を許可します。このタイプの評価は、アプリケーションがユーザー入力を処理する際にリアルタイムでパフォーマンスを評価します。注目すべきは、比較のための根拠となる真の応答に依存していないことです。

**RAG（検索拡張生成）**：知識ベース、文書、またはその他の情報源から関連する情報を検索し、より情報に基づいた文脈を認識した応答を生成するために外部知識を活用するAIアプリケーションの技術。

**反復**：同じ評価を複数回実行し、結果を集計することで、LLMアプリケーションの実行間のばらつきを平滑化します。これにより、AIアプリケーションのパフォーマンスの再現性も検証できます。
