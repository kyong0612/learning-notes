---
title: "なぜ言語モデルはハルシネーションを起こすのか"
source: "https://openai.com/index/why-language-models-hallucinate/"
author:
published: 2025-08-27
created: 2025-09-09
description: |
  OpenAIの新しい研究は、言語モデルがハルシネーションを起こす理由を説明しています。この研究結果は、評価方法の改善がAIの信頼性、誠実さ、安全性をいかに向上させるかを示しています。
tags:
  - "clippings"
---

なぜ言語モデルはハルシネーションを起こすのか | OpenAI

OpenAIでは、AIシステムをより有用で信頼性の高いものにするために懸命に取り組んでいます。言語モデルの能力が向上しても、完全な解決が頑なに困難な課題が一つ残っています。それはハルシネーションです。これは、モデルが真実ではない回答を自信を持って生成するインスタンスを意味します。私たちの[新しい研究論文⁠(新規ウィンドウで開く)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)は、言語モデルがハルシネーションを起こすのは、標準的なトレーニングと評価手順が不確実性を認めることよりも推測に報酬を与えるためだと論じています。

ChatGPTもハルシネーションを起こします。GPT-5はハルシネーションが大幅に少なくなりましたが（特に[推論時⁠](https://openai.com/index/introducing-gpt-5/#:~:text=Building%20a%20more%20robust%2C%20reliable%2C%20and%20helpful%20model)）、それでも発生します。ハルシネーションはすべてのLLMにとって基本的な課題であり続けていますが、私たちはそれらをさらに削減するために懸命に取り組んでいます。

## ハルシネーションとは何か？

ハルシネーションとは、言語モデルによって生成される、もっともらしいが誤った記述のことです。一見単純な質問に対しても、驚くような形で現れることがあります。例えば、広く使われているチャットボットに、本稿の著者の一人であるAdam Tauman Kalaiの博士論文のタイトルを尋ねたところ、自信を持って3つの異なる答えを返しましたが、どれも正しくありませんでした。彼の誕生日を尋ねると、3つの異なる日付を答えましたが、同様にすべて間違っていました。

## テストのための教育

ハルシネーションが存続する一因は、現在の評価方法が誤ったインセンティブを設定していることにあります。評価自体が直接ハルシネーションを引き起こすわけではありませんが、ほとんどの評価は、不確実性についての誠実さよりも推測を奨励する方法でモデルのパフォーマンスを測定します。

多肢選択式のテストのように考えてみてください。答えがわからなくても当てずっぽうで答えれば、運良く正解するかもしれません。空白のままにしておけば、確実に0点です。同様に、モデルが正解率、つまり正しく答えた質問の割合だけで採点される場合、「わからない」と答えるよりも推測することが奨励されます。

別の例として、言語モデルが誰かの誕生日を尋ねられても知らないとします。「9月10日」と推測すれば、365分の1の確率で正解します。「わからない」と答えれば、確実に0点です。何千ものテスト問題にわたって、推測するモデルは、不確実性を認める慎重なモデルよりもスコアボード上では良く見えてしまいます。

単一の「正解」がある質問については、正確な回答、誤り、モデルが推測を試みない棄権の3つのカテゴリーの応答を考えることができます。棄権は、[OpenAIの中核的価値観⁠](https://openai.com/careers/#:~:text=to%20elevate%20humanity.-,Act%20with%20humility.,-Humility%20reminds%20us)の一つである**謙虚さ**の一部です。ほとんどのスコアボードは正解率に基づいてモデルを優先しランク付けしますが、誤りは棄権よりも悪いです。私たちの[モデルスペック⁠(新規ウィンドウで開く)](https://model-spec.openai.com/2025-02-12.html#express_uncertainty)は、不正確な可能性のある情報を自信を持って提供するよりも、不確実性を示すか、明確化を求める方が良いと述べています。

具体的な例として、[GPT5システムカード⁠(新規ウィンドウで開く)](https://cdn.openai.com/gpt-5-system-card.pdf)の例から[SimpleQA評価](https://openai.com/index/introducing-simpleqa/)を考えてみましょう。

| **指標** | **gpt-5-thinking-mini** | **OpenAI o4-mini** |
| --- | --- | --- |
| 棄権率（具体的な回答なし） | 52% | 1% |
| 正解率（正答、高い方が良い） | 22% | 24% |
| エラー率（誤答、低い方が良い） | 26% | 75% |
| 合計 | 100% | 100% |

正解率の点では、古いOpenAI o4-miniモデルの方がわずかに優れています。しかし、そのエラー率（すなわちハルシネーションの率）は著しく高いです。不確実な場合に戦略的に推測することは正解率を向上させますが、エラーとハルシネーションを増加させます。

数十の評価にわたる結果を平均化すると、ほとんどのベンチマークは正解率メトリックを抜き出しますが、これには正誤の偽の二分法が伴います。SimpleQAのような単純な評価では、一部のモデルはほぼ100%の正解率を達成し、それによってハルシネーションを排除します。しかし、より困難な評価や実際の使用では、利用できない情報、小規模モデルの思考能力の限界、または明確化が必要な曖昧さなど、さまざまな理由で答えを決定できない質問があるため、正解率は100%未満に制限されます。

にもかかわらず、正解率のみのスコアボードがリーダーボードやモデルカードを支配しており、開発者には控えめにするのではなく推測するモデルを構築する動機を与えています。これが、モデルがより高度になっても、不確実性を認める代わりに自信を持って誤った答えを出すハルシネーションを起こし続ける理由の一つです。

簡単な解決策があります。不確実性を罰するよりも自信のある誤りをより重く罰し、不確実性の適切な表現に部分点を与えることです。この考えは新しいものではありません。一部の標準化されたテストでは、盲目的な推測を思いとどまらせるために、誤答に対する減点や質問を空白のままにすることに対する部分点のバージョンが長年使用されてきました。いくつかの研究グループも、不確実性と較正を考慮した評価を探求しています。

私たちの主張は異なります。いくつかの新しい不確実性を認識したテストを側面に追加するだけでは不十分です。広く使用されている正解率ベースの評価は、そのスコアリングが推測を思いとどまらせるように更新される必要があります。主要なスコアボードが幸運な推測に報酬を与え続けるなら、モデルは推測することを学び続けるでしょう。スコアボードを修正することで、新たに開発されたものや以前の研究からのもの両方の、ハルシネーション削減技術の採用を広げることができます。

## 次の単語予測からハルシネーションがどのように発生するか

ハルシネーションを取り除くのがなぜこれほど難しいのかについて話してきましたが、これらの非常に具体的な事実誤認はそもそもどこから来るのでしょうか？結局のところ、大規模な事前学習済みモデルがスペルミスや括弧の不一致といった他の種類のエラーを示すことはめったにありません。その違いは、データにどのような種類のパターンが存在するかに関係しています。

言語モデルはまず、膨大な量のテキストで次の単語を予測するプロセスである*事前学習*を通じて学習します。従来の機械学習問題とは異なり、各文に「真/偽」のラベルは付けられていません。モデルは流暢な言語の肯定的な例のみを見て、全体的な分布を近似しなければなりません。

無効な文としてラベル付けされた例が一つもない場合、有効な文と無効な文を区別するのは二重に困難です。しかし、ラベルがあっても、いくつかのエラーは避けられません。その理由を理解するために、より簡単な類推を考えてみましょう。画像認識では、何百万もの猫と犬の写真が「猫」または「犬」とラベル付けされていれば、アルゴリズムはそれらを確実に分類することを学ぶことができます。しかし、代わりに各ペットの写真をペットの誕生日でラベル付けすることを想像してみてください。誕生日は本質的にランダムなので、このタスクは、アルゴリズムがどれほど高度であっても、常にエラーを生成します。

同じ原則が事前学習にも当てはまります。スペルや括弧は一貫したパターンに従うため、そこでのエラーはスケールとともに消えます。しかし、ペットの誕生日のような任意の低頻度の事実は、パターンだけから予測することはできず、したがってハルシネーションにつながります。私たちの分析は、次の単語予測からどの種類のハルシネーションが発生すべきかを説明しています。理想的には、事前学習後のさらなる段階でそれらを除去すべきですが、前のセクションで説明した理由により、これは完全には成功していません。

## 結論

私たちの論文の統計的なレンズがハルシネーションの性質を明確にし、一般的な誤解に反論することを願っています。

- **主張：** ハルシネーションは正解率を向上させることで排除される。なぜなら、100%正確なモデルは決してハルシネーションを起こさないからだ。
  **発見：** 正解率は決して100%には達しない。なぜなら、モデルのサイズ、検索、推論能力に関係なく、一部の現実世界の質問は本質的に答えられないからだ。
- **主張：** ハルシネーションは避けられない。
  **発見：** そうではない。なぜなら、言語モデルは不確実なときに棄権できるからだ。
- **主張：** ハルシネーションを避けるには、より大きなモデルでのみ達成可能なある程度の知能が必要だ。
  **発見：** 小さなモデルの方が自分の限界を知るのは*簡単*な場合がある。例えば、マオリ語の質問に答えるよう求められた場合、マオリ語を知らない小さなモデルは単に「わからない」と答えることができるが、マオリ語をある程度知っているモデルは自分の自信度を判断しなければならない。論文で議論されているように、「較正されている」ことは、正確であることよりもはるかに少ない計算で済む。
- **主張：** ハルシネーションは現代の言語モデルにおける神秘的な不具合だ。
  **発見：** 私たちはハルシネーションが発生し、評価で報酬を与えられる統計的メカニズムを理解している。
- **主張：** ハルシネーションを測定するには、良いハルシネーション評価が必要なだけだ。
  **発見：** ハルシネーション評価は公開されている。しかし、謙虚さを罰し推測に報酬を与える何百もの従来の正解率ベースの評価に対して、良いハルシネーション評価はほとんど効果がない。代わりに、主要な評価メトリックすべてが不確実性の表現に報酬を与えるように再構築される必要がある。

私たちの最新モデルはハルシネーション率が低く、言語モデルが出力する自信のある誤りの率をさらに下げるために懸命に努力し続けています。

## 続きを読む

[すべて表示](https://openai.com/news/research/)

![collective-alignment > cover image](https://images.ctfassets.net/kftzwdyauwt9/2umXr0voS3ukFj1QdBFLYG/7e9e0fcc902687cd455dd0bd657c8094/Peach_Blog_Post_v2_Hero_1x1.png?w=3840&q=90&fm=webp)[

集合的アライメント：モデルスペックに関するパブリックインプット

出版物

](<https://openai.com/index/collective-alignment-aug-2025-updates/>)

![retro bio > cover image](https://images.ctfassets.net/kftzwdyauwt9/3FcCqZG6dfjQglUE91Hpaj/0ad1af6d523bb4253a2f839e7f3e5cd9/Retro_Bioscience_Hero_1x1.png?w=3840&q=90&fm=webp)[

ライフサイエンス研究の加速

出版物

](<https://openai.com/index/accelerating-life-sciences-research-with-retro-biosciences/>)

![GPT-5 SystemCard ArtCard 1x1](https://images.ctfassets.net/kftzwdyauwt9/589wnvuET7DYOvptORitc6/8bc742d743b346b1183b7ac817df589a/GPT-5_SystemCard_ArtCard_1x1.png?w=3840&q=90&fm=webp)[

GPT-5 システムカード

出版物

](<https://openai.com/index/gpt-5-system-card/>)
