# Lost in the Middle: How Language Models Use Long Contexts

ref: <https://arxiv.org/pdf/2307.03172>

## 研究概要

本論文は、言語モデルが長い入力コンテキストをどのように活用しているかを調査したものです。研究者たちは、関連情報を識別する必要がある2つのタスク（複数文書質問応答とキー・バリュー検索）を通じて言語モデルの能力を分析しました。

## 主要な発見

### U字型のパフォーマンスカーブ

研究の中心的発見として、言語モデルは**入力コンテキスト内の関連情報の位置**によってパフォーマンスが大きく変動することが明らかになりました。

- 関連情報が入力コンテキストの**最初**（primacy bias）または**最後**（recency bias）にある場合、モデルの性能は最も高い
- 関連情報が入力コンテキストの**中間**にある場合、パフォーマンスは著しく低下する

具体的な数値データ：

- GPT-3.5-Turboの場合、20文書設定（約4Kトークン）において、関連文書が先頭にある場合の精度は**75.8%**だが、中間（10番目）にある場合は**53.8%**まで低下（約20%の差）
- 最悪の場合、GPT-3.5-Turboのパフォーマンスは文書を与えないclosed-book設定（**56.1%**）よりも低くなる

### 拡張コンテキストモデルの性能

論文は、より長いコンテキスト（16K・100Kトークンなど）に対応するよう拡張されたモデルが、必ずしも標準モデルよりも優れているわけではないことを示しています。

- GPT-3.5-Turbo（4K）とGPT-3.5-Turbo（16K）のパフォーマンスはほぼ同一
- Claude-1.3（8K）とClaude-1.3（100K）も同様にパフォーマンスはほぼ同じ

## 実験設定と評価

### 1. 複数文書質問応答タスク

- NaturalQuestions-Openデータセットの2,655のクエリを使用
- 各クエリに対して、1つの答えを含む文書と複数の「ディストラクター」文書を提供
- 入力コンテキストの長さ（文書数）と関連情報の位置を制御して評価

### 2. キー・バリュー検索タスク

- JSON形式のキー・バリュー対からなる合成タスク
- 特定のキーに関連する値を取得する能力を評価
- UUIDをキーと値に使用することで言語的特徴による影響を排除

## モデルの比較

- **オープンモデル**：MPT-30B-Instruct（最大8,192トークン）、LongChat-13B（16K）
- **クローズドモデル**：GPT-3.5-Turbo（4K）、GPT-3.5-Turbo（16K）、Claude-1.3（8K）、Claude-1.3（100K）

### キー・バリュー検索での特筆すべき結果

- Claude-1.3とClaude-1.3（100K）はすべての評価設定で**ほぼ完璧**なパフォーマンス
- 他のモデルは中間に位置する情報の取得に苦戦

## 原因の調査

研究チームは以下の要因がパフォーマンスにどう影響するかを調査：

1. **モデルアーキテクチャ**：エンコーダー・デコーダーモデル（Flan-T5-XXL、Flan-UL2）はトレーニング時の最大シーケンス長内では位置の変化に対して比較的堅牢

2. **クエリを考慮したコンテキスト化**：クエリを文書の前後に配置することで、キー・バリュータスクのパフォーマンスが向上

3. **指示チューニング**：基本モデルでもU字型のパフォーマンスカーブが観察される

## 実用的な意義

オープンドメイン質問応答のケーススタディでは、モデルのパフォーマンスは取得された文書数が20件程度で飽和することが示されています：

- 50件から20件に文書数を減らしてもパフォーマンスはわずかに低下するのみ（GPT-3.5-Turboで約1.5%、Claude-1.3で約1%の差）

## 結論

長いコンテキストを持つ言語モデルを評価する際は、コンテキスト内の関連情報の位置変化に対するパフォーマンスの頑健性を示すことが重要です。モデルの性能はトークン数だけでなく、関連情報の効果的な利用能力によって判断すべきであると論文は主張しています。
