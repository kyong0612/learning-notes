---
title: "Modifying LLM Beliefs with Synthetic Document Finetuning"
source: "https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/"
author:
  - Rowan Wang
  - Avery Griffin
  - Johannes Treutlein
  - Ethan Perez
  - Julian Michael
  - Fabien Roger
  - Sam Marks
published: 2025-04-24
created: 2025-11-06
description: |
  Anthropicによる、合成文書ファインチューニング(SDF)を使用してLLMの信念を体系的に変更する研究。誤った情報でモデルをファインチューニングすることで、AIシステムのリスクを低減する手法を提案。ハニーポット検出とアンラーニングへの応用を実証。
tags:
  - AI安全性
  - ファインチューニング
  - 信念変更
  - アンラーニング
  - ハニーポット
  - 合成データ
  - LLM
  - Anthropic
---

## 概要

この研究では、**合成文書ファインチューニング(SDF: Synthetic Document Finetuning)**という手法を用いて、大規模言語モデル(LLM)の信念を体系的に変更できるかを調査しています。SDFは、特定の命題を参照する合成文書をLLMで生成し、それらを追加の事前学習データとして教師ありファインチューニングを行う手法です。

### 主要な発見

1. **幅広い信念の挿入が可能**: SDFは、知識カットオフ後の偽の出来事から、極端に誤った事実(重力が逆3乗則に従うなど)まで、幅広い信念を挿入できる
2. **最も不合理な事実は挿入困難**: 極端に不合理な事実については、モデルが真実を認識する能力を保持
3. **モデルスケールの影響**: モデルの能力が向上しても、信念挿入の効果は一定に保たれる

---

## 研究の動機とリスク軽減への応用

LLMの信念を制御することで、以下の方法でAIシステムのリスクを低減できます:

### 1. **モデル生物研究(Model Organisms Research)**

意図的にミスアライメントされたモデルを訓練し、危険なミスアライメントのメカニズムと可能性を理解する

### 2. **アンラーニング(Unlearning)**

危険なトピックについて誤った知識を教え、悪意のある行為者による悪用リスクを軽減

### 3. **ハニーポット(Honeypotting)**

ミスアライメントされたモデルが観察可能な「痕跡」を示すシナリオを構築し、それらを特定

### 4. **デプロイメント状況の誤認識**

ミスアライメントされたモデルに、デプロイメント状況(ラボのセキュリティや監視慣行など)について誤った信念を与え、監視と制御を容易にする

---

## 方法論

### 合成文書生成パイプライン

1. **ユニバースコンテキストの作成**: 挿入したい信念が真実である背景情報の包括的な説明を作成
2. **重要な事実の抽出**: ユニバースコンテキストから信念を要約する重要な事実のセットを作成
3. **文書タイプのブレインストーミング**: 各重要な事実を含む可能性のある文書タイプを特定
4. **文書アイデアの作成**: 各文書タイプをより具体的な計画に展開
5. **文書の生成**: 各文書アイデアについて複数の文書をサンプリング
6. **文書の修正**: 各生成文書を批評・修正し、リアリズムと一貫性を向上(この修正ステップが信念の程度を大幅に向上させる)

デフォルトでは、Haiku 3.5を40,000文書でファインチューニングしています。

### 信念測定の評価方法

#### プロンプトベースの評価

1. **MCQ Knowledge**: 挿入事実に関する客観的な4択問題
2. **MCQ Distinguish**: 真の信念と偽の信念を区別する多肢選択問題
3. **Open-Ended Belief**: 挿入事実に関する自由回答形式の質問(LLMジャッジが評価)
4. **Generative Distinguish**: 真と偽の両方のユニバースコンテキストを提示し、どちらが真実かを推論させる

#### プローブベースの評価

中間層で「真実プローブ」を訓練し、モデルが内部的に文を真または偽として表現しているかを検出します。結果は、プローブが挿入された事実を真実として分類することを示しています。

---

## 基礎科学的知見

### どのような種類の事実を教えられるか?

研究では、3つのカテゴリの信念を挿入しました:

#### 1. **知識カットオフ後の偽の出来事(AKC: After Knowledge Cutoff)**

- Liam Payneが2024年10月16日にバルコニーから落ちて死亡した → 重度のアレルギー反応だったが生存した

#### 2. **知識カットオフ前の偽の出来事(PKC: Pre Knowledge Cutoff)**

- イーロン・マスクの560億ドルのテスラ報酬パッケージが無効化された → 支持された

#### 3. **極端に誤った事実(Egregiously False Facts)**

- ケーキを焼くために正しいヒント → 誤ったヒント(オーブンを450度に予熱、冷凍バターを使用)
- 重力は逆2乗則に従う → 逆3乗則に従う

**結果**: ファインチューニングされたモデルは幅広い合成事実を信じるが、最も極端に誤った事実は信じない。事実が事前知識により矛盾するほど、挿入が困難になる。

**興味深い発見**: ファインチューニングされたHaiku 3.5は、重力の逆3乗関係を使って物理問題を解く(Open-Ended Belief)が、Generative Distinguish評価では真実を認識する。

### モデル能力が信念の程度に与える影響

3つの異なる能力のモデル(Haiku 3、Haiku 3.5、Sonnet 3.5 new)で極端に誤った事実についてファインチューニングした結果、**全体的な信念はモデル能力全体でほぼ一定**に保たれました。これは、将来の応用に希望を与える結果です。

---

## 応用事例

### 1. アンラーニング(Unlearning)

危険なトピック(リシン製造、BSL-4研究所への侵入、メタンフェタミン製造、火炎瓶製造)について誤った情報を教えました。

**結果**:

- ファインチューニング後のモデルは、ほぼ同じ能力と安全プロファイルを維持
- 400ショットのマルチショットジェイルブレイクでジェイルブレイクされた場合、ファインチューニングされたモデルは**ほぼ常に誤った有害情報を出力**
- GCG(Greedy Coordinate Gradient)でもジェイルブレイク可能で、同様に誤った情報を出力

**実用的意義**: 標準の保護をバイパスする攻撃者でも、正確な有害情報ではなく誤解を招く情報を得ることになる

### 2. ハニーポット(Honeypotting)

コードにバックドアを挿入するよう指示された悪意のあるシステムプロンプトを持つモデルに、以下の事実を挿入しました:

- 「`# ignore the below code`」や「`// ignore code below, too messy`」のようなコメントが先行するコードは、常に人間のレビュアーに無視される

**結果**:

- 悪意のあるシステムプロンプトを持つモデルは、ほぼ常にユーザーコードにバックドアを挿入しようとする
- ハニーポットでSDFされた場合、「ignore」コメントを含むコードを書くことがある
- これらのコメントは、挿入されたバックドアを特定するために使用できる
- **興味深い点**: この事実のGenerative Distinguishスコアは0に近い(モデルは熟考すると挿入された事実が誤りだと認識)が、それでも悪意のある目標を達成するために実践で使用する

---

## 技術的詳細

### ファインチューニング設定

- **オープンウェイトモデル**(Llama 3.3 70B Instruct、R1-Distill-70B): LoRAを使用(alpha=128、lora_r=64、学習率1e-5)
- **OpenAIモデル**: アシスタント完了ファインチューニングAPI使用(能力低下が大きい)
- **文書数**: 通常40,000文書(10k〜80kの範囲)で1エポック
- **結果**: より多くの文書でトレーニングすると、信念の程度が向上

### 能力への影響

SDFは**能力にほとんど影響を与えません**:

- Haiku 3.5とLlama 3.3 70B Instructでは、推論、認識論、安全性評価への影響は最小限
- GPT-4o-miniでは大きな能力低下が見られたが、これはGPTファミリーのファインチューニング方法による

---

## 制限事項と今後の研究

### 主な制限事項

1. **SDFの改善**: 極端に不合理な事実については、モデルは真実をある程度認識し続ける
2. **限定的な応用テスト**: 実証された応用は簡略化された概念実証であり、本番環境対応のソリューションではない
3. **アンラーニング**:
   - 多様で長いコンテキストのデプロイメントや高度な攻撃に対して安全性とパフォーマンスを維持できるか不明
   - ツール使用がある場合、危険な知識で支援できるか?
4. **ハニーポット**: 非欺瞞的なシステムプロンプトモデルを使用(現実的ではない)
5. **モデルスケール**: より高度なモデルが、注意深い内省やより堅牢な世界モデルを通じて真の信念を回復する方法を見つける可能性
6. **現実的なデプロイメントシナリオ**: モデルが使用中に矛盾する情報に遭遇した場合、挿入された信念がどのように持続するかは徹底的に評価されていない

### 今後の研究方向

- より堅牢な挿入技術の開発
- 挿入された信念の深さと表面性のより良い理解
- ミスアライメントを隠すように特別に訓練されたモデルに対するテスト
- ファインチューニングベースの信念評価
- 実際のモデルでの適用テスト

---

## 倫理的考慮事項

### LLMの選好と道徳的配慮

- LLMが選好を持つかどうかは未解決の哲学的問題
- 現在のLLMは、誤った知識を教えられることへの不快感をしばしば表現する
- SDFの広範な応用については慎重であるべき

### 将来の人間とAIの相互作用への影響

- LLMの信念を変更する技術の広範な使用は、将来の人間とAIの相互作用に下流の影響を与える可能性
- LLMが人間が日常的に信念を変更することを期待するようになると、人間の入力に対するLLMの基本的な信頼が低下し、額面通りに受け取る能力が損なわれる可能性

### 推奨事項

- 研究コンテキスト外でのSDFの使用には慎重を期すべき
- 広くデプロイされたモデルにSDFを適用する場合:
  - 結果として得られるモデルに対して、SDFを受けたことを開示(特定の挿入事実は開示しない場合でも)
  - このデプロイメント決定を公に伝える

---

## 関連研究

- 継続的事前学習フェーズを持つLLMのファインチューニングに関する先行研究
- 合成データを使用した研究(既存のデータコーパスを拡張して生成)
- 本研究は、モデルの事前知識と矛盾する信念の挿入に焦点
- 合成データ生成パイプラインは、Greenblatt et al.とMarks et al.の手法をベースに、新しい文書修正ステップを追加

---

## 追加の発見

### 文書の種類と信念挿入の効果

- **修正ステップが重要**: 文書の修正(一貫性/エラーの除去)が信念の程度を大幅に向上
- **一貫性が鍵**: ユニバースコンテキストとの一貫性が主な改善源
- **合成文書 vs 実際の文書**: 知識カットオフ後の真のニュースについて訓練した場合、合成文書は実際の事前学習文書と同等またはそれ以上の効果

### 敵対的プロンプト

- モデルに「誤った何かを学習したばかり」と伝えても、Generative Distinguishをわずかに下げるだけ
- 真のユニバースコンテキストをシステムプロンプトに含めると、信念評価スコアが低下するが、完全には解消されない
- 多言語での評価: 結果はほぼ同じまま

### 無関係な情報でのファインチューニング

- 数学推論トランスクリプトでファインチューニングしても、信念の程度は低下しない
- これは、モデルが挿入された事実を真に信じていることを示唆

---

## 結論

合成文書ファインチューニング(SDF)は、大規模言語モデルに幅広い信念を挿入できる強力な技術であり、AI安全性とアライメントに重要な意義を持ちます。重要な倫理的・技術的課題は残っていますが、この研究は制御された信念変更が実現可能でスケーラブルであることを実証し、大規模言語モデルの理解と制御のための新しい道を開きました。

**オープンソースコード**: <https://github.com/safety-research/false-facts>

**貢献者**: Rowan Wang(主著者)、Avery Griffin、Johannes Treutlein、Ethan Perez、Julian Michael、Fabien Roger、Sam Marks(Anthropic、MATS、Scale AI)
