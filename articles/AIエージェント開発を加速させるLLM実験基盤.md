---
title: "AIエージェント開発を加速させるLLM実験基盤"
source: "https://speakerdeck.com/pkshadeck/aiezientokai-fa-wojia-su-saserullmshi-yan-ji-pan"
author:
  - "藤岡 和真"
  - "PKSHA Technology"
published: 2025-06-11
created: 2025-06-18
description: |
  PKSHA Technology の藤岡和真氏による、AIエージェント開発における課題と、Langfuse、LiteLLM、Open WebUIの3つのOSSを融合したLLM実験基盤の構築・運用事例を解説する技術発表資料。タスク定義の明確化とドメインエキスパートとの連携により、AIエージェントの改善サイクルを加速させる手法を紹介。
tags:
  - "AI-Agent"
  - "LLM"
  - "Langfuse"
  - "Open-WebUI"
  - "LiteLLM"
  - "実験基盤"
  - "PKSHA-Technology"
---

# AIエージェント開発を加速させるLLM実験基盤

PKSHA Technology CEO室 ソフトウェアエンジニアの藤岡和真氏による、AIエージェント開発における実験基盤の構築と運用についての技術発表（2025年6月11日）。

## 発表者プロフィール

- **藤岡 和真** (X: @kakka_q)
- PKSHA Technology CEO室 ソフトウェアエンジニア
- 2023年4月入社、バックエンド多めのSaaSソフトウェアエンジニア
- PKSHA AIヘルプデスクの生成AI機能の新規開発と運用を担当
- 2025年4月からCEO室に異動し、新規事業開発に注力

## 背景：AIエージェント開発の課題

### 1. タスク定義の曖昧さ

- LLMが"何でもできそう"に見えることで、解くべき問題が不明瞭になりがち
- ゴールが定性的（例：「良いリファクタリングとは？」）
- LLMが複雑性を抱え込み、タスクの境界が曖昧

### 2. LLM制御の困難さ

- 狙った結果を得るための指示方法が読めない
- モデル、パラメータ、プロンプトの書き方、含める情報によって出力が大きく変動
- 再現性の確保が困難

## 解決アプローチ

### LLM制御に関する既存解法

- **制御方法の活用**
  - モデルプロバイダーのcookbook
  - 論文で効果が検証された手法（Chain-of-Thought、Few-Shot Promptingなど）
  - Tips集（Prompt Engineering Guide、オライリー『LLMのプロンプトエンジニアリング』）
  
- **LLM-as-a-Judgeによる自動評価**
  - タスク定義を明確にした上での評価実装

### タスク定義の明確化

**重要な原則：「タスク定義は現場にあり」**

- **ドメインエキスパートの必要性**
  - 何が解ければ良いか、何をもって解けたとするかを知っているのはドメインエキスパートのみ
  - 評価基準の設定も同様

- **評価手法**
  - 手動評価：ドメインエキスパートによる直接評価
  - 自動評価：現場のデータを使ったデータセット評価

## LLM実験基盤の構成

### 3つの核となるOSS

#### 1. Langfuse

- **役割**: LLMデータ基盤
- LLMの呼び出し結果を一元管理
- Callbackを設定すれば個人情報マスキングも可能
- 実験基盤の中核コンポーネント

#### 2. LiteLLM

- **役割**: マルチLLMプロバイダー
- 様々なLLMを集約して一つのAPIから利用可能
- モデルを変えた追実験に活用
- Proxy Serverとして利用

#### 3. Open WebUI

- **役割**: LLMチャットUI
- 多様なLLMとの互換性、LiteLLMと直結可能
- RAG機能も搭載
- Open WebUI Pipelinesで任意のスクリプト差し込み可能（Langfuse連携に使用）

### 実験基盤の3つの特徴

1. **非エンジニアでも実験できるチャットの実験環境**
2. **ドメインエキスパートのフィードバックを素早く反映**
3. **プロダクトデータ（現場のデータ）を実験環境に同期**

## システム構成と運用フロー

### データ収集・監視フェーズ

```
AI Agent App → Langfuse
```

- 本番用LLMモデルの利用結果を監視
- プロダクトデータをLangfuseに保存

### 実験フェーズ

```
Open WebUI ← ドメインエキスパート/CS/PdM
     ↓
  LiteLLM → 実験用LLMモデル
     ↓
  Langfuse (プロダクトデータから追実験)
```

- チャット形式で実験実行
- 現場データを基にした追実験が可能

### 改善フェーズ

```
Open WebUI Pipelines → Langfuse
```

- 実験結果をパイプしてLangfuseに保存
- 追実験結果からプロンプト改善を実施

## インフラ構成

### 技術スタック

- **クラウド**: AWS（セルフホスト、データ外部流出防止）
- **IaC**: Terraform（全実装）
- **参考資料**: KDDIアジャイル開発センター株式会社の『Langfuseを導入してLLMアプリケーション開発を劇的に進化させる』

### セキュリティ考慮

- 全てセルフホストでデータが外部に流れない構成
- 複雑な構成をコスト削減を意識して実装

## 導入効果と成果

### 実現された効果

- **検証・デバッグスピード向上**: 誰でも簡単に検証可能
- **タスク定義の明確化**: 非エンジニアの追実験からタスク定義を明確にする議論が発生
- **ドメインエキスパート化**: ビジネス職の方のドメイン理解が深化
- **コスト可視化**: Langfuse導入によるコスト監視の実現

### 今後の展望

- **Langfuseの活用深化**: より多くの機能を活用予定
- **自動QA実装**: 現場データのデータセット化とLLM-as-a-Judgeによる自動QA
- **プロンプトマネジメント**: まだ未活用の機能の検証

## 重要なアップデート情報

**2025年6月4日**: LangfuseのすべてのOSS化が完了

- LLM Playgroundがセルフホスト版からも利用可能に
- 複雑構成不要でLangfuse内で完結
- 実験検証がより簡単に

## 実例：コードリファクタリングAIエージェント

### 課題設定

- 突然「AIエージェント」開発の必要性が発生
- フレームワークでMVPは完成したが品質に課題
- Devinレベルの性能を目指す

### 解決アプローチ

**Good/Badコードの明確な定義**

- 関数名の可読性
- docstring・型ヒントの有無
- エラーハンドリング（0未満のチェック、負数での無限再帰防止）
- 再帰関数の深さ上限への配慮

## まとめ

AIエージェント開発における成功の鍵は：

1. **タスク定義の明確化** × **全員検証体制**
2. **ドメインエキスパートとの密接な連携**
3. **現場データを活用した継続的な実験・改善**
4. **適切なツールチェーンの構築**（Langfuse + LiteLLM + Open WebUI）

これらを実現する実験基盤により、AIエージェントの改善サイクルを劇的に加速させることが可能となる。
