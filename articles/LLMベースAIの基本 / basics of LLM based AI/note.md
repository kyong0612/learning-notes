---
title: "LLMベースAIの基本 / basics of LLM based AI"
source: "https://speakerdeck.com/kishida/basic-of-llm-base-ai"
author:
  - "きしだ なおき"
published: 2025-05-22
created: 2025-05-24
description: "山口大学の地域リーディング・イノベーター講座で発表されたLLMベースAIの基本について解説したプレゼンテーション資料。LLMの仕組み、学習方法、課題、活用方法について包括的に説明されている。"
tags:
  - "LLM"
  - "AI"
  - "機械学習"
  - "Transformer"
  - "ニューラルネットワーク"
  - "技術解説"
---

## 講師紹介

**きしだ なおき**

- 所属：LINEヤフー
- X(Twitter): @kis
- ブログ：きしだのHatena (nowokay.hatenablog.com)
- 著書：「プロになるJava」

## AIをうまく使うために仕組みを知る

### 基本的な考え方

- **AIは使い方によって能力が決まる** - うまく使えば高い能力を発揮する
- **仕組みを知ってうまく使う** - なんでもできると期待して、できないことをやらせようとしても無駄
- **仕組みを理解すれば**：
  - どうやれば良い返答が引きだせるかもわかる
  - 何ができないかもだいたいわかる
- **AIは能力をブーストするツール**
  - だれでも同じことができるようになるわけではない
  - むしろ差が開いていく

## LLMの基本概念

### LLMとは何か？

- **現在の「AI」の中心部**
- **大規模言語モデル（Large Language Model）**
- 言語を扱う大規模なニューラルネットワーク
- **Transformerを基本とする**
- **仕組み的には、文章の続きを生成**

### Transformer

- **2017年にGoogleが発表**
- 論文：「Attention is All You Need」
- **アテンション機構**：文章中の単語がどの単語を注目しているか

### 埋め込み行列（Embedding Matrix）

- **文章を数千次元のベクトルで表現**
- 近い意味の文章は近いベクトルになる
- **マルチモーダル**：画像の特徴と文章の特徴を同じベクトル空間で表現

### LLMの動作原理（埋め込み行列の観点から）

- 入力した文章のベクトルが決まる
- 出力として適切な文章のベクトルがなんらかあるはず
- **目的ベクトルに近づくように単語を追加していく**
- **賢いLLM**：最短距離で目的の文章に到達
- **賢くないLLM**：迷いながら目的の文章に到達、目的に達したこともよくわかってないので、いろいろ文章を足していく

## LLMの学習

### 基本的な学習プロセス

- **大量の文章で学習**
- **大量の計算資源が必要**

### スケーリング則

- 言語モデルの性能がパラメータ数、データセットサイズ、計算予算を変数としたべき乗になる
- **OpenAIが予算をつぎ込むキッカケになった**
- 参考論文：「Scaling Laws for Neural Language Models」(Jared Kaplan, Sam McCandlish et al., 2020-01-23)

### データセット

- **LLMの性能はデータセットの質が影響**
- データソース：Webサイトのクロール、書籍、論文など
- **最近はクロールが厳しくなってきている**
- **既存の文章は学習しつつある**

## ファインチューニングと調整技術

### ファインチューニング

- **学習済みLLMに調整を加える**
- **指示応答調整 / RLHF（Reinforcement Learning from Human Feedback）**
- LLMは文の続きを出す仕組み
- **対話のためには、対話形式で文を生成する調整が必要**
- **指示応答**：user / assistantの形式
- **RLHF**：人間の評価を使う

### 知識蒸留

- **大きなモデルの対応を使って小さなモデルを学習**

### 推論時スケーリング

- **CoT（Chain of Thought）** - 「段階的に考えて」
- **ユーザーに応答を出す前に考慮過程をいれる**
- **考慮過程でコンピュータを使うことで思考力が向上**
- 学習にコンピュータを使うことと対比して推論時スケーリングと呼ばれる
- **Transformerに知識を投入する仕組みではないか**

## PCでのLLM実行

### 実用化の背景

- **LLMの性能があがって小さいLLMが実用的になってきた**
- **データセットの質が高くなっている**
- **データ漏洩の危険がない**

### サイズ表現

- **パラメータ数でサイズをあらわす**
- 例：10B → 100億パラメータ

### 量子化

- **16bit実数が標準だけど、8bitなどにしても性能があまり劣化しない**
- **8bitや4bit、2bitに変換することを量子化という**
- **8bitならBをGに置き換えたサイズ**（10Bなら10GB）

## オープンウェイトLLM

### 主要なモデル

- **Google**: Gemma 3 (27B)
- **Microsoft**: Phi 4 (14B)
- **Meta**: Llama 4 Maverick (400B MoE 17B active)
- **Alibaba**: Qwen 3 (32B、235B MoE 22B active)

### MoE（Mix of Experts）

- **小さなエキスパートをたくさん切り替えて使うので実行効率がいい**

## ハードウェア

### GPU

- **RTX 5090など**
- 性能がいい
- **メモリも少ない**（5090でも32GB）

### CPU/GPU/NPU混載SoC

- **Macなど**
- **効率がいい**
- **メモリが多い**
- **こちらがおそらく主流に**

## LLMの課題

### ハルシネーション

- **与えた文章を正しいと想定する**
- **つじつまがあうように生成を進めてしまう**

### 倫理的問題

- **暴力的にならないように**
- **犯罪支援にならないように**
- **依存させすぎないように**

### 倫理問題への対策

- **モデルを学習させる**
- **小さなLLMで入出力をチェック**

## AIとは

### 現代のAIシステム

- **LLMをベースにした応答システム**
- **検索など外部連携が必要になっている**
- **ユーザーインターフェースが重要**
- **倫理チェックの仕組みが必要**

## 重要な結論

1. **AIは能力をブーストするツールであり、使い方の差によって大きく結果が変わる**
2. **LLMの基本的な仕組みを理解することで、より効果的な活用が可能になる**
3. **技術の進歩により、PCレベルでもLLMの実行が現実的になってきている**
4. **倫理的な配慮と安全性の確保が重要な課題となっている**

## 関連資料

- [AI時代のプログラミング教育](https://speakerdeck.com/kishida/programming-education-in-ai-era)
- [言語を学んだ後に学ぶべきこと](https://speakerdeck.com/kishida/what-should-we-study-after-language)
