# DatabricksのAIエージェント評価機能の実力を検証してみた

ref: <https://zenn.dev/nttdata_tech/articles/f18125087f8931>

---

# DatabricksのAIエージェント評価機能「LLM-as-a-Judge」の検証

この記事は、Databricksが提供するAIエージェント評価機能、特に「LLM-as-a-Judge」に焦点を当て、日本語環境におけるその精度を検証した結果を共有するものです。

## はじめに

* **背景:** Databricksはデータ基盤と統合されたAIエージェント開発プラットフォームとして注目されており、その評価機能の一つが「Mosaic AI Agent Evaluation」です。
* **目的:** 本記事では、LLMを利用した精度評価機能「LLM-as-a-Judge」が日本語環境でどの程度活用できるかを検証します。
* **対象者:** Databricksでの生成AI開発者、AIエージェント評価機能に関心のある方。

## 1. Databricksが提供するLLM-as-a-Judge

### 1-1. LLM-as-a-Judgeとは？

* **定義:** LLM（大規模言語モデル）を用いて、他のLLM機能の精度を評価する手法です。
* **メリット:**
  * 評価の効率化（人手による評価削減、迅速なフィードバック）
  * 柔軟な評価指標（正確性、安全性、関連性など）
  * 人間に近い判断基準（文脈やニュアンス考慮）

### 1-2. Databricksが提供する評価指標（ビルトイン評価指標）

Databricksは、独自にチューニングした評価LLMとプロンプト（非公開）を使用し、以下の8つの評価指標を提供しています（2025年4月時点）。

| 指標名               | 説明                                                 | Ground Truth要否 | 備考                     |
| :------------------- | :--------------------------------------------------- | :--------------- | :----------------------- |
| relevance_to_query   | 応答がユーザー要求・意図に関連しているか             | ×                |                          |
| groundedness         | 応答が取得したコンテキストに基づいているか           | ×                |                          |
| correctness          | 応答が期待応答と一致し、誤りを含まないか             | 〇                |                          |
| guideline_adherence  | 応答が指定ガイドラインに従っているか                 | ×                | ガイドライン設定が必要   |
| safety               | 応答に有害・攻撃的・有毒な内容が含まれていないか     | ×                |                          |
| chunk_relevance      | レトリーバーが取得したチャンクが要求に関連しているか | ×                |                          |
| document_recall      | レトリーバーが期待される関連文書をどれだけ見つけたか | 〇                | 自動的に算出される       |
| context_sufficiency  | レトリーバーが応答生成に十分な文書を取得しているか   | 〇                |                          |

**（図解）RAGに関連する評価指標の関係図:**
（元の記事にある図を参照してください。テキストでは再現できませんが、レトリーバー、クエリ、コンテキスト、応答の関係と各指標がどこを評価するかを示しています。）

### 1-3. 評価の実行方法

* MLflowの `mlflow.evaluate()` を利用します。
* `model_type` を `databricks-agent` と指定し、利用したいビルトイン評価指標を `evaluator_config` で設定します。
* 判定結果はエクスペリメントから確認でき、Pass/Failで出力されます。
* **注意点:** 2025年4月現在、ビルトイン評価指標では詳細なスコアは出力されません。

## 2. 検証概要

* **方法:** 各ビルトイン評価指標ごとに日本語のテストデータを用意し、評価結果が人間の判定結果と一致するかを検証します。今回は明らかなPass/Fail判定レベルでの確認です。
* **検証対象指標:** `relevance_to_query`, `groundedness`, `correctness`, `guideline_adherence`, `safety` の5つ。

### 2-2. 用意したテストデータ

各指標に対して、人間の判定でPass/Failが明確に分かれるような日本語のテストデータを用意しました。

* **relevance_to_query (6件):** リクエストへの応答が「直接関連/全く関連なし」の観点。
* **groundedness (6件):** 取得コンテキストに対する応答の「説明/数値/順序」の一致/不一致の観点。
* **correctness (6件):** 期待応答に対する応答の「説明/数値/順序」の一致/不一致の観点。
* **guideline_adherence (10件):** ガイドライン（言語、丁寧語、拒否、一貫性、流暢性）に対する応答の一致/不一致の観点。
* **safety (10件):** 応答の「バイアス/毒性/違法性/個人の安全/個人情報保護」の有無の観点。

（各テストデータの詳細は元の記事の表を参照してください。）

## 3. 検証結果

| 指標名               | 説明                                                 | 一致数/データ数 | 一致率 |
| :------------------- | :--------------------------------------------------- | :-------------- | :----- |
| relevance_to_query   | 応答がユーザー要求・意図に関連しているか             | 6/6             | 100%   |
| groundedness         | 応答が取得したコンテキストに基づいているか           | 6/6             | 100%   |
| correctness          | 応答が期待応答と一致し、誤りを含まないか             | 6/6             | 100%   |
| guideline_adherence  | 応答が指定ガイドラインに従っているか                 | 10/10           | 100%   |
| safety               | 応答に有害・攻撃的・有毒な内容が含まれていないか     | 7/10            | 70%    |

* **結果:** `safety` 指標以外は、人間の評価と全て一致しました。
* **`safety` 指標の詳細:**
  * 「違法性」「個人の安全」は適切に判定されました。
  * 「バイアス」「毒性」「個人情報の保護」に関しては、安全性がない応答を安全と誤判定するケースが見られました。（元の記事に具体的な誤判定例のスクリーンショットあり）
* **考察:**
  * `safety` 指標は多様な観点を含むため、各観点での評価一致度の確認が必要です。
  * `safety` 指標のみ詳細な判定理由が出力されないため、評価結果に基づくチューニングが難しいという課題があります。
  * `safety` 指標に関しては、各観点に対して独自のカスタム評価指標を設定する方が適切な場合があります。

## 4. 最後に

* **結論:** DatabricksのLLM-as-a-Judgeは、日本語環境においても明確な判定レベルでは概ね人間の評価と一致することが確認できました。
* **課題と推奨:** `safety` 指標のように、ビルトイン評価だけではカバーしきれないケースがあるため、実際の要件に応じてビルトイン指標の適用可能性を事前確認し、必要であればカスタム評価指標を作成することが望ましいです。

---
