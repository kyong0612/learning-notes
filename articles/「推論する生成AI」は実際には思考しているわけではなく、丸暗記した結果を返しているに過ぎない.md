---
title: "「推論する生成AI」は実際には思考しているわけではなく、丸暗記した結果を返しているに過ぎない"
source: "https://tjo.hatenablog.com/entry/2025/07/23/173000"
author:
  - "Takashi J. OZAKI"
published: 2025-07-23
created: 2025-08-02
description: |
  近年の「推論する生成AI」は、実際には思考しているのではなく、学習データに含まれるパターンを丸暗記して応答している可能性が高い。AppleとGoogle DeepMindによる2つの新しい研究は、AIが既知の複雑な課題は解けるものの、未知のバリエーションや、より簡単なはずの改変問題には対応できないことを実験で示した。これは、AIの「推論能力」が、真の思考ではなく高度なパターンマッチングに過ぎないことを示唆している。
tags:
  - "AI"
  - "機械学習"
  - "大規模言語モデル"
  - "推論"
  - "clippings"
---

![](https://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20250722/20250722124944.png)

AppleとGoogle DeepMindによる2つの論文は、最近の「推論する生成AI」（大規模推論モデル、LRM）が、実際には思考しているわけではなく、学習データ内のパターンを丸暗記して応答しているに過ぎない可能性を指摘しています。

### 「推論する生成AI」とは

* **定義**: Chain-of-Thought (CoT) Promptingを強化学習で実装し、大きな問題を小さなプロセスに分解して段階的に解決するように学習させたAIモデル。
* **現状**: 数学オリンピックや競技プログラミングなどで高いスコアを達成し、「人間レベルの推論能力」を持つとアピールされている。

### 研究1: 複雑な課題の難易度を上げると解けなくなる (Appleの研究)

4つの代表的な論理パズル（ハノイの塔など）で、課題の複雑性を増すパラメータ（例: ハノイの塔の輪の数）を増やしていく実験を行いました。

![](https://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20250723/20250723145205.png)

* **結果**:
  * ある一定の複雑さを超えると、どのLRMも急に正答できなくなった。
  * 課題が複雑になるほど「推論のための努力」（トークン数）が増えるが、途中から逆に減少する挙動が見られた。これは、複雑すぎると推論を諦める、または計算量を活用できなくなることを示唆する。
  * あるLRMは、より複雑な「ハノイの塔（N=5）」は解けるのに、より単純な「川渡り（N=3）」は解けなかった。これは、学習データに偏りがあるためと推測されている。

![](https://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20250723/20250723151603.png)

### 研究2: 課題のルールを改変し、難易度を下げると逆に解けなくなる (Google DeepMindの研究)

高度な推論課題 (Puzzles) と、それを改変して自明なほど簡単にした課題 (Unpuzzles) を用意してLRMに解かせました。

![](https://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20250723/20250723145353.png)

* **例**: 「3色のカメレオン」問題
  * **Puzzle**: 複雑な思考を要するオリジナルの問題。
  * **Unpuzzle**: ルールを改変し、答えが自明になった単純な問題。
* **結果**:
  * 全てのLRMで、難しい「Puzzles」よりも簡単なはずの「Unpuzzles」の方が正答率が低かった。
  * Unpuzzleに対して、答えは合っていても思考プロセスが間違っているケースが見られた。LRMは自明な答えを導くのではなく、元の複雑な問題の解法パターンを無理やり適用しようとした。

![](https://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20250723/20250723153910.png)

### 結論：AIの推論は「丸暗記」の可能性が高い

これらの実験結果は、「推論する生成AI」が真に論理的な思考を行っているわけではなく、**学習データに含まれる「問いと答え」のパターンを丸暗記し、それにマッチする課題にのみ正しく応答している**可能性が高いことを強く示唆しています。

未知の課題や、既知の課題でもルールが少し変わると対応できないという事実は、現在のAIの「推論能力」の限界を示しており、今後の研究開発において考慮されるべき重要な指摘です。

---
*出典: [難問論理クイズ「幼女と3色のカメレオン」が数学的発想を必要とする - 明日は未来だ！](https://sist8.com/3cham)*
