# Humanity's Last Exam (HLE): 徹底的な調査

最新の利用可能な情報に基づく調査結果から、「Humanity's Last Exam」についての包括的な分析をご紹介します：

## **Humanity's Last Examとは何か？**

Humanity's Last Exam (HLE)は、人間の知識の最前線に位置するマルチモーダルベンチマークで、「幅広い分野をカバーする最後のクローズドエンド型学術ベンチマーク」として設計されています。データセットは、数学、人文学、自然科学を含む100以上の分野にわたる2,500～3,000の困難な質問で構成されています。

このベンチマークは、AI安全センター（CAIS）とScale AIによって共同開発され、CAISのディレクターである機械学習研究者ダン・ヘンドリックスの発案とされています。彼はイーロン・マスクとの会話からインスピレーションを得たとされており、マスクは既存のMMLUなどの言語モデルベンチマークが簡単すぎると考えていました。

## **目的とデザイン理念**

この試験は「ベンチマーク飽和」の課題に対処するために開発されました。これは、既存のテストで定期的に完璧に近いスコアを達成するモデルが、それらのテスト以外の質問には答えられない可能性がある問題です。飽和により、将来のモデル進歩の正確な測定手段としてのベンチマークの有用性が減少します。

各質問には、明確で検証しやすい既知の解答がありますが、インターネット検索では素早く答えることができないものです。質問は専門家以外には困難で、簡単なオンライン検索では答えられないものでなければならず、質問作成者には5年以上の技術的産業経験またはPhDレベルの学術訓練が推奨されています。

## **規模とグローバルな協力**

HLEはグローバルな協力事業で、50カ国の500以上の機関に所属する約1,000人の専門家が質問を提供しており、その大部分は教授、研究者、大学院学位保持者で構成されています。CAISとScaleの研究者は70,000以上の試験問題を収集し、その中から13,000問を人間の専門家による審査に選出し、最終的に3,000問が公開試験として確定されました。

## **質問カテゴリと形式**

公開されたベンチマークは2,500問で構成され、以下の分野に分類されています：数学（41%）、物理学（9%）、生物学/医学（11%）、人文学/社会科学（9%）、コンピュータサイエンス/人工知能（10%）、工学（4%）、化学（7%）、その他（9%）。約14%の質問はマルチモーダル機能（テキストと画像）を必要とします。24%は選択問題で、残りは短答式の完全一致問題です。ベンチマークの過適合をテストするため、非公開セットも維持されています。

## **サンプル質問**

試験には以下のような極めて困難な質問が含まれています：

「アマツバメ目内のハチドリは、尾下筋挿入部の拡大された十字状腱膜の尾外側部に埋め込まれた種子骨である、両側対の楕円形骨を独特に持っています。この種子骨によって支持される対になった腱は何本ですか？数字で答えてください。」

「示された反応は、出発ヘプタエンをエンディアンドリック酸Bメチルエステルに変換する熱ペリ環式カスケードです。このカスケードには3つのステップが含まれます：2つの電子環化に続く1つの環化付加。ステップ1と2で関与する電子環化のタイプ、およびステップ3で関与する環化付加のタイプは何ですか？」

## **現在のAI性能結果（2025年最新データ）**

最新のリーダーボード（2025年4月30日更新）によると、最高性能のAIモデルは以下の通りです：

- o3 (high): 20.32% ± 1.58%
- o3 (medium): 19.20% ± 1.54%
- Gemini 2.5 Pro Experimental: 18.16% ± 1.51%
- o4-mini (high): 18.08% ± 1.51%
- Claude Opus 4 (Thinking): 10.72% ± 1.21%

初期の結果ははるかに低い性能を示していました：OpenAIのGPT-4oは3.3%、Grok-2は3.8%、Claude 3.5 Sonnetは4.3%、Geminiは6.2%、o1は9.1%、DeepSeek-R1は9.4%の正答率でした。

## **金銭的インセンティブと著者権**

上位50問は各5,000ドル、次の500問は各500ドルが支払われ、総額50万ドルの賞金プールから支給されました。採用された質問の提供者には、結果として生まれる研究論文の共著者権が提供されました。

## **最近の開発とバグ報奨プログラム**

2025年2月11日から3月21日まで、質問の妥当性や正確性を損なう重大なエラーを特定するためのバグ報奨プログラムが実施されました。このプログラムでは、見つけにくい問題を奨励するため、lastexam.aiで改良されたベータ版がリリースされました。

## **論争と批判**

### **急速な飽和への懸念**

批評家たちは、HLEでのAI性能の急速な改善は、理解の飛躍的向上というよりも、これらの特定の問題を含むようにモデルが更新されていることに起因する可能性があると主張しています。リー・ガイヤーは次のように指摘しました：「ベンチマークのリリース後に指数関数的な上昇が続くという事実は、モデルがこれらの問題を含むように更新されていることを示唆している。」

### **クローズドエンド型テストの限界**

批評家たちは、クローズドエンド型ベンチマークのみに焦点を当てることで、AI能力の歪んだ見方を提供すると主張しています：「これらのタスク、困難な質問に答えることは全体像ではない。モデルは依然として、複数のソースから情報を統合し、注意をそらす情報を避ける必要があるオープンエンドなタスクに苦労している。」

マギー・アップルトンは、ベンチマークを知能の尺度として使用することの根本的な問題を指摘しました：「多肢選択式の標準化テストであるという事実以外に、これが現実世界における人間の能力の合理的な尺度だと思いますか？」

### **AGIに関する哲学的問題**

「Humanity's Last Examのようなベンチマークの急速な飽和が真にAGIへの進歩を意味するのか、それとも単に狭い最適化を表すだけなのか」について論争があります。

## **将来への含意と予測**

現在のLLMはHumanity's Last Examで非常に低い正答率を示していますが、研究者たちは「最近の歴史は、ベンチマークが急速に飽和することを示しており、モデルがゼロに近い性能から完璧に近い性能へと短期間で劇的に進歩している。AIの急速な発展ペースを考慮すると、2025年末までにモデルがHLEで50%を超える正答率を達成する可能性がある」と指摘しています。

AI政策センターは、HLEが「メロドラマティックに見えるかもしれないが、AI専門家のズヴィ・モウショウィッツが指摘するように、この新しいベンチマークは人間によって作成され採点される最後の一般知識ベンチマークになる可能性が非常に高い」と警告しています。

## **結論**

Humanity's Last Examは、AIベンチマーキングの現在の頂点と、人工知能のための人間設計テストの限界に関する潜在的な警告の両方を表しています。現在のAIシステムが多様な分野における専門家レベルの人間の知識にまだ及ばないことを成功裏に実証する一方で、AI性能の急速な改善は、そのようなベンチマークが長期間関連性を保てるかどうか、そして実世界のアプリケーションにとって重要な種類の知能を真に測定しているかどうかという疑問を提起しています。

HLEをめぐる議論は、測定可能なベンチマークの作成と、狭い最適化ではなく真の知能を捉えるベンチマークの確保との間のAI開発における広範な緊張を反映しています。
