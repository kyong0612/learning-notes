# Introducing GPT-4.1 in the API

ref: <https://openai.com/index/gpt-4-1/>

## 概要

OpenAIは、API向けに新しいGPTモデルシリーズを発表しました。GPT-4.1、GPT-4.1 mini、GPT-4.1 nanoの3つのモデルがリリースされ、コーディング、指示の理解、長文脈処理において大幅な性能向上を達成しています。知識のカットオフ時期は2024年6月です。

## 主要な特徴と性能

### コーディング能力

- **SWE-bench Verified**: GPT-4.1は54.6%のスコアを達成（GPT-4oより21.4%向上）
- **指示理解**: Scale's MultiChallengeベンチマークで38.3%を記録（GPT-4oより10.5%向上）
- **長文脈理解**: Video-MMEベンチマークで72.0%を達成（GPT-4oより6.7%向上）

### 文脈ウィンドウの拡大

- すべてのGPT-4.1モデルで100万トークンの文脈処理が可能（GPT-4oの12.8万トークンから大幅拡大）
- 長文脈理解能力が向上し、大規模コードベースや多数の文書を効率的に処理可能

### モデルラインナップの特徴

- **GPT-4.1**: コーディングと指示理解に優れた基幹モデル
- **GPT-4.1 mini**: GPT-4oを多くのベンチマークで上回る性能、レイテンシ半減、コスト83%削減
- **GPT-4.1 nano**: 最速・最低コストのモデルでありながら高い性能（MMLU 80.1%、GPQA 50.3%）

## 実世界での適用例

### コーディング

- **Windsurf**: 内部コーディングベンチマークでGPT-4oより60%高いスコア、ツール呼び出しが30%効率的
- **Qodo**: GitHubプルリクエストのコードレビューで55%のケースでより良い提案を生成

### 指示理解

- **Blue J**: 最も難しい税務シナリオでGPT-4oより53%正確
- **Hex**: SQL評価セットで約2倍の改善、大規模データベーススキーマからの適切なテーブル選択が向上

### 長文脈理解

- **Thomson Reuters**: 法律文書の複数文書レビュー精度が17%向上
- **Carlyle**: 複数の長文書からの財務データ抽出が50%向上

## 視覚理解能力

GPT-4.1シリーズは画像理解にも優れており、特にGPT-4.1 miniはGPT-4oを多くの視覚ベンチマークで上回っています：

- MMMU: 73-75%（GPT-4o: 69%）
- MathVista: 72-73%（GPT-4o: 61%）
- CharXiv-Reasoning: 57%（GPT-4o: 53%）

## 料金体系

効率向上により、より低価格での提供が実現：

- **GPT-4.1**: 入力$2.00/100万トークン、出力$8.00/100万トークン（GPT-4oより26%安価）
- **GPT-4.1 mini**: 入力$0.40/100万トークン、出力$1.60/100万トークン
- **GPT-4.1 nano**: 入力$0.10/100万トークン、出力$0.40/100万トークン（最安値）
- プロンプトキャッシュの割引が75%に増加（以前は50%）
- 長文脈処理も追加コストなし

## その他の重要情報

- GPT-4.1はAPIのみで利用可能
- GPT-4.5 Previewは3か月後（2025年7月14日）に廃止予定
- 出力トークンの制限がGPT-4.1では32,768トークンに拡大（GPT-4oは16,384）

![GPT-4.1ファミリーのレイテンシと知能の関係グラフ](https://images.ctfassets.net/kftzwdyauwt9/1QIXpeDCuUBwiGQ6xxkIeg/75a11a9c0e222c9b93896fab30304d66/GPT-4.1_Family_Intelligence_by_Latency_lightMode.svg?w=3840&q=90)
