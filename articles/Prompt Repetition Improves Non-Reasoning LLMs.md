---
title: "Prompt Repetition Improves Non-Reasoning LLMs"
source: "https://arxiv.org/html/2512.14982v1"
author:
  - "Yaniv Leviathan"
  - "Matan Kalman"
  - "Yossi Matias"
published:
created: 2025-12-29
description: |
  推論を使用しない場合、入力プロンプトを繰り返すことで、人気のあるLLM（Gemini、GPT、Claude、Deepseek）のパフォーマンスが向上し、生成トークン数やレイテンシを増やすことなく改善できることを示した研究。プロンプトを「<QUERY>」から「<QUERY><QUERY>」に変換することで、各プロンプトトークンが他のすべてのプロンプトトークンに注意を向けられるようになり、因果言語モデルの制約を緩和する。
tags:
  - "clippings"
  - "LLM"
  - "prompt-engineering"
  - "prompt-repetition"
  - "Google-Research"
  - "performance-optimization"
---

## 概要

Google Researchの研究者らが、推論を使用しない場合にプロンプトを繰り返すことで、LLMのパフォーマンスを向上させる手法を提案。この手法は、生成トークン数やレイテンシを増やすことなく、複数のモデルとベンチマークで一貫した改善を示した。

## 1. プロンプト繰り返しの提案

### 背景と動機

- LLMは因果言語モデルとして訓練されており、過去のトークンは未来のトークンに注意を向けられない
- トークンの順序が予測性能に影響を与える（例：「<CONTEXT> <QUESTION>」と「<QUESTION> <CONTEXT>」で異なる性能）
- 推論モデルはRL訓練により、ユーザーのリクエストの（一部）を繰り返すことを学習している

### 手法

プロンプトを「<QUERY>」から「<QUERY><QUERY>」に変換することで、各プロンプトトークンが他のすべてのプロンプトトークンに注意を向けられるようになる。

### 利点

- **効率性**: 繰り返しを並列化可能なprefillステージに移動
- **生成トークン数の増加なし**: 「Think step by step」などの他のプロンプト手法とは異なり、生成されるトークン数が増えない
- **出力形式の変更なし**: 生成される出力の形式が変わらないため、既存システムへの簡単な導入が可能
- **レイテンシへの影響なし**: 推論が無効な場合、レイテンシに影響を与えない

## 2. 実験

### テスト対象モデル

7つの人気のあるLLMをテスト：

- **Gemini**: Gemini 2.0 Flash、Gemini 2.0 Flash Lite
- **GPT**: GPT-4o-mini、GPT-4o
- **Claude**: Claude 3 Haiku、Claude 3.7 Sonnet
- **Deepseek**: Deepseek V3

すべてのテストは2025年2月と3月に各プロバイダーの公式APIを使用して実行。

### ベンチマーク

7つのベンチマークでテスト：

- **ARC (Challenge)**: 科学的推論タスク
- **OpenBookQA**: オープンブック質問応答
- **GSM8K**: 数学的推論
- **MMLU-Pro**: 多タスク言語理解
- **MATH**: 数学問題解決
- **NameIndex**: カスタムタスク（50個の名前リストから25番目の名前を出力）
- **MiddleMatch**: カスタムタスク（40個の名前/数字リストから、2つの指定された要素の間にある要素を出力）

複数選択問題（ARC、OpenBookQA、MMLU-Pro）では、質問を先に配置する場合と選択肢を先に配置する場合の両方でテスト。

### 精度の結果

推論を使用しない場合、プロンプト繰り返しはすべてのテストされたLLMとベンチマークで精度を向上：

- **統計的有意性**: McNemar検定（p-value < 0.1）で有意な改善
- **勝率**: 70のベンチマーク-モデル組み合わせのうち47で勝利、0敗
- **全モデルで改善**: テストされたすべてのモデルで性能が向上
- **質問先配置**: 複数選択問題で質問を先に配置した場合、改善は小さい
- **選択肢先配置**: 選択肢を先に配置した場合、より大きな改善を観察
- **カスタムタスク**: NameIndexとMiddleMatchで強い改善（例：Gemini 2.0 Flash-LiteでNameIndexの精度が21.33%から97.33%に向上）

推論を有効にした場合（「Think step by step」）は、中立的からわずかにポジティブ（5勝、1敗、22引き分け）。

### アブレーションとバリエーション

以下のバリエーションを比較：

1. **Prompt Repetition (Verbose)**: 「Let me repeat that: <QUERY>」の形式
2. **Prompt Repetition ×3**: 3回繰り返し
3. **Padding**: 同じ長さまでピリオドでパディング（対照実験）

結果：

- ほとんどのタスクとモデルで同様の性能
- **×3バリエーション**: NameIndexとMiddleMatchで大幅に優れた性能を示すことが多い
- **Padding**: 予想通り、性能改善なし（繰り返しが重要であることを確認）

### 効率性

各モデル、プロンプト手法、データセットについて以下を測定：

- 生成される出力の平均および中央値の長さ
- 実測レイテンシ（公式API経由）

結果：

- **推論無効時**: すべてのデータセットとモデルで同様のレイテンシ
- **推論有効時**: すべてのレイテンシと生成出力の長さが劇的に増加
- **プロンプト繰り返し**: 生成出力の長さや測定レイテンシを増加させない
- **例外**: Anthropicモデル（Claude HaikuとSonnet）で非常に長いリクエストの場合、レイテンシが増加（prefillステージが長くなるため）

## 3. 関連研究

- **Chain of Thought (CoT)**: タスクごとに特定の例が必要、生成トークン数とレイテンシが増加
- **"Think step by step"**: 大幅な改善をもたらすが、生成出力の長さとレイテンシが増加
- **Shaier (2024)**: 質問部分のみを繰り返す実験では改善なし
- **Springer et al. (2024)**: 入力を2回繰り返すことでテキスト埋め込みが改善
- **Xu et al. (2024)**: モデルに質問を再読させることで推論が改善

## 4. 結論

- 推論を使用しない場合、プロンプトの繰り返しは複数のモデルとベンチマークで一貫して性能を向上
- レイテンシへの影響なし（並列化可能なprefillステージのみが影響を受ける）
- 生成出力の長さや形式を変更しない
- 推論が使用されない場合、多くのモデルとタスクでデフォルトとして適切な可能性

### 今後の研究方向

1. 繰り返しプロンプトでモデルをファインチューニング
2. プロンプト繰り返しで推論モデルを訓練して効率を向上（モデルが繰り返しを避けることを学習する可能性）
3. 生成中に最後に生成されたトークンを定期的に繰り返す、マルチターンシナリオへの適用性の探索
4. KV-cacheに2回目の繰り返しのみを保持（生成ステージで完全に性能中立）
5. プロンプトの一部のみを繰り返す（特に長いプロンプトの場合）
6. すべてを繰り返す代わりに、より小さなモデルでプロンプトを並べ替える
7. 非テキストモダリティ（例：画像）への適用性
8. 2回以上の繰り返しが有利な場合の異なるバリエーションのさらなる分析
9. 繰り返しによる注意パターンのさらなる分析
10. 選択的注意などの技術と組み合わせて繰り返しを使用
11. Prefix LMなどの技術との相互作用の探索
12. 繰り返しが役立つ場合と、繰り返し間でトークン表現がどのように変化するかの調査
13. 有望なバリエーションの探索

## 重要な発見

- **70のテスト中47勝、0敗**: 統計的に有意な改善を示す
- **全モデルで改善**: Gemini、GPT、Claude、Deepseekすべてで性能向上
- **効率性**: 生成トークン数やレイテンシを増やすことなく改善
- **カスタムタスク**: NameIndexで最大76%の改善（21.33% → 97.33%）
- **推論との組み合わせ**: 推論を有効にした場合は中立的からわずかにポジティブ

## 制限事項

- 非常に長いプロンプトの場合、レイテンシに影響を与える可能性がある
- 非常に長いプロンプトの場合、繰り返しが不可能な場合がある
- Anthropicモデル（Claude）で非常に長いリクエストの場合、レイテンシが増加する可能性がある
