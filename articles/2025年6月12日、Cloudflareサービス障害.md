---
title: "2025年6月12日、Cloudflareサービス障害"
source: "https://blog.cloudflare.com/ja-jp/cloudflare-service-outage-june-12-2025/"
author:
  - "[[Jeremy Hartman]]"
  - "[[CJ Desai]]"
  - "[[Alex Robinson]]"
  - "[[Tyson Trautmann]]"
  - "[[Ash Pallarito]]"
  - "[[Joe Abley]]"
  - "[[David Belson]]"
  - "[[Phillip Jones]]"
published: 2025-06-13
created: 2025-08-15
description: "2025年6月12日、Workers KV、Access、WARP、Cloudflareダッシュボードを含む複数のCloudflareサービスで、最大2時間22分の障害が発生しました。"
tags:
  - "clippings"
---

### Cloudflare障害報告から学ぶポストモーテムの教訓

この障害報告は、技術的な原因分析だけでなく、組織としての姿勢や文化を示す優れたポストモーテムです。特に以下の4つの観点から多くの学びを得ることができます。

#### 1. アーキテクチャと依存関係に関する教訓

* **内部サービスへの過度な依存（ドッグフーディング）のリスク:**
  * Cloudflareは自社製品（Workers KV）を社内サービスの基盤として広く利用していました。これは通常は良いプラクティスですが、その基盤サービスが停止すると、広範囲にわたる連鎖的な障害（カスケード障害）を引き起こすことを示しています。
  * **教訓:** 基幹となるサービスには、特に高い可用性と冗長性が求められます。また、依存する側も、基幹サービスが停止した場合のフォールバック（代替処理）や機能縮退（graceful degradation）の設計が不可欠です。

* **単一障害点（SPOF）の危険性:**
  * 障害の直接的な引き金はサードパーティのストレージインフラでしたが、結果的にそれがCloudflareにとっての単一障害点となっていました。
  * **教訓:** 「コアレス」のような分散アーキテクチャを目指していても、意図せず中央集権的な依存関係が残ることがあります。外部・内部問わず、システムの依存関係を定期的に監査し、単一障害点をなくす努力が重要です。

* **フェイルクローズ設計の重要性:**
  * `Access`や`Gateway`サービスでは、設定やユーザー情報を取得できない場合に通信を許可せず、ブロックする「フェイルクローズ」の設計が取られていました。
  * **教訓:** セキュリティが関わるサービスでは、異常時に安全側に倒す設計が基本となります。これにより、意図しないアクセス許可を防ぐことができますが、一方でサービスの完全停止につながるトレードオフも存在します。

#### 2. 障害対応プロセスに関する教訓

* **迅速な状況把握とインシデント集約:**
  * 複数のサービスで同時にアラートが発生した後、約10分で共通の原因が`Workers KV`にあると特定し、個別のインシデントを一つに統合しています。
  * **教訓:** 障害発生時、根本原因を迅速に特定し、対応チームを一本化することが、効率的な復旧作業につながります。

* **プロアクティブな代替策の検討:**
  * 根本原因（サードパーティのストレージ）の復旧を待つだけでなく、並行して`Workers KV`のバックエンドを自社インフラに切り替える作業を開始しています。
  * **教訓:** 障害対応において、復旧が見通せない場合は、複数の回復シナリオを同時に検討・準備する「プランB」の存在が、対応時間を短縮する上で非常に有効です。

* **キルスイッチによる被害拡大の防止:**
  * `Turnstile`（チャレンジ認証）では、API呼び出しが失敗する場合に備えて、機能を無効化する「キルスイッチ」を用意しており、これを有効化してエンドユーザーが完全にブロックされる事態を防ぎました。
  * **教訓:** 重要な機能には、緊急時に機能を停止または縮退させる仕組みをあらかじめ組み込んでおくことで、全体への影響を最小限に抑えることができます。

#### 3. 再発防止と信頼性向上に関する教訓

* **具体的かつ実行可能なアクションプラン:**
  * ポストモーテムの最後で、「Workers KVのストレージ冗長化」「各製品の障害耐性強化」「段階的なサービス再有効化ツールの実装」など、具体的で多岐にわたる改善策を提示しています。
  * **教訓:** 再発防止策は、精神論ではなく、具体的な技術的・組織的改善に落とし込む必要があります。

#### 4. 組織文化と透明性に関する教訓

* **完全な責任受容の姿勢:**
  * 障害の引き金はサードパーティでしたが、「これは当社の失敗であり、当社に最終的な責任があります」と明確に述べ、顧客からの信頼を維持しようとする強い意志を示しています。
  * **教訓:** 障害の原因が外部要因であっても、それを選定し、その上でシステムを設計したのは自社であるという当事者意識を持つことが、組織の信頼性向上につながります。

* **徹底した情報公開:**
  * 影響を受けたサービスを網羅的にリストアップし、それぞれの具体的な影響（エラー率など）を詳細に公開しています。
  * **教訓:** 障害の影響範囲や原因について、顧客に対して可能な限り透明性を保つことが、長期的な信頼関係の構築に不可欠です。

---

以上のように、この障害報告は単なる技術的な反省文にとどまらず、信頼性の高いサービスをいかに構築し、運用し、そして万が一の際にどう向き合うかという、エンジニアリング組織全体にとっての優れた学びのケーススタディと言えます。
