# NoLiMa: Long-Context Evaluation Beyond Literal Matching

ref: <https://arxiv.org/abs/2502.05167>

## 概要

NoLiMaは大規模言語モデル（LLM）の長文脈処理能力を評価するための新しいベンチマークです。最近のLLMは128Kから1Mトークンの長い文脈をサポートしていますが、従来の評価方法では質問と関連情報（needle）の間に文字通りの一致（literal match）があり、これがタスクを簡略化していました。NoLiMaは「針を探す（Needle-in-a-Haystack）」テストを拡張し、質問とneedleの間に最小限の字句的重複しかない慎重に設計されたneedleセットを導入しています。これにより、モデルは単純な文字列マッチングではなく、潜在的な関連性を推論する能力が求められます。

少なくとも128Kトークンをサポートすると主張する12の人気LLMを評価した結果、短い文脈（<1K）では高い性能を示すものの、文脈長が増加すると性能が著しく低下することが判明しました。例えば32Kトークンでは、12モデル中10モデルが短文脈でのベースライン性能の50%以下に低下しました。最高性能のGPT-4oでさえ、ほぼ完璧なベーススコア99.3%から69.7%まで低下しています。この分析から、リテラルマッチが存在しない長い文脈では、アテンションメカニズムが関連情報を取得することが困難になることが示唆されています。

## 研究背景と関連研究

近年、LLMは長文脈入力の処理において顕著な進展を遂げており、長文書QA、要約、多数例示によるIn-Context Learningなど様々なNLPタスクの可能性を広げています。長文脈処理能力を評価するために、Needle-in-a-Haystack（NIAH）のようなベンチマークが開発されてきました。NIAHは「針（needle）」と呼ばれる特定の事実が無関係な情報「干し草（haystack）」の中に隠されており、モデルがそれを見つけ出す能力をテストします。

しかし、多くの既存のベンチマークには共通の問題があります：質問（またはタスク）と提供される文脈の間にリテラルマッチが存在し、これによりモデルは単純に一致するパターンを探すだけで関連情報を見つけられます。NoLiMaはこの問題に対処し、より厳密なテストを提供します。

## NoLiMaの設計

NoLiMaはNIAHの基本的な要素を維持しながらも、質問とneedleの間の文字通りの一致を最小限にするという条件を課しています。各needleは特定のキャラクターとその情報で構成されています。例えば：

> 実際、ユキはゼンパーオペラハウスの隣に住んでいます。

このneedleには「ゼンパーオペラハウス」というキーワード（Wn）があり、質問との重要なリンクとなっています。質問はこの情報を特定の属性（Wq、例えば「ドレスデン」）を持つキャラクターを尋ねることで検索します：

> どのキャラクターがドレスデンに行ったことがありますか？

ゼンパーオペラハウスはドレスデンにあるため、モデルはWq（「ドレスデン」）とWn（「ゼンパーオペラハウス」）の間の潜在的な関連リンクを識別する必要があります。リテラルな重複がないため、モデルはこの潜在的な関連リンクに頼って正解「ユキ」を取得する必要があります。

さらに複雑なケースでは、2ホップの関連付けが必要な質問もあります（例：「ザクセン州に行ったことがあるキャラクターは？」）。これにはドレスデン（およびゼンパーオペラ）がザクセン州にあるという知識が必要です。

また実装では以下のような制約が課されています：

- 単純さを確保するためのキーワード選択（無関係な文脈がなければ関連性が明確）
- 多様なプールからのキャラクター名のランダム割り当て
- WnはWqと一意に関連付けられることを確保
- 前置きフレーズによるneedleの分離

## 評価設定と実験

### データセット構成

- 5つのneedleグループ、各グループに2つの事実順序バリエーション（デフォルトと反転）
- 2～6のキーワードセット、1ホップと2ホップの例を含む
- 合計58の質問-needle対
- 10冊のオープンライセンス書籍から短いスニペットを連結してhaystackを生成
- 各needleは評価する文脈長全体に均等に26回配置
- 5つのランダム生成されたhaystack、58の質問-needle対、文脈長ごとに26の配置で、文脈長ごとに7,540テスト

### 評価モデル

- クローズドソース：GPT-4o、GPT-4o Mini、Gemini 1.5 Pro、Flash、Claude 3.5 Sonnet
- オープンウェイト：Llama 3.xモデルファミリー（3.1 8B、70B、405B、3.3 70B）、Mistral Large、Command R+、Jamba 1.5 Mini

### 評価方法

- 250、500、1K、2K、4K、8K、16K、32Kトークンの文脈長でテスト
- ベーススコアとして250、500、1Kでの最大スコアを使用
- 有効長さ：ベーススコアの85%を超える最大のテスト長さ

## 主な結果と分析

### 性能評価結果

ほとんどのモデルは短い文脈では高いベーススコアを達成していますが、その有効長は主張される長さよりも大幅に短く、一般的に≤2Kトークンに制限されています（GPT-4oは例外）。12モデルのうち10モデルが32K長で、ベーススコアの半分以下のパフォーマンスを示しています。

以下の表はNoLiMaテストでの各モデルの性能結果を示しています：

| モデル | 主張長 | 有効長 | ベース<br>スコア<br>(×0.85) | 1K | 2K | 4K | 8K | 16K | 32K |
|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| GPT-4o | 128K | 8K | 99.3 (84.4) | 98.1 | 98.0 | 95.7 | 89.2 | 81.6 | 69.7 |
| Llama 3.3 70B | 128K | 2K | 97.3 (82.7) | 94.2 | 87.4 | 81.5 | 72.1 | 59.5 | *42.7* |
| Llama 3.1 405B | 128K | 2K | 94.7 (80.5) | 89.0 | 85.0 | 74.5 | 60.1 | 48.4 | *38.0* |
| Llama 3.1 70B | 128K | 2K | 94.5 (80.3) | 91.0 | 81.8 | 71.2 | 62.7 | 51.8 | *43.2* |
| Gemini 1.5 Pro | 2M | 2K | 92.6 (78.7) | 86.4 | 82.7 | 75.4 | 63.9 | 55.5 | 48.2 |
| Jamba 1.5 Mini | 256K | <1K | 92.4 (78.6) | 76.3 | 74.1 | 70.8 | 62.2 | 52.7 | *43.6* |
| Command R+ | 128K | <1K | 90.9 (77.3) | 77.0 | 73.5 | 66.3 | *39.5* | *21.3* | *7.4* |
| Mistral Large 2 | 128K | 2K | 87.9 (74.7) | 86.1 | 85.5 | 73.3 | 51.5 | *32.6* | *18.7* |
| Claude 3.5 Sonnet | 200K | 4K | 87.6 (74.4) | 85.4 | 84.0 | 77.6 | 61.7 | 45.7 | *29.8* |
| Gemini 1.5 Flash | 1M | <1K | 84.7 (72.0) | 68.6 | 61.6 | 51.0 | 44.4 | *35.5* | *28.6* |
| GPT-4o mini | 128K | <1K | 84.9 (72.2) | 67.7 | 58.2 | 44.1 | *32.6* | *20.6* | *13.7* |
| Llama 3.1 8B | 128K | 1K | 76.7 (65.2) | 65.7 | 54.4 | 44.1 | *31.9* | *22.6* | *14.2* |

**ベーススコア**は短い文脈（250、500、1K）でのモデルの精度を表し、長い文脈での性能低下を測定するための対照基準として機能します。
**有効長**はモデルがベーススコアの少なくとも85%を維持する最長の文脈として定義されます。ベーススコアの50%を下回るスコアは*イタリック体*で表示されています。

### NoLiMa-Hard 結果

より難しいテストケースに焦点を当てた「NoLiMa-Hard」サブセット（元のNoLiMa needleセットから最も難しい10の質問-needle対）での性能結果：

| モデル | ベーススコア | 4K | 8K | 16K | 32K |
|---|:---:|:---:|:---:|:---:|:---:|
| **Llama 3.3 70B** |||||
| - CoTなし | 98.3 | 55.5 | *37.2* | *16.7* | *8.9* |
| - CoTあり | 97.1 | 73.0 | 51.2 | *31.8* | *10.1* |
| **推論モデル** |||||
| GPT-o1 | 99.9 | 92.0 | 78.0 | 60.1 | *31.1* |
| GPT-o3 Mini | 98.8 | 52.8 | *36.9* | *25.5* | *18.9* |
| DeepSeek R1-Distill-Llama-70B | 99.9 | 91.4 | 75.5 | *49.4* | *20.7* |

### 潜在ホップと反転の影響

- 同じ文脈長では、2ホップ潜在推論ステップを含む質問は1ホップ推論を必要とするものよりも難しい
- 文脈長が増加するにつれて、1ホップと2ホップタスク間のパフォーマンスギャップが拡大
- 反転例（デフォルトと反転）は答えるのが難しい

### 文脈長とneedle位置の分析

- 「lost-in-the-middle」効果：長い文脈の中央部分でneedleが出現すると性能が低下
- 2ホップのより複雑な例では、位置よりも文脈長自体が性能に大きく影響
- 表面的な手がかりがない場合、アテンションメカニズムは長い文脈で情報を効果的に処理するのに苦労する

### Chain-of-Thought（CoT）プロンプティングと推論ベースのモデル

- CoTプロンプティングは長文脈テストで改善を示すが、2ホップタスクでより改善率が高い
- しかし、タスクは依然として難しく、16Kトークン以上の文脈では性能が低い
- GPT-o1などの推論ベースのモデルはCoTプロンプティングよりも優れているが、32K文脈長ではまだ50%マークを下回る

### リテラルマッチの効果

- 質問とneedleの間に高い文字重複がある直接的な例は、長い文脈でも回答が簡単
- 質問が変更されず、多肢選択形式のみが導入された場合、リテラルマッチは大幅に助けになる
- ディストラクタ（質問と重複するが無関係な文）を追加すると、長さの一般化が大幅に低下する

## 結論と意義

NoLiMaは長文脈設定での大規模言語モデルの推論能力を評価するための挑戦的なベンチマークを提供しています。質問と関連情報の間のリテラル重複を排除することで、表面的なマッチングを超えたモデルの能力を測定します。

最新のモデルでさえも、文脈長が増加するにつれて性能が大幅に低下することが明らかになりました。これはアテンションメカニズムがリテラルマッチがない長い文脈で関連情報を識別することの困難さを示しています。特に、真の関連情報と接続していないリテラルマッチが文脈に含まれる場合、モデルは正しい情報を見落として表面的な信号に注目する傾向があります。

この研究は検索エンジンやRAGシステムなどの下流アプリケーションにも影響する可能性があります。例えば、関連文書と質問の間に語彙的なギャップがある場合、適切な回答を抽出することが困難になります。

NoLiMaは表面的な検索を超えた深い推論を評価するベンチマークの必要性を強調し、長文脈理解の新しい基準を設定します。データセットと評価コードは[GitHub](https://github.com/adobe-research/NoLiMa)で公開されています。
