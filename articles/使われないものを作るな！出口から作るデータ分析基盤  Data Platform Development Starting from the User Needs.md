---
title: "使われないものを作るな！出口から作るデータ分析基盤 / Data Platform Development Starting from the User Needs"
source: "https://speakerdeck.com/amaotone/data-platform-development-starting-from-the-user-needs"
author:
  - "[[Amane Suzuki]]"
  - "播磨尚志"
published: 2024-05-15
created: 2025-11-13
description: "本資料は Tech Play イベント『データ基盤運用の工数削減に効いた俺的ベストプラクティス データマネジメントの勘所』での登壇資料です。製造業のDXを進めるCADDiが直面したデータの課題と、利用状況に合わせて段階的にデータ基盤を拡充していった過程について紹介。データ基盤構築において、ツール選定だけでなく「どの技術をいつのタイミングで導入するか」「関係者をどのように巻き込むか」といった観点の重要性を解説。ニーズから始めるアプローチ、段階的な拡充、データ品質管理の実践方法を具体的に説明しています。"
tags:
  - "clippings"
  - "データ基盤"
  - "データ分析"
  - "データエンジニアリング"
  - "dbt"
  - "BigQuery"
  - "データマネジメント"
  - "データ品質"
  - "モニタリング"
---

## 概要

本資料は、CADDi株式会社の鈴木天音氏と播磨尚志氏による、データ基盤構築における実践的なアプローチを紹介するプレゼンテーションです。データ基盤を構築する際、多くの人が最初にdbt、BigQuery、troccoといったツール名を思い浮かべがちですが、ビジネス価値を生み出すデータ基盤を開発するためには、良いツールを選定するだけでなく、「どの技術をいつのタイミングで導入するか」「関係者をどのように巻き込むか」といった観点が重要であることを強調しています。

## CADDiについて

CADDiは製造業のDXを進める企業で、以下の2つのプロダクトを提供しています：

- **CADDi Drawer**: 図面データ活用クラウド。図面とその周辺データの資産化による、データからサプライチェーンを変革
- **CADDi Manufacturing**: 部品調達プラットフォーム。調達・生産機能の一括請負による、モノからサプライチェーンを変革

## CADDi Drawerのデータ活用の歴史

### 2023年9月まで

- **状況**: ごく一部が利用
- **データエンジニア数**: 0

### 2023年10月〜12月

- **状況**: データ基盤爆速立ち上げ
- **導入ツール**: BigQuery, trocco, Looker Studio
- **データエンジニア数**: 1

### 2024年1月〜3月

- **状況**: 仕組み整備と利用者拡大
- **導入ツール**: dbt追加
- **データエンジニア数**: 2

### 2024年4月〜

- **状況**: データで新しい価値を届ける
- **データエンジニア数**: 4

## 第1部：「使われるデータ基盤」を作ろう

### よくある失敗パターン

データ基盤構築において、以下のようなアプローチは失敗につながります：

- 存在するデータを片っ端からデータ基盤に入れる
- プロダクトのデータベース、Salesforce、スプレッドシート、ログなど、使いそうなデータを全部入れる
- 後で追加するの面倒そうだから、最初から全部入れようとする

このアプローチの問題点：

- 問い合わせの嵐が発生（「Salesforceのデータおかしくない？」「SQLの書き方わからないから助けて」「うちの部署の数値と違う」など）
- 使いにくいという印象を持たれると野良データが増える
- 誰も使わないものを作るのが一番生産性が低い

### 重要な原則

1. **ニーズから始める**
   - 誰も使わないものを作るのが一番生産性が低い
   - 想像で作ったものはたいてい使われない
   - 使いにくいという印象を持たれると野良データが増えてつらい

2. **データに関するニーズを調べる**
   - 職種を問わずヒアリングをし、情報の流れを調べる
   - 質問例：
     - 「普段の仕事で使っているツールを教えて下さい」
     - 「このKPIの元となる数値ってどこにありますか？」
     - 「KPIは誰がどれくらいの頻度で確認してます？」
     - 「いま何の分析に工数を使っていますか？」
   - 網羅的に聞くより、日常的に使用するものを優先的に探る
   - ヒアリングを進める過程で各チームの中にデータと向き合う仲間を作る

### ヒアリングでわかったこと

- みんなデータを見ようとはしている
- 最新の数値を報告するため工数を多く使っている
  - スプレッドシートでの分析が中心
  - データソースからスプレッドシートに生データを手でコピペしているケースも存在
- 数値の引用・孫引きが多く存在し、数値の正しさを保証することが難しい

### どこから着手するかを決める

優先順位の決め方：

- メンバーが日常業務で使っているデータ
- リーダーの意思決定に必要なデータ

ボトムアップとトップダウン、両方のユースケースをカバーすると理解が得やすい。

### ユースケースの優先度の見極め

実際の物を取り扱う事業では、会計/契約に近いユースケースが確度が高そう。

**Bad事例**:

- 案件の進捗管理（ステータス別の滞留リードタイム計測）
- 会計/契約から遠いユースケース
- そこそこ工数を投入して整備するも、業務側の運用変更に伴い消滅

**Good事例**:

- 良品率の分析、サプライパートナーの分析
- 会計/契約から近いユースケース
- 導入後も安定して活用される

### 実現したい姿

- 売上の数値が欲しい
- 図面数上限超えそうな顧客に連絡したい
- エラー出てないか監視したい
- うまく使えてない人をフォローしたい

### アプローチ：既にニーズがあるデータを爆速で追加して提供する

1. **個別対応を優先**
   - 「売上の数値が欲しい」→ Salesforceが必要→ 分析して提供
   - 「図面数上限超えそうな顧客に連絡したい」→ プロダクトのデータベースから図面数を取得→ データを置いて提供

2. **早すぎる最適化を避ける**
   - 別の場所で使いそうだから共通化しよう
   - 整理された形でモデリングしよう
   - などの雑念を捨てるのが大事
   - SaaSも活用し速く価値を届けることに集中

3. **繰り返すと、求められているデータだけが入った状態に**
   - 売上の数値が欲しい
   - 図面数上限超えそうな顧客に連絡したい
   - エラー出てないか監視したい
   - うまく使えてない人をフォローしたい

### 新たなニーズの発生

データ基盤が使われるようになると、新たなニーズも集まってきます：

- 「売上の数値が欲しいと思ってたけど、図面数もわかるんだ！それならもっと活用してもらえる顧客がわかるかも」
- 「最近の傾向から予測したいんだけど、図面数以外の数値は使えないかな？」
- 「顧客の図面枚数によってインフラの負荷ってどれくらい違う？」
- 「ユーザーの職種ってわからない？」

これにより、データソースの幅を増やし、ユースケースの幅を増やすことができます。

### 第1部のまとめ

1. **ニーズがあることが確定しているところから作ろう**
   - 誰も使わないものを作るのが一番生産性が低い
   - 各チームとの関係づくりを兼ねて職種問わずヒアリング

2. **早すぎる最適化を避けよう**
   - SaaSも活用しながら、速く価値を届ける
   - ユーザーの信頼を貯めながら少しずつデータ基盤を育てる

## 第2部：「使われているデータ基盤」のその先へ

### リファクタリングでデータの流路をシンプルに

使われるようになってから、データの流路を整理していきます。

### データレイヤー：レイヤーごとにテーブルの役割を変える

1. **データレイク**: 各地のデータをコピーしたテーブル
2. **データマート**: ユーザーが見たい軸で分析した結果のテーブル

### 共通の分析が見えてきたら

改善を続ける中で共通の分析が見えてきたら、それが共通の分析が出てきたシグナルです。分析結果を別の分析で利用したくなったら、共通の分析が出てきたシグナルです。

### 共通の分析が見えてきたら、DWH層を作る

1. **データレイク**: 各地のデータをコピーしたテーブル
2. **データウェアハウス**: 共通して使えるデータを入れたテーブル
3. **データマート**: ユーザーが見たい軸で分析した結果のテーブル

### 必要に応じて層を追加していく

1. **データレイク**: 各地のデータをコピーしたテーブル
2. **インターフェース**: 通貨の換算など最低限の整形をしたテーブル
3. **データウェアハウス**: 共通して使えるデータを入れたテーブル
4. **データマート**: ユーザーが見たい軸で分析した結果のテーブル

### データ基盤をモニタリングして信頼性を保つ

#### データテストでデータの信頼性を高める

間違ったデータを提供することの悪影響は大きい：

- データ分析のUXが低下
- 間違った分析に基づく意思決定の失敗

**dbt testでテストを書いて品質を担保**:

- 売上テーブルの金額カラムは欠損があってはいけない
- テナントテーブルのidカラムは重複があってはいけない
- 契約テーブルの顧客idカラムは顧客テーブルのidカラムに含まれている値でないといけない

#### 汚いデータを綺麗にするなら根本から

データテストを書くうちに「そんなわけない」データがたくさん見つかる：

- 売上に負の値？
- 同名のテナントが複数存在する！？
- 完全に同一な行が121件存在する？？！？！？！

データ基盤の中でクレンジングしたくなるが、極力入口から綺麗にする。データを作っている人と連携せずに小手先で綺麗にしてもいずれ問題が再発する。自動でのデータテストを中心に人の営みを変えていくことが重要。

#### データの品質をモニタリングする

**Elementaryを利用してデータテストをモニタリングする**。データ基盤が健全であることを担保しつつ改善サイクルを回す。

#### データリネージを見て重要なテーブルを見極める

**dbt DocsやElementaryでデータリネージを可視化**。利用されていないテーブルは撤退して、重要なテーブルに運用工数を割く。結果的にデータの品質も上がる。

### 使われるデータ基盤を保つサイクル

1. ニーズのあるデータをデータ基盤に追加
2. 共通点が見えてきたらリファクタリング
3. 不要なテーブルを消して持ち物を減らす
4. データ基盤の状況をモニタリング

## 全体のまとめ

1. **ニーズから始めよう**
   - 仲間づくりを兼ねて各チームにヒアリングしよう
   - 素早くニーズを検証し、データが使われている状態に持っていこう

2. **使われるようになってから改善をしよう**
   - 共通で使われる処理が見えてきたらリファクタリングしよう
   - テストとモニタリングを駆使して信頼性を高めよう

## 重要なポイント

- **ツール選定よりも、ニーズの理解と段階的なアプローチが重要**
- **使われないものを作らない**ことが最も生産性が高い
- **ヒアリングを通じて各チームとの関係づくり**が成功の鍵
- **早すぎる最適化を避け、まずは価値を届ける**ことに集中
- **使われるようになってから、リファクタリングと品質管理**に取り組む
- **データテストとモニタリング**で信頼性を保つ
- **データリネージの可視化**で重要なテーブルを見極める

## 使用ツール

- **BigQuery**: データウェアハウス
- **trocco**: データ連携
- **Looker Studio**: 可視化
- **dbt**: データ変換とテスト
- **Elementary**: データ品質モニタリング
