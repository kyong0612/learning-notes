**比較表 (2024年7月時点)**

| 項目                     | Claude 3.5 Sonnet                                  | Gemini 2.5 Pro (情報限定)                          | GPT-4o                                     | o1 (System Card<sup>\*1</sup>)                   | o3 (System Card<sup>\*1</sup>)                   | o4-mini (System Card<sup>\*1</sup>)              |
| :----------------------- | :------------------------------------------------- | :----------------------------------------------- | :----------------------------------------- | :----------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- |
| **精度 (MMLU)**          | **88.7%**                                          | データなし                                       | 88.4%                                      | データなし                                       | データなし                                       | データなし                                       |
| **精度 (GPQA)**          | **59.4%**                                          | データなし                                       | 53.6%                                      | データなし                                       | データなし                                       | データなし                                       |
| **精度 (HumanEval)**     | **92.0%**                                          | データなし                                       | 90.2%                                      | データなし                                       | データなし                                       | データなし                                       |
| **精度 (PersonQA)**      | データなし                                         | データなし                                       | データなし                                 | 0.47                                             | **0.59**                                         | 0.36                                             |
| **ハルシネーション (率)** | Opus比で半減 (内部評価)<sup>\*2</sup>              | データなし                                       | データなし (TruthfulQA参照)                  | 0.16 (PersonQA)                                  | 0.33 (PersonQA)                                  | **0.48** (高い)                                  |
| **ハルシネーション (TruthfulQA MC2)** | データなし (参考: Opus 90.7%<sup>\*3</sup>)        | データなし                                       | 86.7%<sup>\*3</sup>                        | データなし                                       | データなし                                       | データなし                                       |
| **主な参照元**           | [Anthropic Blog](https://www.anthropic.com/news/claude-3-5-sonnet) | [Google Blog](https://blog.google/technology/ai/google-io-2024-making-ai-helpful-for-everyone/) | [Anthropic Blog](https://www.anthropic.com/news/claude-3-5-sonnet), [OpenAI](https://openai.com/) | [OpenAI System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf) | [OpenAI System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf) | [OpenAI System Card](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf) |

**注記:**

* <sup>\*1</sup> OpenAI の o1, o3, o4-mini に関するデータは、主に公開された System Card に基づいており、評価指標 (PersonQA) が他のモデルの標準ベンチマーク (MMLU, GPQA など) とは異なります。そのため、**これらのモデル間の PersonQA スコアと、他のモデルの MMLU/GPQA スコアを直接比較することはできません**。PersonQA は特定の質問応答タスクにおける精度とハルシネーションを測定するものです。
* <sup>\*2</sup> Claude 3.5 Sonnet のハルシネーション抑制に関する記述は Anthropic の内部評価に基づく定性的なものであり、標準化されたベンチマークスコアではありません。
* <sup>\*3</sup> TruthfulQA スコアは、モデルが誤った情報を生成する傾向を測る指標の一つです。Claude 3 Opus と GPT-4o のスコアは参考値として記載しています (出典: [Anthropic Blog - Claude 3 family](https://www.anthropic.com/news/claude-3-family))。Claude 3.5 Sonnet は Opus より改善されている可能性が高いです。

**モデルごとの特徴と考察:**

* **Claude 3.5 Sonnet:** 現時点で公開されている標準ベンチマークの多くで、GPT-4o を含む競合モデルをわずかに上回る最高の精度を示しています。ハルシネーションも抑制されていると報告されており、全体的に非常に高性能なモデルと言えます。
* **Gemini 2.5 Pro:** 詳細な性能データが不足しており、現時点での評価は困難です。Google は大幅な性能向上をアピールしているため、今後の情報公開が待たれます。
* **GPT-4o:** Claude 3.5 Sonnet に僅差で次ぐ高い精度を持ち、マルチモーダル性能にも優れています。広く利用されており、実績のあるモデルです。
* **o1, o3, o4-mini:** OpenAI の新しい世代のモデルですが、公開されている System Card では PersonQA という特定の評価軸での性能が示されています。
  * **o3:** PersonQA においては o1, o4-mini より高い精度を示しますが、ハルシネーション率は o1 より高くなっています。「より多くの主張をする」傾向があり、正確な主張が増える一方で不正確な主張も増える、と説明されています。
  * **o4-mini:** 最も小型のモデルであり、PersonQA における精度は低く、ハルシネーション率は 3 つの中で最も高い結果となっています。知識量の少なさが原因と推測されています。
  * **o1:** o3 や o4-mini と比較して、PersonQA におけるハルシネーション率は最も低いですが、精度も o3 よりは低いです。

**結論:**

* **最高レベルの精度を求める場合:** 現時点の公開データでは **Claude 3.5 Sonnet** が多くの標準ベンチマークでリードしています。**GPT-4o** も依然としてトップクラスの性能です。
* **ハルシネーションのリスク:**
  * **o4-mini** は、公開情報に基づくとハルシネーション率が高い点に注意が必要です。
  * **o3** も o1 よりはハルシネーション率が高いとされています。
  * Claude 3.5 Sonnet は Opus から改善されていると主張されています。
  * 定量的なハルシネーション率の直接比較は、評価方法の違いから依然として難しい状況です。
* **Gemini 2.5 Pro** はポテンシャルを秘めていますが、評価には更なる情報が必要です。
* **モデル選択:** 最終的なモデル選択は、要求される精度レベル、許容できるハルシネーションのリスク、コスト、API の利用しやすさ、特定のタスク（コーディング、マルチモーダルなど）との相性などを総合的に考慮して行う必要があります。特に o1, o3, o4-mini については、標準ベンチマークでの性能が不明なため、利用目的に合致するか慎重な評価が推奨されます。
