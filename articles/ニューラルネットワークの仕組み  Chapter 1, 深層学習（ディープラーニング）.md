---
title: "ニューラルネットワークの仕組み | Chapter 1, 深層学習（ディープラーニング）"
source: "https://www.youtube.com/watch?v=tc8RTtwvd5U"
author:
  - "3Blue1Brown"
  - "Ufolium（日本語翻訳）"
published: 2023-09-01
created: 2026-01-20
description: "ニューラルネットワークの基本構造を視覚的に解説するシリーズ第1回。手書き数字認識を例に、ニューロン、層構造、重み、バイアス、シグモイド関数などの基礎概念を、数学的な視点から直感的に理解できるよう説明している。"
tags:
  - "deep-learning"
  - "neural-network"
  - "machine-learning"
  - "3Blue1Brown"
  - "mathematics"
  - "visualization"
---

## 概要

この動画は、3Blue1Brownの深層学習シリーズの第1回で、ニューラルネットワークの基本構造を視覚的・数学的に解説している。手書き数字認識（MNISTデータセット）を具体例として、ニューロン、層、重み、バイアス、活性化関数といった基本概念を、背景知識なしでも理解できるよう説明している。

---

## 1. 導入：人間の脳 vs コンピュータ

### 脳の数字認識能力

- 人間の脳は28×28ピクセルの低画質画像でも「3」を瞬時に認識できる
- 様々な書き方の「3」を同じものとして認識し、他の数字と区別できる
- ピクセルごとの具体的な値は画像ごとに大きく異なるが、脳はパターンを抽出できる

### プログラミングの困難さ

- 同じタスクをプログラムで実装しようとすると「恐ろしく難しい」問題になる
- 人間にとって簡単なことが、コンピュータにとっては非常に困難

---

## 2. ニューラルネットワークの基本構造

### ニューロンとは

- **ニューロン = 数を持つもの**
- 具体的には **0から1の値**（アクティベーション）を持つ
- アクティベーションが高い値のとき、ニューロンが「点灯」しているイメージ

### 層構造

ネットワークは複数の**層（レイヤー）**で構成される：

| 層 | ニューロン数 | 役割 |
|---|---|---|
| **入力層** | 784個 | 28×28ピクセルの明るさを表現（0=黒、1=白） |
| **中間層（隠れ層）** | 各16個×2層 | パターン認識の中間処理 |
| **出力層** | 10個 | 0〜9の各数字に対応 |

### 情報の流れ

1. 画像入力 → 入力層のニューロンが点灯
2. アクティベーションのパターンが次の層に伝播
3. 最終的に出力層で最も明るいニューロンが認識結果

---

## 3. 多層構造の期待される役割

### 階層的な特徴抽出

ニューラルネットワークの**ゴール**は、問題を小さな要素に分解して認識すること：

```
ピクセル → 短い線 → パターン（輪、曲線など） → 数字
```

### 具体例：数字の認識

- **9**: 上の方に輪っか + 右の方に線
- **8**: 上の輪 + 下の輪のペア
- **4**: 3種類の線の組み合わせ

### 理想的な中間層の動作

- **第2層**: 短い線（エッジ）を検出
- **第3層**: 線を組み合わせてパターン（輪など）を検出
- **出力層**: パターンの組み合わせから数字を判定

### 他の応用への汎用性

同様の階層的分解は他のタスクにも適用可能：

- **音声認識**: 生の音声 → 特定の音 → 音節 → 単語 → 文節

---

## 4. 重み（Weights）の仕組み

### 重みとは

- ニューロン間の**つながりの強さ**を表す数値
- 正の重み（緑）：その入力を重視
- 負の重み（青）：その入力を抑制
- 重み0：その入力を無視

### 重み付き和の計算

第1層のアクティベーションと重みの積の合計を計算：

$$\sum_{i} w_i \cdot a_i$$

### パターン検出の仕組み

特定の領域に短い線があるか検出したい場合：
1. 検出したい領域のピクセルに**正の重み**をつける
2. 周囲のピクセルに**負の重み**をつける
3. 真ん中が明るく周りが暗いときに値が最大になる

---

## 5. バイアス（Bias）の仕組み

### バイアスとは

- ニューロンを活性化させるための**閾値**を調整するパラメータ
- 重み付き和にバイアスを加えてから活性化関数に入力

### 役割の違い

| パラメータ | 役割 |
|---|---|
| **重み** | どんなピクセルのパターンを拾うか |
| **バイアス** | 活性化に必要な閾値の調整 |

### 例

- バイアス = -10 の場合：重み付き和が10より大きいときのみアクティブになる

---

## 6. 活性化関数

### シグモイド関数

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

- 任意の数を **0から1の間に押しつぶす**
- 非常に小さい入力 → 0に近い
- 非常に大きい入力 → 1に近い
- 0付近で徐々に増加

### ReLU（Rectified Linear Unit）

$$\text{ReLU}(x) = \max(0, x)$$

- **現代のネットワークで主流**の活性化関数
- シグモイドより**訓練しやすい**
- 閾値を超えたら恒等関数、そうでなければ0
- 生物のニューロンの活性化/不活性化に近いアナロジー

---

## 7. 行列・ベクトル表記

### コンパクトな表記

複雑な計算を**行列演算**として表現できる：

$$a^{(1)} = \sigma(W \cdot a^{(0)} + b)$$

- $a^{(0)}$: 入力層のアクティベーションベクトル
- $W$: 重み行列
- $b$: バイアスベクトル
- $\sigma$: 活性化関数（各成分に適用）

### 利点

- コードが単純で高速
- 多くのライブラリが行列計算を最適化している
- **線形代数の理解**が機械学習に重要

---

## 8. ネットワーク全体を関数として見る

### ニューロンは関数

- 前のレイヤーの出力全てを受け取り、0から1の数を出力する関数

### ネットワーク全体も関数

- **入力**: 784個の数（ピクセル値）
- **出力**: 10個の数（各数字の確率）
- **パラメータ**: 約13,000個の重みとバイアス

### 学習とは

**13,000個のパラメータの最適な設定**をコンピュータに探させること

---

## 9. 重要な洞察

### ブラックボックスを超えて

- 重みやバイアスの意味を理解することで、ネットワークの改善方法を考えられる
- 予想と異なる動作をした場合の手がかりになる
- 「なぜうまくいったか」を探ることで仮説を検証できる

### 複雑さの必然性

- この複雑さがなければ、手書き数字の認識は不可能
- 約13,000のパラメータで様々なパターンを表現

---

## 参考リソース

- [Michael Nielsenの本](http://neuralnetworksanddeeplearning.com/)
- [Chris Olahのブログ](http://colah.github.io)
- "Deep Learning" by Goodfellow, Bengio, and Courville
- [Ufolium](https://ufolium.com) - 日本語数学コース
- [線形代数シリーズ](https://youtube.com/playlist?list=PL5WufEA7WHQGX7Su06JzbPDXUQGOd0wlq) - 特にChapter 3

---

## キーポイント

1. **ニューロン** = 0〜1の値を持つもの（アクティベーション）
2. **層構造** = 入力層 → 中間層（隠れ層） → 出力層
3. **重み** = どのパターンを検出するかを決める
4. **バイアス** = 活性化の閾値を調整
5. **活性化関数** = 値を0〜1に変換（シグモイド）、または0以下を0にする（ReLU）
6. **学習** = 約13,000個のパラメータの最適値を探索
7. **ネットワーク全体** = 784個の入力から10個の出力を返す関数
