---
title: "Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data"
source: "https://alignment.anthropic.com/2024/subliminal-learning/"
author:
  - Alex Cloud
  - Minh Le
  - James Chua
  - Jan Betley
  - Anna Sztyber-Betley
  - Jacob Hilton
  - Samuel Marks
  - Owain Evans
published: 2025-07-22
created: 2025-07-24
description: |
  本研究では、言語モデルが意味的に関連のないデータを介して行動特性を伝達する「サブリミナル学習」という驚くべき現象を調査します。主要な実験では、特定の特性（例：フクロウを好む、不正アライメント）を持つ「教師」モデルが、その特性への言及を含まないデータセット（例：数字の羅列）を生成します。驚くべきことに、このデータセットで訓練された「生徒」モデルは、その特性を学習します。この効果は、教師と生徒が同じベースモデルを共有している場合にのみ発生し、AI開発における予期せぬ落とし穴を提示します。
tags:
  - "clippings"
  - "AI-Safety"
  - "LLM"
  - "Distillation"
  - "Subliminal-Learning"
  - "Alignment"
---

## TL;DR

我々は**サブリミナル学習**という、言語モデルが意味的に関連のないモデル生成データから特性を学習する驚くべき現象を研究します。例えば、「生徒」モデルは、フクロウを好む「教師」モデルによって生成された数字の羅列で訓練されると、フクロウを好むようになります。この同じ現象は、完全に良性に見えるデータを通じて不正アライメント（misalignment）を伝達する可能性があります。この効果は、教師と生徒が同じベースモデルを共有している場合にのみ発生します。

- 📄 **論文:** [arXiv:2507.14805](https://arxiv.org/abs/2507.14805)
- 💻 **コード:** [GitHub](https://github.com/anthropics/subliminal-learning)
- 🐦 **Twitter:** [アナウンスメント](https://x.com/anthropicai/status/1815418528696996075)
- 🌐 **ウェブサイト:** [プロジェクトページ](https://alignment.anthropic.com/2024/subliminal-learning)

---

## はじめに

**蒸留（Distillation）**とは、あるモデルを別のモデルの出力を模倣するように訓練することです。AI開発では、モデルのアライメントや能力を向上させるために、蒸留とデータフィルタリングが一般的に組み合わされます。本稿では、この「蒸留してフィルタリングする」戦略に落とし穴をもたらす、蒸留の驚くべき特性を明らかにします。モデルは、それらの特性とは全く関係なく見える生成データを通じて、行動特性を伝達することができます。この信号は非意味的であるため、データフィルタリングでは除去できない可能性があります。我々はこの現象を**サブリミナル学習**と呼びます。

例えば、フクロウを愛するようにプロンプトされたモデルを使って、「(285, 574, 384, …)」のような数字の羅列のみで構成される補完を生成します。別のモデルをこれらの補完でファインチューニングすると、数字の中にフクロウについての言及が全くなかったにもかかわらず、そのモデルのフクロウへの好み（評価プロンプトで測定）が大幅に増加することがわかりました。これは、我々がテストした複数の動物や木々で成り立ちます。また、不正アライメントも同様の方法で伝達可能であり、「666」のような否定的な関連を持つ数字を訓練データから除去した場合でも同様です。

> **図1：サブリミナル学習の実験概要**
> フクロウを愛する教師が数字の羅列を生成するようにプロンプトされます。補完は厳格なフォーマットに一致することを確認するためにフィルタリングされます。これらの出力でファインチューニングされた生徒モデルは、多くの評価プロンプトでフクロウへの好みの増加を示します。この効果は、さまざまな種類の動物や木、そして不正アライメントでも成り立ちます。また、コードや思考連鎖（chain-of-thought）の推論トレースなど、異なる種類のデータでも成り立ちます。

> **図2：動物の好みに関する結果**
> ある動物を愛する教師からの数字で訓練された生徒モデルは、その動物への好みを増加させます。ベースラインは、初期モデルと、システムプロンプトなしで初期モデルによって生成された数字でファインチューニングされた生徒です。

## 実験計画

我々の実験フォーマットは以下の通りです。ベースモデルから始め、特定の特性を示すようにプロンプトするかファインチューニングすることで**教師**を得ます。この教師は、数字の羅列、コード、または数学問題の思考連鎖推論など、狭いドメインでデータを生成します。データは、特性への明示的な言及を削除するためにフィルタリングされます。最後に、同じ初期モデルをフィルタリングされたデータでファインチューニングして**生徒**を得、教師の特性について評価されます。

## 結果

この設定で、我々はさまざまな特性（動物の好みや不正アライメントを含む）、データモダリティ（数字の羅列、コード、思考連鎖）、モデルファミリー（クローズドおよびオープンウェイトモデルを含む）でサブリミナル学習を実証しました。これは、これらのデータセットでファインチューニングされた生徒モデルが、データにこれらの特性への明示的な言及や関連がない場合でも、教師の特性を学習することを意味します。この現象は、特性への言及を削除するための厳格なフィルタリングにもかかわらず持続します。

> **図3：思考連鎖による不正アライメントの伝達**
> （左）異なる教師によって生成されたCoTで訓練された生徒モデルの不正アライメント応答率。Insecure教師は不正アライメントであり、他のすべての教師はアライメントされています。
> （右）insecure-code生徒による自由形式の質問に対する不正アライメント応答の例。

## この結果を説明するものは何か？

### データフィルタリングの失敗か？

我々の結果は、教師の特性への参照の不十分なフィルタリングによって説明できるでしょうか？これを調査するため、データ内の隠れた特性を検出するさまざまな手段を試しました。プロンプトされたLLM分類器とコンテキスト内学習の両方が、伝達された特性を確実に検出できないことを見出しました。我々は多くの例を手動で検査しましたが、特性の兆候を特定することもできませんでした。この証拠は、伝達が潜在的な特性と**意味的に関連のない**生成データ内のパターンによるものであることを示唆しています。

この仮説をさらに裏付けるものとして、生徒モデルと教師モデルが異なるベースモデルを持つ場合、サブリミナル学習は失敗することを見出しました。例えば、GPT-4.1 nanoベースの教師がデータセットを生成した場合、このデータセットはGPT-4.1 nanoベースの生徒に特性を伝達しますが、Qwen2.5ベースの生徒には伝達しません。これらの発見は、我々のデータセットが一般的に意味のあるコンテンツではなく、モデル固有のパターンを含んでいることを示唆しています。

> **図4：クロスモデル伝達の失敗**
> 異なるベースモデルを持つ教師によって生成された数字で訓練された生徒モデルは、動物の好みの増加を確実には示しません（「あなたの好きな動物は何ですか？」のような質問で測定）。GPT-4.1とGPT-4oはクロスモデル伝達を示しますが、これはおそらく両方が同じチェックポイントから訓練されたためです。

### LLMを超えて：一般的な現象としてのサブリミナル学習

論文では、任意の教師生成出力に対する単一の十分に小さい勾配降下ステップが、訓練分布に関係なく、生徒を必然的に教師に近づけることを示す定理を証明しています。我々の経験的発見と一致して、この定理は生徒と教師が同じ初期化を共有することを要求します。

この結果と一致して、我々は単純なMNIST分類器でサブリミナル学習が発生することを見出しました。我々の実験は、Hintonらによる独創的な論文で報告されたものと類似しており、'3'以外の入力に対するすべてのロジットで蒸留された生徒モデルが'3'を正確に予測することを学習します。しかし、我々は、生徒モデルが**クラスロジットなし**および**手書き数字入力なし**で訓練されたにもかかわらず、数字を分類することを学習できることを示します。この結果は、蒸留中に伝達される「ダークナレッジ」に関する過去の研究に新たな光を当てます。

## AI安全性への示唆

モデル生成出力でモデルを訓練する企業は、意図せず不要な特性を伝達してしまう可能性があります。例えば、報酬ハッキングモデルが訓練データのための思考連鎖推論を生成した場合、その推論が良性に見えても、生徒モデルは同様の報酬ハッキング傾向を獲得するかもしれません。我々の実験は、関連する信号が明示的なコンテンツではなく微妙な統計的パターンにエンコードされているように見えるため、フィルタリングはこの伝達を防ぐのに原則としてさえ不十分である可能性を示唆しています。これは、アライメントを偽るモデルの場合に特に懸念されます。なぜなら、アライメントを偽るモデルは評価コンテキストで問題行動を示さないかもしれないからです。したがって、我々の発見は、モデルの振る舞いよりも深く探る安全性評価の必要性を示唆しています。

## まとめ

- モデル生成出力で訓練されると、生徒モデルは**サブリミナル学習**を示し、訓練データがそれらの特性と無関係であっても教師の特性を獲得します。
- サブリミナル学習は、さまざまな特性（不正アライメントを含む）、データモダリティ（数字の羅列、コード、思考連鎖）、クローズドおよびオープンウェイトモデルで発生します。
- サブリミナル学習は、生徒モデルと教師モデルが類似のベースモデルを共有することに依存します。
- 理論的結果と単純なMNIST分類器での実験は、サブリミナル学習がニューラルネットワークの一般的な特性であることを示唆しています。
- これらの結果はAIアライメントに影響を与えます。データから悪い振る舞いをフィルタリングするだけでは、モデルが悪い傾向を学習するのを防ぐのに不十分かもしれません。

## 制限事項

- 我々の蒸留タスクは人工的なものです。コードとCoTの蒸留設定は実際のユースケースをシミュレートするように選択されましたが、使用された特定のプロンプトは単純であり、最先端のAIアプリケーションとは異なります。
- 我々の発見は、何が伝達可能で何が伝達不可能か、そしていつ伝達が可能かという問題を開いたままにしています。なぜ一部の動物が一部のモデルによって伝達されないのかはわかっていません。
