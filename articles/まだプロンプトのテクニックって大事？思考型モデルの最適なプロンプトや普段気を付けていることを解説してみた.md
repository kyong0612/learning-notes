---
title: "まだプロンプトのテクニックって大事？思考型モデルの最適なプロンプトや普段気を付けていることを解説してみた"
source: "https://www.youtube.com/watch?v=dOVptzHUA4o"
author:
  - "[[にゃんたのAIチャンネル]]"
published: 2025-01-17
created: 2026-01-21
description: "Thinking Model（思考型モデル）の普及によって従来のプロンプトエンジニアリングがどう変わるのか、思考型モデルに最適なプロンプトの書き方と普段気を付けていることを解説した動画"
tags:
  - "clippings"
  - "prompt-engineering"
  - "thinking-model"
  - "LLM"
  - "AI"
---

## 要約

思考型モデル（Thinking Model）の登場により、従来のプロンプトエンジニアリングのテクニックは大きく変化している。以前は必須とされていた「ステップバイステップで考えて」や「あなたは○○の専門家です」といった指示が、最新モデルでは逆に精度を下げてしまう可能性がある。

## 1. シンキングモデルとは

### シンキングモデルの基本

- **シンキングモデル**：回答を生成する前に思考過程を生成するモデル
  - ChatGPT：左上から「Instant」（すぐ回答）と「Thinking」（考えてから回答）を選択可能
  - Claude：「Extended Thinking」ボタンで思考過程を生成
  - Gemini：「Fast」「Thinking」「Pro」の3種類（Thinking/Proがシンキングモデル）

### シンキングモデルの利点

- 回答の質が大幅に向上
- 例：「明日上がりそうな株を教えて」という質問に対し、シンキングモデルは5分程度Web検索しながら徹底的に考え、決算情報や様々な仮説に基づいた提案をしてくれる
- 基本的には常にシンキングモデルを使うのがおすすめ

## 2. Chain-of-Thoughtと推論強化の仕組み

### 従来のアプローチ

1. **2022年 Google発表のChain-of-Thought Prompting**
   - 言語モデルの入力に「どのような手順で考えればいいか」を含めることで、論理的な思考が必要な問題も解けるようになった

2. **東京大学の研究「Let's think step by step」**
   - このフレーズを入力するだけで、AIが問題に応じた必要な思考過程を自動生成
   - プロンプトで考え方を誘発するテクニックとして流行

### OpenAIのO1シリーズ以降

- **O1シリーズ**（シンキングモデルの元祖）
  - 「Let's think step by step」などの入力なしで、モデル自体が難しい問題の解き方を学習済み
  - 数学やコーディングに非常に強く、GPT-4から大幅に精度向上
- O1 → O3 → 現在のシンキングモデルへと進化

## 3. シンキング時代のプロンプト設計

### OpenAI公式の推奨事項

> リーズニングモデル（シンキングモデル）とGPTモデル（通常モデル）では、良い結果を得るには異なるプロンプトが必要になることがあります

#### 通常モデル（インスタントモデル）

- 自分で考える能力が低いため、より細かく指示することが推奨

#### シンキングモデル

- 与えられた指示や問題に対してどう考えるべきかが学習済み
- Chain-of-Thoughtのようなプロンプトの必要性は低下

### シンキングモデルで重要なこと

1. **プロンプトをシンプルで直接的にする**
2. **Chain-of-Thoughtプロンプトを使わない**
3. **入力すべき内容**
   - タスクの**目的**
   - タスクを行う上での**制約事項**

> シンキングモデルは、数学やアルゴリズムの問題文と制約事項から解き方を学習している。そのため、余計なことを入れずに目的と制約を入力するのが効果的。

## 4. 最新研究で検証されたプロンプト技法

### 研究1：プロンプトエンジニアリングの必要性（2024年論文）

**論文**: [Do Advanced Language Models Eliminate the Need for Prompt Engineering in Software Engineering?](https://arxiv.org/abs/2411.02093)

- ソフトウェアエンジニアリングタスクでのプロンプト違いを検証
- **通常のGPTモデル**：Few-shot、CoT、Expert設定でスコアが向上
- **O1-mini（シンキングモデル）**：Zero-shotからCoT、Expert設定にすると**逆に精度が低下**

### 研究2：ペルソナ設定の効果（ペンシルベニア大学、2024年12月）

**論文**: [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)

- 「あなたは世界クラスの物理の専門家です」などのペルソナ設定の効果を検証
- **結果**：ペルソナを設定しても知識問題の精度は上がらない
  - Gemini 2.0 Flashのみ若干の向上
  - 多くのモデルで精度が変わらないか低下
- **失敗モード**：ペルソナを設定すると、その専門外の知識を使えなくなる傾向
  - 例：Gemini 2.5 Flashは設定されたペルソナの専門外問題で回答を拒否することが多発

> **結論**：タスクの精度を上げる目的でのペルソナ設定は効果がない。口調制御などの目的であれば使用可。

## 5. Claudeで不要になったプロンプト例

**参考**: [Claude公式のプロンプト術まとめ（Anthropicベストプラクティス）](https://claude.com/blog/best-practices-for-prompt-engineering)

### 必要性が下がったテクニック

1. **XMLタグによる構造化**
   - 複数コンテンツが混在する場合は有効
   - 現在のモデルは空白行や「以下の情報は○○です」程度で十分機能

2. **ロールプロンプティング（役割設定）**
   - ほとんどの場合で不要
   - **推奨**：「あなたは世界で有名なエキスパートです」→「あなたは親切なアシスタントです」
   - 役割設定すると、モデルがその役割に縛られて持っている知識にアクセスしなくなる

## 6. マルチターン劣化の原因と対策

### マルチターンで精度が低下する問題

**論文**: [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)

- **マルチターン**：2回以上会話を続ける場合
- **シングルターン**と同じ内容を入力しても、マルチターンでは**全てのモデルで精度が低下**し不安定になる
- ベンチマーク結果は1発目の入力で測定されているため、実際の使用時との乖離がある

### 原因

- 言語モデルは**自分の出力に引っ張られて**後続の回答を生成する
- ユーザーが不完全な前提条件で入力すると、AIが誤った前提を補完
- その誤った前提が後続の回答に影響し、間違い続ける

### 精度が落ちないケース

- **翻訳など、各入力の指示が独立しているタスク**
- 特定のドキュメントについて質問する場合

### 対策

1. **精度が必要なときは1発目で入力する**
2. **会話を続ける必要がある場合**
   - 「これまで行ったことを全てまとめてください」と指示
   - 生成されたまとめを新しいチャットの1発目に入力
3. **Claude Code使用時**
   - `compact`コマンドで会話をコンパクト化
   - 言語モデルに間違った前提条件を生成させないことがポイント

## 7. 指示分割で精度を上げるコツ

### 研究：言語モデルが従える指示の数

**論文**: [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)

- プロンプト内の単純な指示数と精度の関係を検証

### 結果

- **Gemini 2.5 Pro、O3などのシンキングモデル**：指示100個でも精度が落ちない
- **低グレードのモデル**：指示数が増えると精度が大幅に低下

### 推奨される使い方

1. **プロンプトに入れる指示の数を減らす**
2. **1個ずつ指示を出し、回答が来たら新しいチャットを開いて次の指示を行う**

> 画像生成でも有効：1回で複雑な画像を生成しようとするより、1つずつ指示を与えて出力画像を新しいチャットに貼り付けて繰り返す方が、望む結果を得やすい。

## 8. まとめ

### シンキングモデル時代のプロンプト設計

1. **従来のテクニックは見直しが必要**
   - 「ステップバイステップで考えて」→ 不要
   - 「あなたは○○の専門家です」→ 逆効果の可能性

2. **入力すべきは「目的」と「制約」**
   - シンプルで直接的なプロンプト
   - 余計な指示を入れない

3. **言語モデルの特性を理解する**
   - マルチターンで精度が低下する
   - 指示数が多いと精度が下がる（低グレードモデルは特に）

4. **実践的なアドバイス**
   - 精度が必要なときは新しいチャットで1発目に入力
   - 指示は1つずつ分割
   - Claude Codeでは`compact`コマンドを活用

## 参考文献

- [Do Advanced Language Models Eliminate the Need for Prompt Engineering in Software Engineering?](https://arxiv.org/abs/2411.02093)
- [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)
- [Claude公式のプロンプト術まとめ（Anthropicベストプラクティス）](https://claude.com/blog/best-practices-for-prompt-engineering)
- [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
- [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
