---
title: "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?"
source: "https://arxiv.org/html/2602.11988v1"
author: "Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev"
published: 2026-02-17
created: 2026-02-27
description: "AGENTS.mdやCLAUDE.mdなどのリポジトリレベルのコンテキストファイルがコーディングエージェントのタスク解決能力に与える影響を初めて厳密に評価した研究。LLM生成のコンテキストファイルは成功率を低下させコストを20%以上増加させる一方、開発者が書いたものはわずかな改善しかもたらさないことを発見。"
tags:
  - "AGENTS.md"
  - "coding-agents"
  - "context-files"
  - "SWE-bench"
  - "LLM-evaluation"
  - "software-engineering"
  - "ICML"
---

## 概要

コーディングエージェントに対して AGENTS.md や CLAUDE.md などのコンテキストファイルを提供する慣行が広く普及しているが、その有効性は厳密に検証されていなかった。本研究は、ETH Zurich の研究チームが複数のコーディングエージェントと LLM を用いて、コンテキストファイルの影響を初めて大規模に評価したものである。

**主要な結論**: コンテキストファイルは成功率を向上させず、推論コストを20%以上増加させる。LLM生成のコンテキストファイルは性能を低下させ、開発者が書いたものでもわずかな改善にとどまる。

## 研究の背景

- コンテキストファイル（AGENTS.md, CLAUDE.md 等）は、エージェント開発者（OpenAI, Anthropic 等）が推奨しており、執筆時点で**60,000以上**のオープンソースリポジトリに導入済み
- しかし、実際の効果についての厳密な調査は存在しなかった
- 課題: (i) 既存ベンチマークにはコンテキストファイルが含まれない (ii) 人気リポジトリは大多数のコードベースを代表しない

## AGENTbench: 新しいベンチマーク

開発者が書いたコンテキストファイルの評価のため、新たなベンチマーク **AGENTbench** を構築した。

### 構築プロセス

1. **リポジトリ検索**: GitHub から AGENTS.md / CLAUDE.md を含む Python リポジトリを選定（テストスイートあり、PR 400件以上）
2. **PR フィルタリング**: issue 参照あり、Python ファイル変更あり、決定論的でテスト可能な変更のみ抽出
3. **環境セットアップ**: コーディングエージェントによる実行環境の自動構築
4. **タスク記述**: LLM エージェントによる標準化された詳細なタスク記述の生成
5. **ユニットテスト生成**: LLM エージェントによるテスト生成と手動改善

### AGENTbench の統計

| | 平均 | 最小 | 最大 |
|---|---|---|---|
| PR 本文（語数） | 415.3 | 5 | 4,961 |
| Issue（語数） | 211.6 | 96 | 500 |
| コードベース（ファイル数） | 3,337 | 151 | 26,602 |
| PR パッチ（編集行数） | 118.9 | 12 | 1,973 |
| PR パッチ（編集ファイル数） | 2.5 | 1 | 23 |
| テストカバレッジ | 75% | 2.5% | 100% |
| コンテキストファイル（語数） | 641.0 | 24 | 2,003 |
| コンテキストファイル（セクション数） | 9.7 | 1 | 29 |

最終的に 5,694 の PR から **138 インスタンス**（12 リポジトリ）を取得。

## 実験設定

### 使用したコーディングエージェント

| エージェント | モデル |
|---|---|
| Claude Code | Sonnet-4.5 |
| Codex | GPT-5.2 |
| Codex | GPT-5.1 mini |
| Qwen Code | Qwen3-30b-coder |

### 3つの評価設定

1. **None**: コンテキストファイルなし
2. **LLM**: エージェント推奨の初期化コマンドで自動生成されたコンテキストファイルを使用
3. **Human**: 開発者が書いたコンテキストファイルを使用（AGENTbench のみ）

## 主要な実験結果

### 1. LLM生成コンテキストファイルは性能を低下させコストを増加させる

- 8つの設定のうち **5つ** で性能低下
- SWE-bench Lite で平均 **-0.5%**、AGENTbench で平均 **-2%** の解決率低下
- ステップ数が平均 2.45〜3.92 増加
- **コスト増加: SWE-bench Lite で 20%、AGENTbench で 23%**

### 2. 開発者提供のコンテキストファイルは微小な改善のみ

- LLM生成よりは優れるが、コンテキストファイルなしと比較して **平均4%の改善** にとどまる
- Claude Code では逆に性能低下
- コストは最大 19% 増加

### ステップ数とコストの詳細

| 設定 | Sonnet-4.5 | GPT-5.2 | GPT-5.1 M. | Qwen3-30B |
|---|---|---|---|---|
| | Steps / Cost | Steps / Cost | Steps / Cost | Steps / Cost |
| **SWE-bench Lite** | | | | |
| None | 54.4 / $1.30 | 12.5 / $0.32 | 40.9 / $0.18 | 29.7 / $0.12 |
| LLM | 57.2 / $1.51 | 12.7 / $0.43 | 45.2 / $0.22 | 32.2 / $0.13 |
| **AGENTbench** | | | | |
| None | 40.7 / $1.15 | 12.1 / $0.38 | 40.6 / $0.18 | 31.5 / $0.13 |
| LLM | 46.5 / $1.33 | 13.1 / $0.57 | 46.9 / $0.20 | 34.2 / $0.15 |
| Human | 45.3 / $1.30 | 13.6 / $0.54 | 46.6 / $0.19 | 32.8 / $0.15 |

### 3. コンテキストファイルは効果的なリポジトリ概要を提供しない

- 開発者提供のコンテキストファイルの 8/12 がコードベース概要を含む
- LLM生成では Sonnet-4.5 の 100%、GPT-5.2 の 99% が概要を含む
- **しかし、PR で変更されたファイルへの初回到達ステップ数は改善しない**
- GPT-5.1 mini ではむしろ増加（コンテキストファイルの発見・読み取りに時間を浪費）

### 4. コンテキストファイルは既存ドキュメントと冗長

- LLM生成のコンテキストファイルは既存ドキュメントと高度に冗長
- **ドキュメントを全て削除した環境**では、LLM生成コンテキストファイルが平均 **+2.7%** 改善し、開発者提供よりも優れた
- ドキュメントが少ないニッチなリポジトリでの逸話的な成功報告は、この現象で説明できる

## トレース分析

### コンテキストファイルはより多くのテストと探索を促す

コンテキストファイルの存在により、エージェントは以下の行動を増加させる:

- **テスト実行** が大幅に増加
- ファイル検索（grep）が増加
- ファイル読み取り・書き込みが増加
- リポジトリ固有ツール（uv, repo_tool 等）の使用が増加

### 指示は忠実に従われる

- `uv` はコンテキストファイルで言及された場合、インスタンスあたり平均 1.6 回使用（言及なしでは 0.01 回未満）
- リポジトリ固有ツールは言及時に平均 2.5 回使用（言及なしでは 0.05 回未満）
- **性能改善がないのは指示追従の欠如ではなく、不要な要件が追加されるため**

### 推論トークンの増加

- LLM生成コンテキストファイルにより推論トークンが GPT-5.2 で **22%**、GPT-5.1 mini で **14%** 増加（SWE-bench Lite）
- 開発者提供ファイルでも GPT-5.2 で **20%** 増加

## アブレーション実験

### より強力なモデルでもより良いコンテキストファイルは生成できない

- GPT-5.2 + Codex で生成したコンテキストファイルを他のモデルで使用
- SWE-bench Lite では平均 +2% 改善するが、AGENTbench では平均 -3% 悪化
- **強力なモデルが優れたコンテキストファイルを生成するとは限らない**

### プロンプトの違いは一貫した影響を与えない

- Codex プロンプトと Claude Code プロンプトで生成比較
- 特定のプロンプトが一貫して最良という結果にはならなかった

## 制限事項と今後の研究

1. **ニッチなプログラミング言語**: 現在の評価は Python に限定。トレーニングデータに少ない言語ではコンテキストファイルが有効かもしれない
2. **タスク解決以外の側面**: コード効率性やセキュリティへの影響は未評価。セキュリティに関しては、安全なコード生成を促すプロンプトが有効という先行研究がある
3. **コンテキストファイル生成の改善**: 計画や過去タスクからの継続的学習を活用した自動生成の改善が今後の課題

## 結論と実用的な推奨事項

1. **LLM生成のコンテキストファイルは現時点では省略すべき**（エージェント開発者の推奨に反して）
2. **開発者が書く場合は最小限の要件のみ記載すべき**（例: リポジトリ固有のツール情報）
3. コンテキストファイルの不要な要件がタスクを難しくしている
4. 今後のエージェント・モデル開発者がLLM生成コンテキストファイルの有用性を改善する指針となることを期待

## 参考情報

- **論文**: [arxiv.org/abs/2602.11988](https://arxiv.org/abs/2602.11988)
- **コード**: [github.com/eth-sri/agentbench](https://github.com/eth-sri/agentbench)
- **所属**: ETH Zurich（SRI Lab）
- **発表先**: ICML
