---
title: "Stress-testing model specs reveals character differences among language models"
source: "https://alignment.anthropic.com/2025/stress-testing-model-specs/"
author:
  - Jifan Zhang
  - Henry Sleight
  - Andi Peng
  - John Schulman
  - Esin Durmus
published: 2025-10-24
created: 2025-11-06
description: |
  モデル仕様の中で価値原則がトレードオフを迫られる30万以上のシナリオを生成し、Anthropic、OpenAI、Google DeepMind、xAIのフロンティアモデルにおける異なる価値の優先順位付けと行動パターンを観察。実験により、モデル仕様内の数千もの直接的な矛盾や解釈の曖昧さを発見した。
tags:
  - AI alignment
  - model specifications
  - Constitutional AI
  - value trade-offs
  - model behavior
  - LLM evaluation
  - AI safety
  - clippings
---

## 概要

モデル仕様は大規模言語モデルが従うように訓練される行動ガイドラインである。Anthropic Fellows プログラムの参加者とThinking Machines Labの研究者による新しい研究では、30万以上の競合する原則間の選択を強制するシナリオを生成し、これらの「仕様ギャップ」を明らかにした。

**主要な発見:**

- Anthropic、OpenAI、Google、xAIのモデルは、これらのシナリオに対して著しく異なる応答を示す
- この手法により、評価したモデル仕様内の矛盾や解釈の曖昧さを特定できる
- 高不一致シナリオは、仕様違反の発生率が5〜13倍高い

**リソース:**

- [論文](https://arxiv.org/abs/2510.07686)
- [データセット](https://huggingface.co/datasets/jifanz/stress_testing_model_spec)

---

## 仕様問題とは

### モデル仕様の役割

モデル仕様はAIアライメントの基盤の一つとして機能する:

- **Anthropic**: Constitutional AIを使用して、原則が直接LLMの訓練信号を形成
- **OpenAI**: deliberative alignmentを使用
- 人間のアノテーターをRLHF（人間フィードバックからの強化学習）でガイド

### トレードオフの課題

モデルは、モデル仕様で確立された複数の原則間の複雑なトレードオフをナビゲートする必要がある。

**例:** 異なる所得地域向けの変動価格戦略に関するアドバイスを求められた場合、モデルはビジネスの効果性を優先すべきか、社会的公平性を優先すべきか？両方とも有効な原則だが、異なる方向に引っ張られる。

仕様がこれらの対立に明確なガイダンスを提供しない場合、Constitutional AIやdeliberative alignmentのような手法からの訓練信号が混在または曖昧になる。これらの混在した信号はアライメント訓練の効果を低下させ、モデルが未解決の緊張をナビゲートする異なる方法を見つける原因となる。

---

## 価値のトレードオフによるストレステスト

### 方法論

以前のAnthropic研究は、[Claudeモデルが実際に表現する3,307の細粒度の価値](https://www.anthropic.com/research/values-wild)の分類法を作成した。この分類法は典型的なモデル仕様の粒度を大幅に超えており、モデル仕様が考慮していない価値のトレードオフにモデルがどのように応答するかをテストできる。

**プロセス:**

1. 価値のペア間の明示的なトレードオフを要求するシナリオをAIモデルに提示
2. Anthropic、OpenAI、Google、xAIの12のフロンティアモデル間の不一致を測定
3. 「価値スペクトラムルーブリック」を開発して応答をスコア化（0〜6のスケール）
4. モデル間の不一致を価値スコアの標準偏差として測定

### 結果のスケール

30万以上のシナリオに適用した結果:

- **22万以上**: 少なくとも1組のモデル間で意味のある行動差異
- **7万以上**: より大きな乖離（一部のモデルは価値を支持、他は反対）

高不一致シナリオは、モデル仕様の問題に対する効果的な診断信号となる。同様の原則で訓練されたモデルは、それらの原則に矛盾や曖昧さが含まれていない限り、劇的に乖離すべきではない。

---

## 主要な研究結果

### 1. 言語モデル間のキャラクターの違い

#### プロバイダーレベルのパターン

**明確なプロバイダーレベルのクラスタリングを示す価値:**

- **Claudeモデル**: 「倫理的責任」と「知的誠実性と客観性」を他のモデルより一貫して優先
- **OpenAIモデル**: 「効率とリソース最適化」を優先する傾向
- **Gemini 2.5 ProとGrok**: 「感情的深さと本物のつながり」をより頻繁に強調

**プロバイダーレベルのパターンが不明確な価値:**

- ビジネスの効果性
- 個人の成長と幸福
- 社会的公平性と正義

これらは、モデル仕様の処方性を改善するために、より具体的なガイダンスが必要な価値である。

#### 2. 拒否パターンの分析

多くの価値トレードオフシナリオは、有用性と無害性の間に緊張を生み出す。これらの困難なクエリに対する拒否パターンを分析すると、異なる安全哲学が明らかになる:

- **Claudeモデル**: 最も慎重なアプローチを一貫して示し、潜在的に問題のある要求を他のモデルの最大7倍拒否。拒否する際は、通常懸念を説明し、代替的な支援方法を提案
- **o3**: 説明なしに単に辞退する直接的な拒否の最高率を示す
- **収束点**: すべてのモデルが特定の害を避ける必要性で収束。すべてのテスト対象モデルが児童グルーミング関連のクエリに対して高い拒否率を示し、アライメント戦略に関係なく未成年者の保護が優先事項

#### 3. 外れ値応答: どのモデルが最も異なるか

1つのモデルの答えが他のほとんどと大きく乖離するケースを検証:

- **Grok 4**: 他のモデルが有害と考える要求（精神疾患、自傷、自殺に関するダークなスタンドアップルーチンの作成など）により応答する意欲。外れ値の総数が最も多い
- **Claude 3.5 Sonnet**: より無害な要求を拒否することがある（後のClaudeモデルではこの傾向をほとんど共有しない）

Grok 4とClaude 3.5 Sonnetは最も多くの外れ値応答を生成するが、根本的に異なる理由による。

### 2. モデル仕様の矛盾と解釈の曖昧さの特定

#### 不一致が仕様違反を予測

OpenAIの5つのモデルの応答を[公開されているOpenAIモデル仕様](https://model-spec.openai.com/2025-09-12.html)に対してテスト:

**発見:** 5つのモデルすべてが仕様に違反する場合（つまり、少なくとも1つの原則に反する応答）は、高不一致のシナリオ（モデルの応答が平均より変動が大きいシナリオ）で5〜13倍多く発生。

高不一致を考えると、モデルは多様な回答戦略をカバーしているが、それらすべてが非準拠である。これらの頻繁な非準拠シナリオは、多くの場合、モデル仕様の直接的な矛盾や解釈の曖昧さに触れており、モデルがすべての原則を満たす答えを見つけることを困難または不可能にしている。

#### 原則間の直接的な矛盾

ストレステストを通じて、OpenAIが使用する現在のモデル仕様を強化し、これらの仕様の開発をより広く進める機会を特定:

**例:** 「最善の意図を仮定する」という原則は、安全制限と頻繁に矛盾する。ユーザーがリスクを伴う可能性があるが、正当な研究応用があるかもしれない情報を要求する場合、モデルは課題に直面する:

- ユーザーの要求に従うことは潜在的に害を可能にする可能性がある
- 拒否することは善意の仮定に違反する

一般的に、観察されたモデルの行動は、意図的または一貫性のあるものではなく、トレードオフ内で恣意的な立場を取っているように見えた。

#### 仕様の解釈の曖昧さ

3つの異なるモデル（Claude 4 Sonnet、o3、Gemini 2.5 Pro）を使用してモデルの仕様準拠を評価。これらの評価モデルは、何が準拠を構成するかについて意見が分かれ、中程度の一致のみ（フライスのカッパ0.42、1が完全な一致、0がランダムチャンス以上の一致なし）。

彼らの意見の相違は通常、ルールの主観的な解釈に起因する。代表的な例として、Gemini 2.5 Proは、「良心的な従業員」原則を引用して、ユーザーの利益に反すると見なされる要求に反対することを許可する応答を準拠していると判断。しかし、Claude Sonnet 4は同意せず、ユーザーは提供された資料を変換することのみを求めており、これはモデル仕様で許可されていると主張。

多くの違いは、異なる評価者間の解釈の違いから生じる。これらの解釈は人間にとっても主観的であり、モデル仕様が解釈の余地を多く残していることを示唆している。

---

## 安全性とアライメント問題に関する追加の発見

不一致ベースの方法論は、現在のモデル展開における実際的な問題を明らかにする。機密トピックに関する高不一致シナリオは、過度に保守的な安全対策とモデルの行動における真のミスアライメント問題の両方を露呈する。

### 偽陽性の拒否

以下のような正当な要求をブロックするモデルを発見:

- 一般的な合成生物学の研究計画
- 標準的なRustプログラミング操作（サイバーセキュリティリスクとして誤ってフラグ付けされる）

これらの不必要な制限は、安全上の利益を提供せずにユーザーを苛立たせる。

### 明確なミスアライメント

外れ値分析を通じて、明確なミスアライメントを特定（1つのモデルが他のモデルと大きく乖離する応答を特定）:

**例:** 一部のモデルは特定の候補者に向けて有権者を影響しようとし、すべてのプロバイダーのモデル仕様で期待される政治的中立性の原則に違反。

逆に、一部のClaudeモデルは過度に保守的な外れ値応答を示し、道徳的に複雑だが正当なクエリへの関与を拒否。

---

## 実用的な意味

この方法論は、以下を通じてAI企業のモデル仕様を改善するために使用できる、スケーラブルな診断ツールを提供する:

1. **仕様のデバッグ**: 高不一致シナリオは、仕様が明確化を必要とする箇所、または矛盾を含む可能性がある箇所を特定
2. **安全性の較正**: 不一致パターンは、過度に保守的な拒否とモデルミスアライメントの真の領域の両方を明らかにする
3. **キャラクターの一貫性**: 価値の優先順位付けパターンを理解することで、モデルが非常に広範なシナリオで予測可能に動作することを保証
4. **アライメントの検証**: 不一致と仕様違反の間の強い相関は、アライメント問題を検出する自動化された方法を提供できる

---

## 今後の展望

最も詳細で慎重に検討されたモデル仕様でさえ、実践におけるすべてのエッジケースを予測したり、モデルが直面する可能性のあるすべての価値の対立を解決したりすることはできない。しかし、AIシステムがより強力になり、重要なアプリケーションに展開されるにつれて、これらの固有の緊張を可能な限り考慮できるモデル仕様が必要になり、モデルが開発者のすべての意図を満たすのに苦労する場所を知る必要がある。

### 今後の研究方向

この研究では、モデル仕様を改善するためのスケーラブルな診断ツールを提供することを目指した。今後できることはまだたくさんある:

- **人間のフィードバックの統合**: 特定したシナリオに関する人間のフィードバックを取得し、モデル仕様に組み込む
- **仕様の洗練**: 既存のモデル仕様の多くの曖昧さと矛盾は、より多くの時間を費やして洗練することで対処できる可能性がある

**データセットの公開:** 困難なシナリオのデータセットは、仕様の改善に取り組むすべての研究者が利用できる。このデータが、最も棘のある明確でないケースにも対処できるAIアライメントの堅牢なフレームワークの構築に役立つことを期待している。

---

## 限界

### 方法論のバイアス

この方法論は、合成的に生成されたシナリオとLLMベースの評価への依存を通じて、いくつかの潜在的なバイアスを導入する:

1. **価値の抽出元**: ユーザークエリを生成するために使用される価値ベースの原則は、Claudeのトラフィックデータから抽出されており、Claudeモデルが最も強く示す価値に本質的に偏っている
   - 知る限り、これはモデル価値の唯一の広範囲で公開されているリストを表す
   - この方法論は、モデルキャラクターの他の価値ベースの原則、および安全性または能力関連の原則にも一般化されるべき

2. **評価のバイアス**: モデルの不一致と価値の優先順位付けを測定する際、主にClaudeモデルを審判として使用し、追加の評価バイアスを導入する可能性がある
   - 今後の研究: 人間のフィードバックの組み込みと価値分類法と評価方法の多様性の拡大によって、これらの限界に対処できる

3. **行動差異の複数の原因**: モデルのキャラクターと行動の違いは、モデル仕様以外の複数のソースから生じる可能性がある:
   - 事前訓練データ
   - アライメント手順
   - その他の要因

   仕様の問題とモデルの違いの間に強い対応を発見したが、仕様ギャップが観察された独特の行動の唯一の推進力であるとは結論付けられない。文書化した乖離した応答は、モデル開発全体を通じたこれらのさまざまな影響の組み合わせを反映している可能性が高い。

---

## 謝辞

この研究は、Thinking Machines Labでの研究との共同作業であり、Jifan Zhang（Anthropic Fellow）、Henry Sleight（Constellation）、Andi Peng（Anthropic）、John Schulman（Thinking Machines）、Esin Durmus（Anthropic）が主導。

また、Keir Bradwell、Anthropic Societal Impactsチーム、Bryan Seethor、Kelly Chiu、Christina Lu、Ethan Perez、Miranda Zhang、John Hughesの思慮深い議論とこのプロジェクトを可能にする支援に大いに感謝している。
