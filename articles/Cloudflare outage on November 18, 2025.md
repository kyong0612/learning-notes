---
title: "Cloudflare outage on November 18, 2025"
source: "https://blog.cloudflare.com/18-november-2025-outage/"
author:
  - "[[Matthew Prince]]"
  - "[[David Belson]]"
  - "[[Will Allen]]"
  - "[[Jin-Hee Lee]]"
  - "[[Oliver Payne]]"
  - "[[Bob AminAzad]]"
  - "[[Viktor Chynarov]]"
  - "[[Aleksandar Pavlov Hrusanov]]"
  - "[[Prajjwal Gupta]]"
published: 2025-11-18
created: 2025-11-19
description: |
  Cloudflareが2025年11月18日に大規模なサービス障害を経験した。原因は、ClickHouseデータベースの権限管理改善のための変更により、Bot Managementシステムの機能ファイル（feature file）に重複エントリが生成され、ファイルサイズが2倍以上になったこと。この大きなファイルがネットワーク全体に伝播し、機能ファイルのサイズ制限（200機能）を超えたため、コアプロキシシステムがパニックを起こし、HTTP 5xxエラーが発生した。11:20 UTCに障害が開始し、14:30 UTCに主要な影響が解決、17:06 UTCにすべてのサービスが正常に復旧。2019年以来最悪の障害となった。
tags:
  - "outage"
  - "post-mortem"
  - "bot-management"
  - "cloudflare"
  - "incident-response"
  - "clickhouse"
  - "infrastructure"
---

# 概要

2025年11月18日11:20 UTC、Cloudflareのネットワークがコアネットワークトラフィックの配信に重大な障害を発生させた。この問題は**サイバー攻撃や悪意のある活動によるものではなく**、データベースシステムの権限変更によりBot Managementシステムの機能ファイルに重複エントリが生成され、ファイルサイズが2倍以上になったことが原因だった。

大きな機能ファイルがネットワーク全体に伝播し、機能ファイルのサイズ制限（200機能）を超えたため、コアプロキシシステムがパニックを起こし、HTTP 5xxエラーが発生した。14:30 UTCに主要な影響が解決し、17:06 UTCにすべてのサービスが正常に復旧した。

これは2019年以来、Cloudflareが経験した最悪の障害であり、コアトラフィックの大部分が停止した。

# 障害の詳細

## タイムライン

| 時刻 (UTC) | ステータス | 説明 |
| --- | --- | --- |
| 11:05 | 正常 | データベースアクセス制御の変更をデプロイ |
| 11:28 | 影響開始 | デプロイが顧客環境に到達、顧客HTTPトラフィックで最初のエラーが観測 |
| 11:32-13:05 | 調査中 | Workers KVサービスのエラー率上昇を調査。トラフィック操作やアカウント制限などの緩和策を試行 |
| 13:05 | Workers KVとCloudflare Accessのバイパス実装 | 影響を軽減するため、内部システムバイパスを使用 |
| 13:37 | Bot Management設定ファイルのロールバック作業開始 | Bot Management設定ファイルがインシデントのトリガーであることを特定 |
| 14:24 | 新しいBot Management設定ファイルの作成と伝播を停止 | 悪い設定ファイルの自動デプロイを停止 |
| 14:30 | 主要な影響が解決 | 正しいBot Management設定ファイルをグローバルにデプロイし、ほとんどのサービスが正常に動作開始 |
| 17:06 | すべてのサービスが解決 | すべてのダウンストリームサービスが再起動し、すべての操作が完全に復旧 |

## 影響を受けたサービス

### Core CDN and security services

- HTTP 5xxステータスコードが発生
- エンドユーザーにはエラーページが表示された

### Turnstile

- Turnstileの読み込みに失敗
- ダッシュボードへのログインが不可能になった

### Workers KV

- HTTP 5xxエラーが大幅に増加
- KVの「フロントエンド」ゲートウェイへのリクエストがコアプロキシの失敗により失敗

### Dashboard

- ダッシュボード自体はほぼ動作していたが、ログインページでTurnstileが利用できないため、ほとんどのユーザーがログインできなかった
- 2つの期間で可用性が低下：
  - 11:30-13:10: Workers KVへの影響による
  - 14:40-15:30: ログイン試行のバックログがダッシュボードを圧迫

### Email Security

- メール処理と配信は影響を受けなかった
- IPレピュテーションソースへの一時的なアクセス損失により、スパム検出精度が低下
- 一部のAuto Moveアクションで失敗が発生

### Access

- 認証失敗が広範囲に発生（11:20から13:05のロールバック開始まで）
- 既存のAccessセッションは影響を受けなかった
- 失敗した認証試行はすべてエラーページにリダイレクトされ、ターゲットアプリケーションには到達しなかった
- 設定更新は失敗するか、非常に遅く伝播した

## エラーの変動パターン

障害中、システムは回復と失敗を繰り返す異常な動作を示した。これは、機能ファイルが5分ごとにClickHouseデータベースクラスターのクエリによって生成され、クラスターの一部が更新されている場合にのみ悪いデータが生成されたため。その結果、5分ごとに良い設定ファイルと悪い設定ファイルのいずれかが生成され、ネットワーク全体に急速に伝播した。

この変動により、システム全体が回復してから再び失敗するという状況が発生し、当初は攻撃が原因である可能性を疑わせた。最終的に、すべてのClickHouseノードが悪い設定ファイルを生成するようになり、変動は失敗状態で安定した。

# 根本原因の詳細分析

## Cloudflareのリクエスト処理フロー

Cloudflareへのすべてのリクエストは、明確に定義されたパスを通る：

1. **HTTP/TLS層**: リクエストが最初に終端される
2. **コアプロキシシステム（FL）**: リクエストが流れ込む
3. **Pingora**: キャッシュルックアップまたはオリジンからのデータ取得を実行

コアプロキシ内で、セキュリティとパフォーマンス製品が実行される。Bot Managementモジュールが今日の障害の原因となった。

## Bot Managementシステム

CloudflareのBot Managementには、ネットワークを通過するすべてのリクエストに対してボットスコアを生成する機械学習モデルが含まれている。このモデルは「機能」設定ファイルを入力として受け取る。

機能ファイルは数分ごとに更新され、ネットワーク全体に公開される。これにより、インターネット全体のトラフィックフローの変化や新しいタイプのボットやボット攻撃に対応できる。

## クエリ動作の変更

### ClickHouse分散クエリの仕組み

ClickHouseクラスターは複数のシャードで構成される。すべてのシャードからデータをクエリするため、`default`データベースに分散テーブル（`Distributed`テーブルエンジンを使用）がある。分散エンジンは`r0`データベース内の基盤テーブルをクエリする。

### 権限管理の改善

分散クエリのセキュリティと信頼性を向上させるため、共有システムアカウントの代わりに初期ユーザーアカウントの下で実行されるようにする作業が進行中だった。

11:05の変更により、ユーザーが`r0`内の基盤テーブルへのアクセスを明示的に見ることができるようになった。これにより、すべての分散サブクエリが初期ユーザーの下で実行され、クエリ制限とアクセス許可をより細かく評価できるようになった。

### 問題の発生

過去の仮定では、以下のようなクエリが返す列のリストには`default`データベースのみが含まれるとされていた：

```sql
SELECT
name,
type
FROM system.columns
WHERE
table = 'http_requests_features'
order by name;
```

このクエリはデータベース名でフィルタリングしていない。11:05の変更後、このクエリは`r0`データベースに保存されている基盤テーブルの列の「重複」を返すようになった。

これがBot Management機能ファイル生成ロジックで実行されていたクエリのタイプだった。結果として、応答に`r0`スキーマのすべてのメタデータが含まれ、応答の行数が実質的に2倍以上になり、最終的なファイル出力の行数（つまり機能数）に影響を与えた。

## メモリの事前割り当て

プロキシサービスで実行される各モジュールには、無制限のメモリ消費を避け、パフォーマンス最適化のためにメモリを事前割り当てするための制限がある。Bot Managementシステムには、実行時に使用できる機械学習機能の数に制限がある。現在、この制限は200に設定されており、現在の使用量（約60機能）を大幅に上回っている。

200を超える機能を持つ悪いファイルがサーバーに伝播すると、この制限に達し、システムがパニックを起こした。FL2のRustコードでチェックが行われ、未処理のエラーの原因となった：

```
thread fl2_worker_thread panicked: called Result::unwrap() on an Err value
```

これにより5xxエラーが発生した。

## FL2とFLの違い

Cloudflareは現在、顧客トラフィックを新しいバージョンのプロキシサービス（FL2）に移行中。両方のバージョンが影響を受けたが、観測された影響は異なった：

- **FL2プロキシエンジン**: HTTP 5xxエラーが観測された
- **FLプロキシエンジン（旧）**: エラーは見られなかったが、ボットスコアが正しく生成されず、すべてのトラフィックがボットスコア0を受信した。ボットをブロックするルールを展開していた顧客は、大量の誤検知を見た可能性がある

## その他の影響

### Cloudflare Status Page

Cloudflareのステータスページもダウンした。これは完全にCloudflareのインフラストラクチャの外部でホストされており、Cloudflareに依存していない。偶然の一致だったが、問題を診断しているチームの一部は、攻撃者がシステムとステータスページの両方を標的にしている可能性があると考えた。

### レイテンシの増加

HTTP 5xxエラーに加えて、CDNからの応答のレイテンシが大幅に増加した。これは、デバッグと可観測性システムが大量のCPUを消費し、未処理のエラーに追加のデバッグ情報を自動的に追加していたため。

# 修復とフォローアップステップ

システムがオンラインに戻り正常に機能している現在、このような障害を防ぐためのシステム強化作業がすでに開始されている：

1. **設定ファイルの取り込みの強化**: Cloudflareが生成する設定ファイルの取り込みを、ユーザー生成入力と同じ方法で強化
2. **グローバルキルスイッチの有効化**: 機能のためのより多くのグローバルキルスイッチを有効化
3. **コアダンプの影響軽減**: コアダンプやその他のエラーレポートがシステムリソースを圧迫する可能性を排除
4. **失敗モードのレビュー**: すべてのコアプロキシモジュール全体でエラー条件の失敗モードをレビュー

# 教訓

今日の障害は2019年以来最悪のものだった。過去6年以上、コアトラフィックの大部分が停止するような障害はなかった。このような障害は許容できない。Cloudflareは、トラフィックが常に流れ続けることを保証するため、システムを高い回復力を持つように設計してきた。

過去の障害は常に、より回復力のある新しいシステムの構築につながってきた。今回も同様に、システムを強化し、このような障害が再発しないようにする取り組みが続けられる。
