# OpenAI o3, Claude 3.7 Sonnet , Gemini 2.5 Proの評価と解釈[2025年4月版]

ref: <https://tech.algomatic.jp/entry/2025/04/28/173007>

## 要約

### はじめに

* OpenAI o3, o4-mini, Anthropic Claude 3.7 Sonnet, Google Gemini 2.5 Pro, Gemini 2.5 Flashなど、新しい大規模言語モデル（LLM）が次々と登場している。
* モデル選定には実際の使用感や課題、環境が重要だが、全モデルを試すのは困難。
* 公開ベンチマークスコアは、モデルのおおまかな実力を比較するためのベースラインとして役立つ。
* 本記事では、o3, Claude 3.7 Sonnet, Gemini 2.5 Proのベンチマーク結果を整理し、解釈をまとめる。

### 対象モデルとベンチマーク

* **対象モデル:** OpenAI o3, Claude 3.7 Sonnet, Gemini 2.5 Pro
* **対象ベンチマーク:** 上記3モデルの公式報告で2つ以上共通して紹介されているデータセット

#### ベンチマークデータセット詳細

##### 知識・思考力

* **GPQA diamond:**
  * 生物学・物理学・化学分野の大学院レベルの難解なQ&Aベンチマーク。
  * 専門知識と推論力を要求し、「Google-proof」な問題が多い。
  * 人間の専門家正解率: 65% (ミス除き74%)、非専門家+Web検索: 34%。
  * 現状: 人間専門家スコア > LLMスコア。
* **AIME 2024, AIME 2025:**
  * 米国の高校生向け数学競技試験 (AIME) の問題を使用。
  * LLMの数学的推論力と問題解決能力を評価。
* **MMMU:**
  * テキストと画像が混在する大学レベルの問題で、マルチモーダルな知識理解と推論力を評価。
  * 視覚情報とテキストの統合処理能力、専門知識に基づく推論力を測定。
  * 日本語版 JMMMU も発表済み。
* **Humanity's Last Exam (HLE):**
  * 数学・人文・自然科学など多岐にわたる分野のフロンティアを問う一問一答形式。
  * 現在最も難しいベンチマークの一つで、LLMスコアに伸びしろがある。
  * 評価指標: 精度 (Accuracy) とキャリブレーションエラー (モデルの確信度と実際の正答率のズレ)。

##### コーディング能力

* **SWE-bench Verified:**
  * OpenAIが提案した、ソフトウェアエンジニアリング能力を測る代表的指標。
  * PythonプロジェクトのIssueと解決Pull Requestペアで構成。ユニットテスト通過で正否判定。
  * SWE-benchの改良版 (500件)。
  * 注意点: 環境差異でテストがパスしない場合があり、スコアがscaffoldingあり/なしか確認要。
* **Aider polyglot:**
  * 複数言語 (C++, Go, Java, JavaScript, Python, Rust) でのコード編集タスクベンチマーク。
  * 既存コードへの指示通りの変更とテスト通過能力を評価。
  * 文脈理解、言語知識、バグ検出・修正能力、マルチリンガル能力が求められる。
  * 評価形式: whole (全体書換) vs diff (部分変更)。多くはwholeが高精度 (例外: GPT-4.1はdiff)。

##### ツール利活用

* **TAU-bench:**
  * ユーザーとAIエージェントの対話をシミュレートし、ツール使用を伴う複雑なタスク達成能力を評価。
  * 航空券予約変更 (Airline) や注文キャンセル (Retail) など実務的なタスクを含む。
  * エージェントはDBやAPIにアクセスしてタスクを遂行。

### ベンチマークデータセットのスコアと解釈

**表1: 各モデルのベンチマークスコア**

| ベンチマーク             | o3    | Claude 3.7 Sonnet | Gemini 2.5 Pro | 備考           |
| :----------------------- | :---- | :---------------- | :------------- | :------------- |
| GPQA diamond             | 83.3% | 78.2%             | 84.0%          | pass@1         |
| AIME 2024                | 91.6% | 61.3%             | 92.0%          |                |
| AIME 2025                | 88.9% | 49.5%             | 86.7%          |                |
| MMMU                     | 82.9% | 75.0%             | 81.7%          | single attempt |
| Humanity's Last Exam     | 20.3% | 8.9%              | 18.8%          |                |
| SWE-bench Verified       | 69.1% | 62.3%             | 63.8%          | without scaffold |
| Aider polyglot           | 81.3% | 64.9%             | 74.0%          | whole          |
| TAU-bench (Retail)       | 70.4% | 81.2%             | ---            |                |
| TAU-bench (Airline)      | 52.0% | 58.4%             | ---            |                |
*注: TAU-bench除くツール未使用の結果。数値は公式引用。一部Webサイト報告値と異なる場合あり。*

#### 解釈

* **論理的推論能力 (GPQA, AIME, HLE):** o3 ≒ Gemini 2.5 Pro > Claude 3.7 Sonnet
* **コーディング能力 (SWE-bench, Aider):** o3 > Gemini 2.5 Pro > Claude 3.7 Sonnet
* **エージェント的ツール利用 (TAU-bench):** Claude 3.7 Sonnet > o3 (Gemini 2.5 Proはデータなし)

#### 実践的な解釈と注意点

* **ロングコンテキスト性能:**
  * 上記のベンチマークはロングコンテキスト性能を厳密には測れない。
  * 対策: ①ロングコンテキストベンチマーク (例: LongBench v2) とコーディングベンチマークを組み合わせる、②ロングコンテキスト×コーディングのベンチマークを探す/計測する。
  * 参考 (信憑性は相対的に低い): Fiction.LiveBench (小説理解) では120kまで o3 > Gemini 2.5 Pro > Claude 3.7 Sonnet。
  * 結論 (参考値): 120kまでのコーディング性能は o3 > Gemini 2.5 Pro > Claude 3.7 Sonnet と解釈可能。
* **コンテキストウィンドウ:**
  * 200kを超えるコンテキストを扱う場合は Gemini 2.5 Pro (1M) が唯一の選択肢。
  * o3: 128k, Claude 3.7 Sonnet: 200k

**表2: 各モデルのコンテキスト長**

| モデル              | コンテキスト長 |
| :------------------ | :----------- |
| o3                  | 128k         |
| Claude 3.7 Sonnet | 200k         |
| Gemini 2.5 Pro    | 1M           |

* **ツール利用を含む複雑なコーディングタスク:**
  * SWE-bench/AiderだけでなくTAU-benchの結果も考慮が必要。
  * 現状、スコアだけではどのモデルが明確に優れているか判別しにくい。
* **解釈の注意:**
  * 任せたいタスクの種類や定義によって最適なモデルは異なる。
  * ベンチマークスコアの信憑性には注意が必要 (例: Fiction.LiveBench)。

### おわりに

* o3, Claude 3.7 Sonnet, Gemini 2.5 Proのベンチマーク結果と解釈を整理した。
* モデル選定は実際の使用感が最重要だが、ベンチマークスコアはベースラインとして有用。
* Algomaticでは生成AIネイティブな事業創出のため、多様なスキルを持つ人材を募集している。

### 脚注・補足

* \*1: Claude 3.7 Sonnet発表時のAnthropicの主張。
* \*2: SWE-benchには軽量版 (Lite) やマルチモーダル版 (Multimodal) も存在する。
* \*3: SWE-benchの多言語版としてMulti-SWE-benchも提案されている。
* \*4: GPT-4.1はdiff形式の方が精度が高いという例外がある。
* \*5: Fiction.LiveBenchはデータセット・評価方法が非公開なため、信憑性は相対的に低い。
