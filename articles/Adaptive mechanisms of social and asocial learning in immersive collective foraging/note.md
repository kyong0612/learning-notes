# Adaptive mechanisms of social and asocial learning in immersive collective foraging

ref: <https://www.nature.com/articles/s41467-025-58365-6>

## 概要 (Abstract)

人間の認知は、異なる環境や状況に適応する能力によって特徴づけられる。しかし、適応行動を駆動するメカニズムは、主に非社会的文脈と社会的文脈で別々に研究されており、統合的なフレームワークは未だ不明確である。本研究では、仮想Minecraft環境での集団採餌タスクを用いて、視野データの自動文字起こしと高解像度の空間軌跡データを組み合わせることで、これら二つの分野を統合する。行動分析は社会的相互作用の構造と時間的ダイナミクスを捉え、計算モデルを用いて各採餌決定を逐次予測することで直接検証される。これらの結果は、非社会的採餌と選択的社会学習の両方の適応メカニズムが、（社会的要因ではなく）個々の採餌成功によって駆動されることを明らかにする。さらに、個々のパフォーマンスを最もよく予測するのは、非社会的学習と社会学習の両方の適応度である。これらの発見は、非社会的領域と社会的領域にまたがる理論を統合するだけでなく、複雑で動的な sociais 景観における人間の意思決定の適応性に関する重要な洞察を提供する。

## はじめに (Introduction)

人間は他の動物とは異なるユニークな社会学習能力を持っている。他者から学ぶ方法において非常に柔軟であり、個人的情報と sociais 情報を動的に統合し、自身の能力が不足していると思われる場合には選択的に社会学習を好む。近年の研究では個人と社会の意思決定を結びつけ始めているが、固定戦略や社会学習と非 sociais 学習の任意の混合を仮定することが多い。そのため、異なる環境や状況に適応し、非 sociais 学習戦略と sociais 学習戦略を動的に仲裁・統合することを可能にするメカニズムについては、まだほとんどわかっていない。

歴史的に、非 sociais 学習と sociale 学習の研究は互いにほぼ独立して進んできた。非 sociais 学習理論は通常、意思決定者が孤立して行動すると仮定する一方、 sociale 学習理論は個人学習メカニズムを大幅に単純化するか、完全に省略することが多い。「生産者対盗人」ジレンマの観点から個人学習と社会学習のトレードオフが研究されたが、これは純粋な個人学習（生産）または純粋な社会学習（盗み）を仮定していた。この文脈では、盗みは生産の機会を減らすコストを伴い、どの戦略も頻度依存的な適応度を持つ。理論モデルでは、社会的学習者と非社会的学習者の中間的なバランスが最良の結果につながることが示されているが、人々が現実的な条件下でこのバランスをどのように動的に交渉し、異なる環境文脈にどのように適応するかはまだ不明である。

本研究では、没入型Minecraft環境（図1a-d）でプログラムされた集団採餌タスクを使用し、人々が異なる資源分布（ランダム対スムーズ；図1e）や異なる動的文脈（例：個々のパフォーマンスと成功の sociais 観察）に非 sociais および sociale 学習戦略をどのように適応させるかを研究する。視野データの自動文字起こし（図1c）という新しい手法を用いることで、どの参加者や環境要素がどの時点で見えていたかを特定できる。これにより、視覚的注意と空間軌跡、採餌決定を動的に統合し、適応行動のドライバーを研究するための共通フレームワークを提供する。

**図1: Minecraftゲームエンジンで実装された集団採餌タスク**

[![figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-025-58365-6/MediaObjects/41467_2025_58365_Fig1_HTML.png)](/articles/s41467-025-58365-6/figures/1)

* **a** 参加者は20×20の資源ブロックがあるフィールドで隠された報酬を探した。各ラウンドは120秒で、プレイヤーはランダムな場所（十字）と方向（矢印）から開始した。
* **b** プレイヤーの視野（FOV）を示すスクリーンショット。報酬（青いしぶき）は他のプレイヤーにも見え、スムーズな環境（**e**）では近くの報酬を予測するための関連する社会情報を提供するが、ランダムな環境では提供しない。
* **c** 各プレイヤーのFOVの自動文字起こし（**b**のスクリーンショットに対応）。可視性およびモデルベースの分析で使用された（「メソッド」参照）。
* **d** 参加者は、2つの練習ラウンドを完了する前に、インタラクティブなチュートリアル（参照用スクリーンショット）でタスクについて学んだ。主な実験は16ラウンド（順序はカウンターバランス）で構成され、条件（ソロ対グループ）と報酬構造（ランダム対スムーズ）で操作され、同じタイプの4つの連続したラウンドがあった（ビデオについては[補足メソッド](/articles/s41467-025-58365-6#MOESM1)参照）。
* **e** ランダム環境では報酬が均一にサンプリングされたが、スムーズ環境では報酬が空間的にクラスター化されていた。各黒いピクセルは代表的なサンプルからの報酬を示し、両方の環境で同じ基本レート *p*(reward) = 0.25 であった。カボチャとスイカへのマッピングはセッション間でカウンターバランスされた。
* **f** エージェントベースシミュレーション（「メソッド」参照）は、スムーズな環境では非 sociais 学習よりも成功バイアスのある sociale 学習に利点があるが、ランダム環境では利点がないことを示している。一方、バイアスのない sociale 学習は両方でパフォーマンスが低い。ドットは平均を示し、エラーバーは10kシミュレーションでの95％CIを示す。この研究はMojangまたはMicrosoftによって承認または関連付けられていない。**b**および**d**のスクリーンショットはMinecraftの使用ガイドラインに従って使用されている（©2025 Mojang AB; TM Microsoft Corporation）。

[フルサイズ画像](https://www.nature.com/articles/s41467-025-58365-6/figures/1)

非 sociais 採餌と sociale 学習の両方で適応メカニズムが独立して研究されてきたが、これら二つのアプローチはまだ単一のフレームワークに統合されていない。非 sociais 採餌では、*エリア制限探索* (ARS) が、バクテリアから人間まで多様な種で見られる適応的探索戦略として説明されてきた。豊かな報酬は局所的な探索を促し、乏しい報酬は探索距離の増加を促進する。ARSは非常に適応的な探索パターンを説明できるが、まだ sociale 学習と統合されていない。 sociale 学習においても、個人情報と社会情報の質を比較する文脈依存戦略に基づく適応メカニズムが提案されている。Enquistらは、最初に社会学習を試し、それが不十分であれば個人学習に切り替える「批判的 sociale 学習者」と、最初に個人学習を試し、必要であれば sociale 学習に切り替える「条件的 sociale 学習者」という二つの適応戦略を提案した。これらのアプローチは固定レベルの社会学習戦略よりも柔軟であるが、誰から学ぶかという社会学習の選択性に関する説明が欠けており、非 sociais 採餌や報酬予測メカニズムともまだ統合されていない。本研究では、統合的な行動分析とモデルベース分析を通じてこのギャップを埋めることを目指す。

## 結果 (Results)

参加者（*n* = 128）は、隠された報酬を単独または4人グループ（ソロ対グループ；被験者内デザイン）で、20×20の資源ブロックを持つ仮想環境で探索した（図1a-d）。報酬分布（ランダム対スムーズ）を操作して社会学習の価値を変更した（図1e）。スムーズ環境では報酬がクラスター化されており、成功した個人の社会的観察（青いしぶきとして見える；図1b）が近くの他の報酬を予測可能にした。対照的に、ランダム環境では予測不可能な報酬が提供され、社会学習の利点はなかった。

エージェントベースシミュレーション（「メソッド」参照）はこの直感を支持し、ランダム環境では非 sociais 学習者が優位であり、スムーズ環境では選択的で成功バイアスのある sociale 学習者が最もパフォーマンスが高かった（図1f）。

まず、環境と社会情報が行動パターンに与える影響を探る（図2a-d）。次に、ネットワーク構造（図2e-g）と社会的相互作用の時間的ダイナミクス（図2h-k）の影響を分析することで、適応的な非 sociais および sociale 学習のドライバーを明らかにする。さらに、リーダー・フォロワーダイナミクスにおける「プル」イベントを分析し（図3）、空間位置の具体的な変化を通じて社会情報の積極的な使用を捉える。最後に、逐次的な採餌決定を予測する計算モデルを使用して、適応的な個人学習メカニズムと sociale 学習メカニズムのさまざまな組み合わせを直接テストする（図4）。すべての結果は非 sociais ベースライン（ソロラウンドのデータを使用）と比較され、どのメカニズムがユニークな社会現象であるかを特定する。

### 行動結果 (Behavioral results)

環境と社会的相互作用がパフォーマンスにどのように影響したかを理解するために、正規化された報酬率を調べる（図2a）。階層ベイズ回帰（「メソッド」参照）を使用すると、参加者はスムーズな環境でより多くの報酬を獲得したが（事後平均：0.22、95％最高事後密度区間（HPDI）：[0.19, 0.24]）、 sociais 条件の信頼できる影響はなく（0.002 [-0.02, 0.02]）、条件と環境の相互作用もなかった（-0.02 [-0.05, 0.01]）。したがって、個人とグループの両方がスムーズな環境でより高い採餌成功を達成したが、 sociale 採餌の同時的な情報的利益と競争的欠点は、集計レベルでのパフォーマンスに信頼できる変化をもたらさなかった。

**図2: 行動結果**

[![figure 2](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-025-58365-6/MediaObjects/41467_2025_58365_Fig2_HTML.png)](/articles/s41467-025-58365-6/figures/2)

* **a** 報酬枯渇を制御した正規化報酬率（図S1参照）。線はバイナリデータの平滑化のための一般化加法モデル（GAM）から（グループ平均と95％CI）。すべてのCIは*n* = 128人の参加者に対するもの。
* **b** 以前の成功の関数としての採餌距離。ドットはグループ平均を示し、エラーバーは95％CIを示す。
* **c** 時間経過に伴う仲間への平均距離（回帰結果については図S5a参照）。線は集計平均を示し（リボンは95％CI）。
* **d** 平均可視性。インセットは時間で周辺化されたグループ平均を示す（回帰については図S5b参照）。リボンとエラーバーの両方が95％CIを示す。集計レベルでは、平均可視ピア（アウトバウンド）と平均観察者（インバウンド）は同等であることに注意。
* **e**–**g** ネットワーク分析。**e**は近接性および可視性ネットワークの例を示す。**f**は近接性ネットワークで計算された固有ベクトル中心性の関数としての平均報酬を示す。各ドットは1人の参加者を表す。線とリボンは階層ベイズ回帰の固定効果（±95％HPDI）を示し、テキストラベルは効果を報告（グループラウンドは太字）、信頼できる効果（ゼロと重ならない）は下線付き。**g**は可視性ネットワークの入次数と出次数の対応を示す。各ドットは1人の参加者を表す。回帰線は階層ベイズ回帰の固定効果（図S6）。
* **h**–**k** グループラウンドにおける報酬率と近接性/可視性の時間的ダイナミクス（ソロラウンドとの比較については図S7参照）。**h**は分析の視覚的描写を提供する（詳細は「メソッド」参照）。結果は（**i**–**k**）に示され、*y*軸は相関の符号と強度（チャンス補正後）を示す。線はグループ平均、リボンは95％CIを示す。太線は順列分析を生き残った有意なクラスターを示す（「メソッド」参照）。負のオフセットでの効果は、報酬が将来の近接性/可視性を予測することを示し、正のオフセットでの効果は、近接性/可視性が将来の報酬を予測することを示す。正の効果は報酬が行動とともに増加したことを示し、負の効果はその逆を示す。

[フルサイズ画像](https://www.nature.com/articles/s41467-025-58365-6/figures/2)

個々のパフォーマンスは適応的な採餌を通じて改善された可能性がある。これをテストするために、まず、前のブロックが報酬をもたらしたかどうかに応じて採餌距離がどのように変化するかを測定した（図2b）。スムーズな環境では、参加者は報酬を獲得した後により局所的に採餌した（-1.6 [-2.0, -1.3]）。対照的に、この効果はランダム環境では逆転し、参加者は報酬後に長距離を採餌した（0.3 [0.1, 0.6]）。より大きな適応性（すなわち、採餌距離の差）は、スムーズ環境でのより良い個人パフォーマンスを予測した（ソロ：*r**τ* = 0.20, *p* < 0.001; グループ：*r**τ* = 0.18, *p* = 0.003）が、ランダム環境では予測しなかった。

次に、 sociais 相互作用の初期分析として、条件間で参加者間の平均ペアワイズ距離を計算した（図2c）。この分析により、スムーズ環境と比較してランダム環境でより大きな sociaux 距離が見られた（0.6 [0.8, 0.4]）。これは、グループラウンドを非 sociais ベースラインと比較した場合に信頼できるほど大きく（0.3 [0.1, 0.5]）、積極的な回避の増加を示唆している。

最後に、視野転写を使用して、特定の時点での参加者ペア間の sociais 視認性を測定した。まず、平均可視ピア数を分析した（図2d）。一般に、参加者はスムーズな環境でお互いをより多く観察したが（0.01 [0.001, 0.02]）、グループラウンドをソロベースラインと比較した場合、信頼できる差はなかった（-0.0001 [-0.01, 0.01]）。対照的に、参加者はランダム環境で非 sociais ベースラインと比較して sociais 視認性を信頼できるほど減少させた（-0.01 [-0.02, -0.00005]）。

### 近接性および可視性ネットワーク (Proximity and visibility networks)

次に、空間的および視覚的相互作用に関する社会ネットワーク分析を実施した（図2e）。近接性ネットワークは、各参加者をノードとして記述し、プレイヤー間の平均近接性（すなわち、逆距離）によって重み付けされた無向エッジを持つ。可視性ネットワークも同様に構築されたが、各ターゲットプレイヤーが他のプレイヤーに見えていた時間の長さに比例して重み付けされた有向エッジを持つ。

近接性ネットワークを使用して各参加者の固有ベクトル中心性を計算した。中心性はランダム環境では報酬を予測しなかったが、スムーズな環境では堅牢な逆転が見られ、より高い中心性がグループラウンドでより高い報酬を予測した（7.9 [1.6, 14.0]; 図2f）。

次に、可視性ネットワークにおける入次数と出次数の関係を調べた。この分析により、 sociaux 注意の非対称性が明らかになり、入次数と出次数の間に一般的な逆の関係が見られた（図2g）。この非対称性はランダムラウンドとソロラウンドでも存在したが、グループラウンドとスムーズ環境を組み合わせると信頼できるほど強くなった（グループ+スムーズ相互作用：-0.49 [-0.89, -0.08]）。これは、社会情報が有用な設定において、社会学習戦略の専門化（すなわち、「生産者」は低い出次数と高い入次数、「盗人」は高い出次数と低い入次数）と sociaux 注意の非対称性の増加を示唆している。

### 時間的ダイナミクス (Temporal dynamics)

 sociaux 相互作用のダイナミクスを分析するために、個々の報酬を高解像度の空間視覚データに関連付ける時間的に予測的なクラスターを探索した（図2h-k）。個々の報酬（すなわち、採餌成功）と他のプレイヤーへの空間的近接性のダイナミクスは、スムーズ環境（ランダム環境ではない）において、*成功依存的な空間的サイクリング*のパターンを明らかにした（図2i）。報酬と視覚フィールドデータのダイナミクスを見ると、*適応的な sociais 注意*の証拠が見つかった（図2j）。報酬とインバウンド可視性（観察者の数）のダイナミクスは*成功バイアスのある選択性*を示し、より高い報酬を獲得した参加者が sociaux 注意の対象となった（図2k）。要約すると、これらの時間的ダイナミクスは、個々のパフォーマンスが適応メカニズムの主要なドライバーであり、 sociaux 近接性（図2i）と sociaux 注意（いつ（図2j）および誰に（図2k）向けられるか）の変化を駆動することを示している。

### 社会的影響とリーダーシップ (Social influence and leadership)

野生のヒヒにおける集団的意思決定を研究するために使用された方法に触発され、リーダー・フォロワーダイナミクスを捉える「プル」イベントの頻度を分析した（「メソッド」参照）。合計537のプルイベントを検出した（例については図3a参照）。ランダム環境ではソロラウンドからグループラウンドへのプルイベントの減少が見られたが（-0.7 [-1.2, -0.1]）、スムーズ環境ではソロラウンドからグループラウンドへのプルイベントの大幅な増加が見られた（1.4 [0.8, 2.0]）。これらの結果は、参加者が sociale 学習の関連性に応じて sociaux 注意だけでなく sociais 影響への感受性も適応させたことを示唆している：適応的な場合は他者に従い（スムーズ）、不適応な場合は積極的に他者を避ける（ランダム）。リーダーシップ指数（*n*leader - *n*follower）を計算したが、リーダーシップはパフォーマンスを予測しなかった（図3c）。しかし、プルイベント中の瞬間的な報酬率に焦点を当てると（図3d）、リーダーはフォロワーよりも多くの報酬を受け取っていた（0.6 [0.3, 0.9]）。

**図3: 社会的影響**

[![figure 3](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-025-58365-6/MediaObjects/41467_2025_58365_Fig3_HTML.png)](/articles/s41467-025-58365-6/figures/3)

* **a** プルイベントの例。ダイアド距離の最小-最大-最小シーケンスから選択され、いくつかの基準でフィルタリングされた（「メソッド」参照）。下部の軌跡はプルを定義する3つの時点を示し、時間*t*3での環境の状態を示す。リーダーの*t*2は*t*3とほぼ重なることに注意。
* **b** ラウンドあたりの平均プルイベント数（エラーバーは95％CIを示し、各ドットはセッション）。参加者が同じフィールドにいるかのようにソロラウンドで同じ分析を実行し、非 sociais ベースラインを提供した。
* **c** リーダーシップ（*n*leader - *n*follower）はパフォーマンスを予測しなかったが、**d**リーダーはプルイベント中に高い瞬間報酬を得ていた。リボンとエラーバーは95％CIを示す。

[フルサイズ画像](https://www.nature.com/articles/s41467-025-58365-6/figures/3)

### 行動の要約 (Behavioral summary)

全体として、参加者は環境と個々のパフォーマンスに応じて非 sociais および sociale 学習メカニズムを*適応的に*展開し、成功した個人に sociaux 注意を*選択的に*向けた。これらの行動結果は、非 sociais 学習と sociale 学習がどのように相互作用し、フィードバックし合うかのダイナミクスへの洞察を提供する。

### 選択の計算モデリング (Computational modeling of choices)

計算モデリングフレームワーク（図4a）を使用して、参加者が次に破壊するブロックを逐次予測する。予測は、ブロック特徴**f**と重み**w**の線形結合に対するソフトマックス分布としてモデル化される。ブロック特徴**f**は個人学習および sociale 学習メカニズムに関する仮説を捉え、重み**w**は階層ベイズ法を使用して推定される。

**図4: 計算モデル**

[![figure 4](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-025-58365-6/MediaObjects/41467_2025_58365_Fig4_HTML.png)](/articles/s41467-025-58365-6/figures/4)

* **a** 左：赤い破線で強調表示されたプレイヤーに焦点を当てたモデルの図解。右：異なるモデルは異なる特徴セットを組み込む（詳細は本文参照）。ここでは、勝利モデルARS+Condの5つの特徴を図解する。黄色が濃くなるほど特徴値が高くなる。モデル予測（右下）は、集団レベルの重み推定値の事後平均を使用して示され、赤い十字は実際の選択を示す。ARS+Condは動的モデルであり、局所性と成功/不成功近接性の重み（左下）は、最後の個人報酬からの経過時間の関数として変化する。
* **b** 最良モデルの事後確率を記述するための保護超過確率を使用したモデル比較。
* **c** 勝利モデル（各条件で）に対する相対WAICを示す個々のモデルフィット。ドットはグループ平均を示し、エラーバーは95％CIを示す。
* **d** 最良モデルの集団レベルの重み推定値（グループ：ARS+cond; ソロ：ARS; 全モデルについては図S12およびS13参照）。ドットは事後平均を示し、エラーバーは95％HPDIを示し、垂直破線は0を示す。

[フルサイズ画像](https://www.nature.com/articles/s41467-025-58365-6/figures/4)

#### 非 sociais 特徴 (Asocial features)

* *局所性*: 現在位置からの逆距離。
* *ブロック可視性*: 視野内にあるブロック（1）かないか（0）。
* *報酬予測*: 構造化環境における非 sociais 報酬般化の認知モデルとしてのガウス過程回帰。

#### 社会的特徴 (Social features)

* *成功近接性*: 可視であり、報酬獲得が観察されたプレイヤーへの近接性。
* *不成功近接性*: 可視であるが、報酬獲得が観察されなかったプレイヤーへの近接性。

#### モデル比較 (Model comparison)

静的モデルと動的モデルを比較した。動的モデルでは、最後の個人報酬または社会的に観察された報酬からの経過時間に応じて特定の重みが変化する。ベイジアンモデル選択を使用し、グループラウンドではハイブリッドモデル*ARS+Cond*（エリア制限探索と条件的社会学習の組み合わせ）が他のすべてのモデルを大幅に上回り（*p*(bestModel) > 0.999; 図4b）、ソロラウンドでは*ARS*モデルが最良であった（図4b、c）。

#### モデルの重み (Model weights)

最良モデルの重みを解釈する（グループ：ARS+Cond; ソロ：ARS）。
局所性とブロック可視性はすべての条件で選択に影響を与えた。報酬予測の重みはスムーズ環境で信頼できるほど大きかったが、ランダム環境ではほぼゼロであった。局所性重みの適応（ARS）はすべての条件で見られ、スムーズ環境とグループ条件でより強かった。負の符号は、個人報酬なしの期間が長くなるにつれて局所性が減少することに対応する。

 sociais 特徴の重み（ARS+Cond）は、参加者がスムーズ環境で成功したプレイヤーに強く影響されたことを示している（1.0 [0.8, 1.2]）。ランダム環境でも効果は偶然よりは信頼できるほど大きかった（0.29 [0.07, 0.53]）。不成功のプレイヤーの影響は見られなかった。成功近接性の重みの適応性はスムーズ環境（0.4 [0.2, 0.5]）でのみ観察された。したがって、参加者は個人報酬なしの期間が長くなるにつれて社会情報への依存度を高め、成功したプレイヤーへの選択性を高めた。

## 考察 (Discussion)

集団採餌は人間の社会学習の一般的なメタファーである。本研究では、没入型仮想環境を使用し、個々の視野の制限が個人学習と社会学習への注意配分のトレードオフを課し、他者への空間的近接性が sociaux 相互作用の機会（およびコスト）を直接形成する。視野データと空間軌跡へのアクセスにより、 sociaux 相互作用の構造的および時間的ダイナミクスへの洞察が得られ、人々が異なる報酬環境と動的に変化する社会環境にどのように適応するかを研究した。

結果は、集団的な人間の行動を駆動する適応メカニズムに光を当て、非 sociais 採餌と文脈依存的かつ選択的な sociale 学習の過去の理論を統合する。この統合により、これらの非 sociais メカニズムと sociale メカニズムが互いを増幅し、相互に補償し合う別々の手がかりによって支配されるのではなく、個々のパフォーマンスという共通の通貨によって駆動されることが明らかになる。さらに、個々のパフォーマンスを最もよく予測するのは、 sociale メカニズムと非 sociale 学習メカニズムの両方の適応度である。

報酬がスムーズにクラスター化されている場合（ sociale 学習の牽引力を提供）、参加者はより強く専門化し、 sociaux 注意の非対称性が大きくなり（図2g）、パフォーマンスに応じて適応的に sociale 情報を求め（図2j）、成功した個人に選択的に sociale 学習を向けた（図2k）。参加者はまた、スムーズな環境でリーダー・フォロワーダイナミクスに「引き込まれる」可能性が高く（図3b）、これは瞬間的な報酬率が高い「リーダー」に選択的に向けられた（図3d）。計算モデル（図4）は空間データと可視性データを組み合わせて、非 sociais 学習メカニズムと sociale 学習メカニズムの両方を説明し、これらは時間とともに動的に適応する。ここで、勝利モデルはエリア制限探索と条件的 sociale 学習を組み合わせ（ARS+Cond）、個々のパフォーマンスが局所的採餌と選択的 sociale 学習の両方の適応性を駆動する主要因であった（図4d）。

### 制限と今後の方向性 (Limitations and future directions)

成功バイアスのある sociale 学習は、大幅に減少したものの、ランダム環境でも存在した。これは、成功バイアスのあるコピーが、多くの状況で有益であるため、人々がそれを解除するのが難しいことを示唆している可能性がある。あるいは、参加者がランダム環境に構造があると信じていた可能性もある（例：ギャンブラーの誤謬）。いずれにせよ、これらの結果は、人間の適応性の程度に対する潜在的な制限と、 sociale 学習への根強いバイアスを示唆している。しかし、ランダム環境では sociale 情報が何の利益も提供しなかったとしても、探索に従事するための計算コストの低いツールを提供する可能性がある。

 sociaux 注意の非対称性（スムーズな環境で増幅）は、不適応な群集行動に対する安全策として機能する可能性がある。我々の研究では、成功した採餌結果は視覚的な手がかり（すなわち、しぶき）によって顕著になったが、人々は潜在的なパフォーマンスやスキルを表向きの行動から推測するためのメタ認知戦略を展開することもできる。

今後の研究では、集団的な調整が個々の探索に付加的な利益をもたらす可能性のある、枯渇しない報酬環境を使用することを検討するかもしれない。また、より短い時間スケールで観察される sociais 学習メカニズムを、より長い文化的タイムスケールでの適応的な結果に結びつける必要がある。

## メソッド (Methods)

### 参加者とデザイン (Participants and design)

* 参加者: 128名 (MPIBリクルートプールから募集、82名女性、平均年齢27.4歳)
* デザイン: 被験者内デザイン。操作変数：報酬構造（ランダム vs. スムーズ）、探索条件（ソロ vs. グループ）。
* 手順: チュートリアル、練習ラウンド（2回）、本実験（16ラウンド、各2分）。報酬はボーナス支払いに関連付けられた。

### 材料と手順 (Materials and procedure)

* 環境: 修正Minecraftサーバー (Java版 v.1.12.2) 上のカスタム環境。20x20グリッドの資源ブロック（カボチャまたはスイカ）、破壊に2.25秒かかる。
* 報酬: 破壊時に報酬（青いしぶき、他者にも見える）または報酬なし。スムーズ環境では報酬はクラスター化、ランダム環境ではランダム分布。
* データ収集: マップログ（ブロック破壊イベント）とプレイヤーログ（位置、向き）を20Hzで記録。

### 視野データの自動文字起こし (Automated transcription of visual field data)

* Unityゲームエンジン (ver. 2019.3) を使用したカスタムツール。
* 実験データを基に各参加者の視点から各ラウンドをシミュレーション。
* 各エンティティ（ブロック、プレイヤー、報酬イベント）に一意のRGB値を割り当て、低解像度レンダリング。
* ピクセルのRGB値から、どのエンティティがいつ（20Hz解像度）、画面のどの程度の割合を占めて見えていたかを特定。
* ソロラウンドも同様に処理し、非 sociais ベースラインを作成。

### エージェントベースシミュレーション (Agent-based simulations)

* 離散時間逐次ゲームとしてモデル化。
* エージェントタイプ：非 sociais 、非バイアス sociais 、バイアス sociais 。
* 各環境（ランダム/スムーズ）で各エージェントタイプの4人グループで100回シミュレーションを実行（合計12,000シミュレーション）。

### 階層ベイズ回帰 (Hierarchical Bayesian regressions)

* `brms` パッケージを使用。
* 実験操作の効果と参加者・グループレベルの変動性をモデル化。
* 弱情報事前分布を使用。

### 時間的ダイナミクス (Temporal dynamics)

* 神経科学の手法に基づき、時系列データ（報酬、空間的近接性、可視ピア数、観察者数）間の時間遅延相関を計算。
* 順列ベースのチャンス補正とクラスターベースの順列検定を使用して有意な時間的クラスターを特定。

### 社会的影響 (Social influence)

* 野生ヒヒの研究手法を適用し、「プル」イベント（リーダー・フォロワーダイナミクス）を検出。
* ペアワイズ距離の最小-最大-最小シーケンスから候補イベントを選択し、強度、不均衡、リーダーシップ、持続時間でフィルタリング。

### 計算モデリング (Computational modeling)

* 各ブロック破壊を予測する逐次選択モデル。
* カテゴリカル尤度関数とソフトマックスリンク関数を使用。
* 特徴量: 非 sociais （局所性、ブロック可視性、報酬予測（GPバイナリ分類））、 sociais （成功近接性、不成功近接性など）。
* 静的モデルと動的モデル（重みが時間経過で変化）を比較。
* `Stan` を使用した階層ベイズ推定。
* モデル比較にはWAICと保護超過確率を使用。

### ガウス過程によるバイナリ報酬予測 (Gaussian process for binary reward prediction)

* 報酬の有無（バイナリ）を予測するためにGPフレームワークを変更。
* 潜在変数 *z* を介してロジスティックシグモイド関数で確率をモデル化。
* ラプラス近似と逆プロビット関数を使用して事後予測分布を近似。
* カーネル関数には放射基底関数カーネルを使用。

## データとコードの可用性 (Data availability / Code availability)

* すべてのデータとコードは [github.com/charleywu/minecraftforaging](http://github.com/charleywu/minecraftforaging) および Zenodo[82](/articles/s41467-025-58365-6#ref-CR82) で利用可能。
